{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Διπλωματική Εργασία\n",
    "## Ταξινόμηση του καρκίνου του μαστού με μεθόδους μηχανικής μάθησης\n",
    "### Εξαγωγή χαρακτηριστικών με PCA\n",
    "\n",
    "> Λάζαρος Πανιτσίδης<br />\n",
    "> Τμήμα Μηχανικών Παραγωγής και Διοίκησης <br />\n",
    "> Διεθνές Πανεπιστήμιο της Ελλάδος <br />\n",
    "> lazarospanitsidis@outlook.com -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diploma thesis\n",
    "## Breast cancer classification using machine learning methods\n",
    "### Selected Features\n",
    "\n",
    "> Lazaros Panitsidis<br />\n",
    "> Department of Industrial Engineering and Management <br />\n",
    "> International Hellenic University <br />\n",
    "> lazarospanitsidis@outlook.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Useful Python Libraries](#1)\n",
    "1. [Data Processing](#2)\n",
    "1. [Gaussian Naive Bayes](#3)\n",
    "1. [Linear Discriminant Analysis](#4)\n",
    "1. [Quadratic Discriminant Analysis](#5)\n",
    "1. [Ridge Classifier](#6)\n",
    "1. [Decision Tree Classifier](#7)\n",
    "1. [Random Forest Classifier](#8)\n",
    "1. [ADA Boost Classifier (Adaptive Boosting)](#9)\n",
    "1. [C-Support Vector Classification](#10)\n",
    "1. [Stochastic Gradient Descent Classifier](#11)\n",
    "1. [eXtreme Gradient Boosting](#12)\n",
    "1. [Light Gradient Boosting Machine](#13)\n",
    "1. [K-Nearest Neighbors Classifier](#14)\n",
    "1. [Multi-layer Perceptron Classifier](#15)\n",
    "1. [Summary](#16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1) Useful Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#import warnings library\n",
    "import warnings\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# some of them are not used in this file\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE, RFECV , mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score , GridSearchCV , LeaveOneOut,KFold,RandomizedSearchCV,StratifiedKFold, HalvingGridSearchCV\n",
    "from skopt import BayesSearchCV # https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV , https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score , make_scorer , classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline , Pipeline # https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder , MinMaxScaler\n",
    "from xgboost import XGBClassifier , plot_importance\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier , RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import lightgbm as lgbm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pygad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 424 cases in this dataset\n",
      "There are 30 features in this dataset\n",
      "There are 212 cases diagnosed as malignant tumor\n",
      "There are 212 cases diagnosed as benign tumor\n",
      "The percentage of malignant cases is: 50.00%\n"
     ]
    }
   ],
   "source": [
    "dataWISC = pd.read_csv('dataWisc.csv')\n",
    "dataWISC.drop([\"id\", \"Unnamed: 32\"], axis = 1, inplace = True)\n",
    "\n",
    "# Undersampling function\n",
    "def make_undersample(_df, column):\n",
    "  dfs_r = {}\n",
    "  dfs_c = {}\n",
    "  smaller = 1e1000\n",
    "  ignore = \"\"\n",
    "  for c in _df[column].unique():\n",
    "    dfs_c[c] = _df[_df[column] == c]\n",
    "    if dfs_c[c].shape[0] < smaller:\n",
    "      smaller = dfs_c[c].shape[0]\n",
    "      ignore = c\n",
    "\n",
    "  for c in dfs_c:\n",
    "    if c == ignore:\n",
    "      continue\n",
    "    dfs_r[c] = resample(dfs_c[c], \n",
    "                        replace=False, # sample without replacement\n",
    "                        n_samples=smaller,\n",
    "                        random_state=0)\n",
    "  return pd.concat([dfs_r[c] for c in dfs_r] + [dfs_c[ignore]])\n",
    "\n",
    "dataWISC = make_undersample(dataWISC,'diagnosis')\n",
    "\n",
    "#Description of the dataset\n",
    "\n",
    "#how many cases are included in the dataset\n",
    "length = len(dataWISC)\n",
    "#how many features are in the dataset\n",
    "features = dataWISC.shape[1]-1 # - diagnosis\n",
    "\n",
    "# Number of malignant cases\n",
    "malignant = len(dataWISC[dataWISC['diagnosis']=='M'])\n",
    "\n",
    "#Number of benign cases\n",
    "benign = len(dataWISC[dataWISC['diagnosis']=='B'])\n",
    "\n",
    "#Rate of malignant tumors over all cases\n",
    "rate = (float(malignant)/(length))*100\n",
    "\n",
    "print (\"There are \"+ str(len(dataWISC))+\" cases in this dataset\")\n",
    "print (\"There are {}\".format(features)+\" features in this dataset\")\n",
    "print (\"There are {}\".format(malignant)+\" cases diagnosed as malignant tumor\")\n",
    "print (\"There are {}\".format(benign)+\" cases diagnosed as benign tumor\")\n",
    "print (\"The percentage of malignant cases is: {:.2f}%\".format(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataWISC.diagnosis                          # M or B \n",
    "x = dataWISC.drop('diagnosis',axis = 1 )\n",
    "target_names=['Benign','Malignant']\n",
    "le= LabelEncoder()\n",
    "le.fit(y)\n",
    "y_le = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>area_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>symmetry_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.228200</td>\n",
       "      <td>561.0</td>\n",
       "      <td>20.20</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>22.30</td>\n",
       "      <td>0.2871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.005579</td>\n",
       "      <td>489.0</td>\n",
       "      <td>22.45</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>18.40</td>\n",
       "      <td>0.2505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.170000</td>\n",
       "      <td>680.9</td>\n",
       "      <td>21.84</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>20.21</td>\n",
       "      <td>0.2369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     concavity_worst  area_mean  area_se  concave points_se  smoothness_worst  \\\n",
       "49          0.228200      561.0    20.20           0.011840            0.1162   \n",
       "285         0.005579      489.0    22.45           0.002924            0.1038   \n",
       "495         0.170000      680.9    21.84           0.011830            0.1216   \n",
       "\n",
       "     texture_mean  symmetry_worst  \n",
       "49          22.30          0.2871  \n",
       "285         18.40          0.2505  \n",
       "495         20.21          0.2369  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = x[['concavity_worst',\n",
    "            'area_mean',\n",
    "            'area_se',\n",
    "            'concave points_se',\n",
    "            'smoothness_worst',\n",
    "            'texture_mean',\n",
    "            'symmetry_worst']]\n",
    "x_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/#:~:text=Given%20the%20improved%20estimate%20of,biased%20estimates%20of%20model%20performance.\n",
    "# cv = LeaveOneOut()\n",
    "rng = np.random.RandomState(13) # random number generator , use it in every random state if shuffle=True for different results.Usefull to test a specific algorithm multiple times within a for loop.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "search_cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "  originalclass.extend(y_true)\n",
    "  predictedclass.extend(y_pred)\n",
    "  #print(classification_report(y_true, y_pred, target_names=target_names)) \n",
    "  return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def print_best_params(search):\n",
    "    print(\"\")\n",
    "    print(\"Best hyperparameters : \", search.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best estimator : \", search.best_estimator_)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method didn't work so it will not be used (nested cross validation which returns the best parameters and their scores)\n",
    "\n",
    "# Following kf is the outer loop\n",
    "outer_kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=13)\n",
    "inner_kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=13)\n",
    "# model = SVC()\n",
    "# params = {'kernel':['rbf','linear'],'C':[1,10]}\n",
    "def nested_cv_with_gscv(model,params,x,y):\n",
    "    outer_loop_accuracy_scores = []\n",
    "    inner_loop_won_params = []\n",
    "    inner_loop_accuracy_scores = []\n",
    "\n",
    "    # Looping through the outer loop, feeding each training set into a GSCV as the inner loop\n",
    "    for train_index,test_index in outer_kf.split(x,y):\n",
    "        \n",
    "        GSCV = GridSearchCV(estimator=model,param_grid=params,cv=inner_kf)\n",
    "        \n",
    "        # GSCV is looping through the training data to find the best parameters. This is the inner loop\n",
    "        GSCV.fit(x[train_index],y[train_index])\n",
    "        \n",
    "        # The best hyper parameters from GSCV is now being tested on the unseen outer loop test data.\n",
    "        pred = GSCV.predict(x[test_index])\n",
    "        \n",
    "        # Appending the \"winning\" hyper parameters and their associated accuracy score\n",
    "        inner_loop_won_params.append(GSCV.best_params_)\n",
    "        outer_loop_accuracy_scores.append(accuracy_score(y[test_index],pred))\n",
    "        inner_loop_accuracy_scores.append(GSCV.best_score_)\n",
    "\n",
    "    for i in zip(inner_loop_won_params,outer_loop_accuracy_scores,inner_loop_accuracy_scores):\n",
    "        print (i)\n",
    "\n",
    "    print('Mean of outer loop accuracy score:',np.mean(outer_loop_accuracy_scores))\n",
    "\n",
    "# https://github.com/rosscleung/Projects/blob/b9abc20db545d9f483e90a9b046ea50c74f25718/Tutorial%20notebooks/Nested%20Cross%20Validation%20Example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The process by which the best model will be selected is as follows:\n",
    "1. Evaluation of the default algorithm with 10-fold cross validation\n",
    "2. Evaluation of the tuned hyperparameter algorithm with nested cross-validation (5-fold Grid Search/Randomized Search inside a 10-fold cross validation)\n",
    "3. Choosing the best model (from steps 1 and 2) and finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3) [Gaussian Naive Bayes](<https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.911     0.962     0.936       212\n",
      "   Malignant      0.960     0.906     0.932       212\n",
      "\n",
      "    accuracy                          0.934       424\n",
      "   macro avg      0.935     0.934     0.934       424\n",
      "weighted avg      0.935     0.934     0.934       424\n",
      "\n",
      "--- Time of execution : 0.05690920000006372 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_gnb = Pipeline([('scaler', StandardScaler()), ('gnb', GaussianNB())])\n",
    "score = cross_val_score(clf_gnb, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.895     0.967     0.930       212\n",
      "   Malignant      0.964     0.887     0.924       212\n",
      "\n",
      "    accuracy                          0.927       424\n",
      "   macro avg      0.930     0.927     0.927       424\n",
      "weighted avg      0.930     0.927     0.927       424\n",
      "\n",
      "--- Time of execution : 5.387287800000195 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "param_grid = { 'gnb__var_smoothing': np.logspace(0,-10, num=100) }\n",
    "\n",
    "search = GridSearchCV(clf_gnb, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4) [Linear Discriminant Analysis](<https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.933     0.991     0.961       212\n",
      "   Malignant      0.990     0.929     0.959       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.962     0.960     0.960       424\n",
      "weighted avg      0.962     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 0.060909499999979744 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_lda = Pipeline([('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis())])\n",
    "\n",
    "score = cross_val_score(clf_lda, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.933     0.991     0.961       212\n",
      "   Malignant      0.990     0.929     0.959       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.962     0.960     0.960       424\n",
      "weighted avg      0.962     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 6.9193594999997 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "param_grid = [\n",
    "    {\n",
    "        'lda__solver' : ['lsqr','eigen'],\n",
    "        'lda__shrinkage':[None,'auto']\n",
    "    },\n",
    "    {\n",
    "        'lda__solver' : ['svd'],\n",
    "        'lda__tol': np.linspace(0, 0.01, num=100)\n",
    "    }\n",
    "]\n",
    "\n",
    "search = RandomizedSearchCV(clf_lda, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5) [Quadratic Discriminant Analysis](<https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.935     0.958     0.946       212\n",
      "   Malignant      0.957     0.934     0.945       212\n",
      "\n",
      "    accuracy                          0.946       424\n",
      "   macro avg      0.946     0.946     0.946       424\n",
      "weighted avg      0.946     0.946     0.946       424\n",
      "\n",
      "--- Time of execution : 0.06294608116149902 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_qda = Pipeline([('scaler', StandardScaler()), ('qda', QuadraticDiscriminantAnalysis())])\n",
    "\n",
    "score = cross_val_score(clf_qda, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.920     0.976     0.947       212\n",
      "   Malignant      0.975     0.915     0.944       212\n",
      "\n",
      "    accuracy                          0.946       424\n",
      "   macro avg      0.947     0.946     0.946       424\n",
      "weighted avg      0.947     0.946     0.946       424\n",
      "\n",
      "--- Time of execution : 24.637874603271484 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'qda__reg_param': np.linspace(0, 1, num=100),\n",
    "    'qda__tol': np.linspace(0, 0.01, num=100)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_qda, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6) [Ridge Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.933     0.991     0.961       212\n",
      "   Malignant      0.990     0.929     0.959       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.962     0.960     0.960       424\n",
      "weighted avg      0.962     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 0.07283449172973633 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_rc = Pipeline([('scaler', StandardScaler()), ('rg', RidgeClassifier())])\n",
    "\n",
    "score = cross_val_score(clf_rc, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.933     0.991     0.961       212\n",
      "   Malignant      0.990     0.929     0.959       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.962     0.960     0.960       424\n",
      "weighted avg      0.962     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 27.177467107772827 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'rg__alpha' : np.linspace(0, 1, num=10),\n",
    "    'rg__fit_intercept' : [True,False],\n",
    "    'rg__copy_X' : [True,False],\n",
    "    'rg__max_iter' : [None],\n",
    "    'rg__tol' : [0.001],\n",
    "    'rg__class_weight' : [None,'balanced'],\n",
    "    'rg__solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'rg__positive' : [False]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_rc, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7) [Decision Tree Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.923     0.906     0.914       212\n",
      "   Malignant      0.907     0.925     0.916       212\n",
      "\n",
      "    accuracy                          0.915       424\n",
      "   macro avg      0.915     0.915     0.915       424\n",
      "weighted avg      0.915     0.915     0.915       424\n",
      "\n",
      "--- Time of execution : 0.09326569999984713 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_tree = Pipeline([('scaler', StandardScaler()), ('tree', DecisionTreeClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_tree, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.934     0.934     0.934       212\n",
      "   Malignant      0.934     0.934     0.934       212\n",
      "\n",
      "    accuracy                          0.934       424\n",
      "   macro avg      0.934     0.934     0.934       424\n",
      "weighted avg      0.934     0.934     0.934       424\n",
      "\n",
      "--- Time of execution : 54.568707800000084 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "param_grid = {\n",
    "    'tree__criterion' :['gini','entropy'],\n",
    "    'tree__splitter' : ['best','random'],\n",
    "    'tree__max_depth': [list(range(2, 20)),None],\n",
    "    'tree__min_samples_split': list(range(2, 6)),\n",
    "    'tree__min_samples_leaf': list(range(1, 8)),\n",
    "    'tree__min_weight_fraction_leaf' : [0.0],\n",
    "    'tree__max_features': [None, 'sqrt', 'log2'],\n",
    "    'tree__max_leaf_nodes' : [None],\n",
    "    'tree__min_impurity_decrease' : [0.0],\n",
    "    'tree__class_weight' : [None,'balanced'],\n",
    "    'tree__ccp_alpha' : [0.0],\n",
    "    'tree__random_state' : [13]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_tree, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=1000)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters :  {'tree__splitter': 'best', 'tree__random_state': 13, 'tree__min_weight_fraction_leaf': 0.0, 'tree__min_samples_split': 2, 'tree__min_samples_leaf': 2, 'tree__min_impurity_decrease': 0.0, 'tree__max_leaf_nodes': None, 'tree__max_features': None, 'tree__max_depth': None, 'tree__criterion': 'gini', 'tree__class_weight': None, 'tree__ccp_alpha': 0.0}\n",
      "\n",
      "Best estimator :  Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('tree',\n",
      "                 DecisionTreeClassifier(min_samples_leaf=2, random_state=13))])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tree__splitter</th>\n",
       "      <th>param_tree__random_state</th>\n",
       "      <th>param_tree__min_weight_fraction_leaf</th>\n",
       "      <th>param_tree__min_samples_split</th>\n",
       "      <th>param_tree__min_samples_leaf</th>\n",
       "      <th>param_tree__min_impurity_decrease</th>\n",
       "      <th>param_tree__max_leaf_nodes</th>\n",
       "      <th>param_tree__max_features</th>\n",
       "      <th>param_tree__max_depth</th>\n",
       "      <th>param_tree__criterion</th>\n",
       "      <th>param_tree__class_weight</th>\n",
       "      <th>param_tree__ccp_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.964686</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.952882</td>\n",
       "      <td>0.917464</td>\n",
       "      <td>0.9404</td>\n",
       "      <td>0.940951</td>\n",
       "      <td>0.01669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.964686</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.952882</td>\n",
       "      <td>0.917464</td>\n",
       "      <td>0.9404</td>\n",
       "      <td>0.940951</td>\n",
       "      <td>0.01669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.964686</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.952882</td>\n",
       "      <td>0.917464</td>\n",
       "      <td>0.9404</td>\n",
       "      <td>0.940951</td>\n",
       "      <td>0.01669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "612       0.003391      0.000489         0.001596        0.000489   \n",
       "61        0.003391      0.000488         0.001795        0.000399   \n",
       "148       0.003789      0.000747         0.002593        0.001353   \n",
       "\n",
       "    param_tree__splitter param_tree__random_state  \\\n",
       "612                 best                       13   \n",
       "61                  best                       13   \n",
       "148                 best                       13   \n",
       "\n",
       "    param_tree__min_weight_fraction_leaf param_tree__min_samples_split  \\\n",
       "612                                  0.0                             4   \n",
       "61                                   0.0                             2   \n",
       "148                                  0.0                             3   \n",
       "\n",
       "    param_tree__min_samples_leaf param_tree__min_impurity_decrease  \\\n",
       "612                            2                               0.0   \n",
       "61                             2                               0.0   \n",
       "148                            2                               0.0   \n",
       "\n",
       "    param_tree__max_leaf_nodes param_tree__max_features param_tree__max_depth  \\\n",
       "612                       None                     None                  None   \n",
       "61                        None                     None                  None   \n",
       "148                       None                     None                  None   \n",
       "\n",
       "    param_tree__criterion param_tree__class_weight param_tree__ccp_alpha  \\\n",
       "612                  gini                     None                   0.0   \n",
       "61                   gini                     None                   0.0   \n",
       "148                  gini                     None                   0.0   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "612  {'tree__splitter': 'best', 'tree__random_state...           0.964686   \n",
       "61   {'tree__splitter': 'best', 'tree__random_state...           0.964686   \n",
       "148  {'tree__splitter': 'best', 'tree__random_state...           0.964686   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "612           0.929324           0.952882           0.917464   \n",
       "61            0.929324           0.952882           0.917464   \n",
       "148           0.929324           0.952882           0.917464   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "612             0.9404         0.940951         0.01669                1  \n",
       "61              0.9404         0.940951         0.01669                1  \n",
       "148             0.9404         0.940951         0.01669                1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(x_new, y) # we need this for adaboost\n",
    "\n",
    "print_best_params(search)\n",
    "search_results = pd.DataFrame(search.cv_results_)\n",
    "search_results.sort_values(by='mean_test_score',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8) [Random Forest Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.958     0.960       212\n",
      "   Malignant      0.958     0.962     0.960       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.960     0.960     0.960       424\n",
      "weighted avg      0.960     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 1.1729044999992766 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_rf = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=13))])\n",
    "                       \n",
    "score = cross_val_score(clf_rf, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.perf_counter() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'rf__bootstrap': [True,False],\n",
    "    'rf__max_depth': [list(range(5,15)), None],\n",
    "    'rf__n_estimators' :[100],\n",
    "    'rf__max_features': [None, 'sqrt', 'log2'],\n",
    "    'rf__max_leaf_nodes' : [None,list(range(5,15))],\n",
    "    'rf__min_samples_leaf': list(range(1,10)),\n",
    "    'rf__min_samples_split': list(range(2, 6)),\n",
    "    'rf__criterion' :['entropy','gini'],\n",
    "    'rf__random_state' : [13]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_rf, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.958     0.960       212\n",
      "   Malignant      0.958     0.962     0.960       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.960     0.960     0.960       424\n",
      "weighted avg      0.960     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 153.84320902824402 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9) [ADA Boost Classifier (Adaptive Boosting)](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#:~:text=An%20AdaBoost%20%5B1%5D%20classifier%20is,focus%20more%20on%20difficult%20cases.>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.967     0.981     0.974       212\n",
      "   Malignant      0.981     0.967     0.974       212\n",
      "\n",
      "    accuracy                          0.974       424\n",
      "   macro avg      0.974     0.974     0.974       424\n",
      "weighted avg      0.974     0.974     0.974       424\n",
      "\n",
      "--- Time of execution : 0.7496328353881836 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_adaboost = Pipeline([('scaler', StandardScaler()), ('adab', AdaBoostClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_adaboost, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.966     0.948     0.957       212\n",
      "   Malignant      0.949     0.967     0.958       212\n",
      "\n",
      "    accuracy                          0.958       424\n",
      "   macro avg      0.958     0.958     0.958       424\n",
      "weighted avg      0.958     0.958     0.958       424\n",
      "\n",
      "--- Time of execution : 162.77391934394836 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'adab__base_estimator' : [DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=3,random_state=13)],\n",
    "    'adab__n_estimators' : np.arange(100,210,10),\n",
    "    'adab__learning_rate' : np.power(10, np.arange(-3, 1, dtype=float)),\n",
    "    'adab__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "    'adab__random_state' : [13],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_adaboost, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10) [C-Support Vector Classification](<https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.967     0.965       212\n",
      "   Malignant      0.967     0.962     0.965       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.965     0.965     0.965       424\n",
      "weighted avg      0.965     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 0.07473206520080566 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_svc = Pipeline([('scaler', StandardScaler()),('svc', SVC())])\n",
    "\n",
    "score = cross_val_score(clf_svc, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.967     0.965       212\n",
      "   Malignant      0.967     0.962     0.965       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.965     0.965     0.965       424\n",
      "weighted avg      0.965     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 6.841865539550781 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = [\n",
    "    {\n",
    "        'svc__kernel': ['rbf'], \n",
    "        'svc__gamma': [1,1e-1,1e-2, 1e-3, 1e-4,'auto','scale'], \n",
    "        'svc__C': [1, 10, 100, 1000],\n",
    "        'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "        'svc__random_state' : [13]\n",
    "    },\n",
    "    {\n",
    "        'svc__kernel': ['linear'], \n",
    "        'svc__C': [1, 10, 100, 1000],\n",
    "        'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "        'svc__random_state' : [13]\n",
    "    },\n",
    "]\n",
    "\n",
    "search = GridSearchCV(clf_svc, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## 11) [Stochastic Gradient Descent Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.967     0.965       212\n",
      "   Malignant      0.967     0.962     0.965       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.965     0.965     0.965       424\n",
      "weighted avg      0.965     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 0.06382966041564941 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_sgd = Pipeline([('scaler', StandardScaler()), ('sgd', SGDClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.959     0.981     0.970       212\n",
      "   Malignant      0.981     0.958     0.969       212\n",
      "\n",
      "    accuracy                          0.969       424\n",
      "   macro avg      0.970     0.969     0.969       424\n",
      "weighted avg      0.970     0.969     0.969       424\n",
      "\n",
      "--- Time of execution : 3.4553422927856445 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'sgd__average': [True, False],\n",
    "    'sgd__l1_ratio': np.linspace(0, 1, num=10),\n",
    "    'sgd__alpha': np.power(10, np.arange(-2, 1, dtype=float)),\n",
    "    'sgd__random_state' : [13]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_sgd, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## 12) [eXtreme Gradient Boosting](<https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.953     0.948     0.950       212\n",
      "   Malignant      0.948     0.953     0.951       212\n",
      "\n",
      "    accuracy                          0.950       424\n",
      "   macro avg      0.950     0.950     0.950       424\n",
      "weighted avg      0.950     0.950     0.950       424\n",
      "\n",
      "--- Time of execution : 0.5654869079589844 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_xgb = Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_xgb, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018sp/lectures/lecturenote19.html\n",
    "# https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6\n",
    "\n",
    "start = time.time()\n",
    "param_grid = {\n",
    "        'xgb__booster' : ['gbtree'],\n",
    "        'xgb__validate_parameters' : [True],\n",
    "        'xgb__learning_rate' : np.arange(0.01,1.1,0.05),\n",
    "        'xgb__gamma' : np.arange(0,1.05,0.05),\n",
    "        'xgb__max_depth' : np.arange(2,11,1),\n",
    "        'xgb__min_child_weight' : np.arange(1,6,1),\n",
    "        'xgb__max_delta_step' : np.arange(0,5,1),\n",
    "        'xgb__subsample' : [0.5],\n",
    "        'xgb__colsample_bylevel' : [1],\n",
    "        'xgb__colsample_bynode' : [1],\n",
    "        'xgb__colsample_bytree' : [1],\n",
    "        'xgb__reg_lambda' : [0,1],\n",
    "        'xgb__reg_alpha' : [0],\n",
    "        'xgb__tree_method' : ['exact'],\n",
    "        'xgb__scale_pos_weight' : [1],\n",
    "        'xgb__objective' : ['binary:logistic'], # 'multi:softmax' -> same scores as 'binary:logistic' with grid search\n",
    "        #'num_class' : [2],\n",
    "        'xgb__n_estimators' : np.arange(100,210,10),\n",
    "        'xgb__random_state' : [13]\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf_xgb, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.958     0.972     0.965       212\n",
      "   Malignant      0.971     0.958     0.964       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.965     0.965     0.965       424\n",
      "weighted avg      0.965     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 225.42901968955994 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## 13) [Light Gradient Boosting Machine](<https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.957     0.953     0.955       212\n",
      "   Malignant      0.953     0.958     0.955       212\n",
      "\n",
      "    accuracy                          0.955       424\n",
      "   macro avg      0.955     0.955     0.955       424\n",
      "weighted avg      0.955     0.955     0.955       424\n",
      "\n",
      "--- Time of execution : 0.47136926651000977 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_lgbm = Pipeline([('scaler', StandardScaler()), ('lgbm', lgbm.LGBMClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_lgbm, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "# https://www.youtube.com/watch?v=5CWwwtEM2TA&ab_channel=PyData & https://github.com/MSusik/newgradientboosting/blob/master/pydata.pdf\n",
    "\n",
    "start = time.time()\n",
    "param_grid = {\n",
    "        'lgbm__boosting_type' : ['gbdt','dart'],\n",
    "        'lgbm__num_leaves' : np.arange(5,55,1),\n",
    "        'lgbm__max_depth' : np.arange(2,11,1),\n",
    "        'lgbm__learning_rate' : np.arange(0.01,1.1,0.05),\n",
    "        'lgbm__n_estimators' : np.arange(100,210,10),\n",
    "        'lgbm__objective' : ['binary'],\n",
    "        'lgbm__min_child_samples' : np.arange(10,35,5),\n",
    "        'lgbm__subsample' : [0.5],\n",
    "        'lgbm__reg_lambda' : [0,1],\n",
    "        'lgbm__reg_alpha' : [0],\n",
    "        'lgbm__subsample' : [0.5],\n",
    "        'lgbm__colsample_bytree' : [1],\n",
    "        'lgbm__scale_pos_weight' : [1],\n",
    "        'lgbm__random_state' : [13]\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf_lgbm, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.967     0.962     0.965       212\n",
      "   Malignant      0.962     0.967     0.965       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.965     0.965     0.965       424\n",
      "weighted avg      0.965     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 86.75096416473389 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## 14) [K-Nearest Neighbors Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.923     0.967     0.945       212\n",
      "   Malignant      0.965     0.920     0.942       212\n",
      "\n",
      "    accuracy                          0.943       424\n",
      "   macro avg      0.944     0.943     0.943       424\n",
      "weighted avg      0.944     0.943     0.943       424\n",
      "\n",
      "--- Time of execution : 0.08078384399414062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "score = cross_val_score(clf_knn, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.937     0.976     0.956       212\n",
      "   Malignant      0.975     0.934     0.954       212\n",
      "\n",
      "    accuracy                          0.955       424\n",
      "   macro avg      0.956     0.955     0.955       424\n",
      "weighted avg      0.956     0.955     0.955       424\n",
      "\n",
      "--- Time of execution : 80.44873404502869 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': list(range(3,10)),\n",
    "    'knn__weights': ['uniform','distance'],\n",
    "    'knn__algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__leaf_size': [10,20,30,40,50],\n",
    "    'knn__p': [1,2],\n",
    "    'knn__metric': ['minkowski','manhattan','chebyshev']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_knn, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='15'></a>\n",
    "## 15) [Multi-layer Perceptron Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.972     0.981     0.977       212\n",
      "   Malignant      0.981     0.972     0.976       212\n",
      "\n",
      "    accuracy                          0.976       424\n",
      "   macro avg      0.976     0.976     0.976       424\n",
      "weighted avg      0.976     0.976     0.976       424\n",
      "\n",
      "--- Time of execution : 3.5692391395568848 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_mlp =  Pipeline([('scaler', StandardScaler()),('mlp', MLPClassifier(shuffle=True,random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_mlp, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tried a wider range of hyperparameters in nested cross validation at first , but over testing, worst attempts were removed (those in comments). Finally, when few hyperparameters remained, they were tested separately with a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solver : ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.62199114\n",
      "Iteration 2, loss = 0.60553357\n",
      "Iteration 3, loss = 0.59031294\n",
      "Iteration 4, loss = 0.57582939\n",
      "Iteration 5, loss = 0.56200772\n",
      "Iteration 6, loss = 0.54887620\n",
      "Iteration 7, loss = 0.53654193\n",
      "Iteration 8, loss = 0.52462492\n",
      "Iteration 9, loss = 0.51320583\n",
      "Iteration 10, loss = 0.50246902\n",
      "Iteration 11, loss = 0.49264039\n",
      "Iteration 12, loss = 0.48287178\n",
      "Iteration 13, loss = 0.47412625\n",
      "Iteration 14, loss = 0.46527346\n",
      "Iteration 15, loss = 0.45728392\n",
      "Iteration 16, loss = 0.44931630\n",
      "Iteration 17, loss = 0.44221726\n",
      "Iteration 18, loss = 0.43518655\n",
      "Iteration 19, loss = 0.42834238\n",
      "Iteration 20, loss = 0.42177938\n",
      "Iteration 21, loss = 0.41572158\n",
      "Iteration 22, loss = 0.41001311\n",
      "Iteration 23, loss = 0.40423657\n",
      "Iteration 24, loss = 0.39881228\n",
      "Iteration 25, loss = 0.39372006\n",
      "Iteration 26, loss = 0.38871792\n",
      "Iteration 27, loss = 0.38377281\n",
      "Iteration 28, loss = 0.37918290\n",
      "Iteration 29, loss = 0.37484304\n",
      "Iteration 30, loss = 0.37049982\n",
      "Iteration 31, loss = 0.36639570\n",
      "Iteration 32, loss = 0.36245967\n",
      "Iteration 33, loss = 0.35876497\n",
      "Iteration 34, loss = 0.35495428\n",
      "Iteration 35, loss = 0.35151144\n",
      "Iteration 36, loss = 0.34802328\n",
      "Iteration 37, loss = 0.34471972\n",
      "Iteration 38, loss = 0.34152122\n",
      "Iteration 39, loss = 0.33839395\n",
      "Iteration 40, loss = 0.33531459\n",
      "Iteration 41, loss = 0.33242683\n",
      "Iteration 42, loss = 0.32958821\n",
      "Iteration 43, loss = 0.32683778\n",
      "Iteration 44, loss = 0.32419991\n",
      "Iteration 45, loss = 0.32158474\n",
      "Iteration 46, loss = 0.31909117\n",
      "Iteration 47, loss = 0.31665742\n",
      "Iteration 48, loss = 0.31423489\n",
      "Iteration 49, loss = 0.31182847\n",
      "Iteration 50, loss = 0.30968401\n",
      "Iteration 51, loss = 0.30747483\n",
      "Iteration 52, loss = 0.30526601\n",
      "Iteration 53, loss = 0.30316738\n",
      "Iteration 54, loss = 0.30106013\n",
      "Iteration 55, loss = 0.29908049\n",
      "Iteration 56, loss = 0.29713451\n",
      "Iteration 57, loss = 0.29517329\n",
      "Iteration 58, loss = 0.29326530\n",
      "Iteration 59, loss = 0.29147043\n",
      "Iteration 60, loss = 0.28968289\n",
      "Iteration 61, loss = 0.28783471\n",
      "Iteration 62, loss = 0.28617826\n",
      "Iteration 63, loss = 0.28453067\n",
      "Iteration 64, loss = 0.28285886\n",
      "Iteration 65, loss = 0.28124478\n",
      "Iteration 66, loss = 0.27964980\n",
      "Iteration 67, loss = 0.27807097\n",
      "Iteration 68, loss = 0.27659776\n",
      "Iteration 69, loss = 0.27512715\n",
      "Iteration 70, loss = 0.27371305\n",
      "Iteration 71, loss = 0.27222773\n",
      "Iteration 72, loss = 0.27089821\n",
      "Iteration 73, loss = 0.26943387\n",
      "Iteration 74, loss = 0.26821767\n",
      "Iteration 75, loss = 0.26682250\n",
      "Iteration 76, loss = 0.26549971\n",
      "Iteration 77, loss = 0.26423862\n",
      "Iteration 78, loss = 0.26296348\n",
      "Iteration 79, loss = 0.26174271\n",
      "Iteration 80, loss = 0.26055252\n",
      "Iteration 81, loss = 0.25935680\n",
      "Iteration 82, loss = 0.25814807\n",
      "Iteration 83, loss = 0.25696228\n",
      "Iteration 84, loss = 0.25581096\n",
      "Iteration 85, loss = 0.25464882\n",
      "Iteration 86, loss = 0.25349738\n",
      "Iteration 87, loss = 0.25246468\n",
      "Iteration 88, loss = 0.25129935\n",
      "Iteration 89, loss = 0.25020074\n",
      "Iteration 90, loss = 0.24912861\n",
      "Iteration 91, loss = 0.24813843\n",
      "Iteration 92, loss = 0.24706955\n",
      "Iteration 93, loss = 0.24604398\n",
      "Iteration 94, loss = 0.24500526\n",
      "Iteration 95, loss = 0.24406067\n",
      "Iteration 96, loss = 0.24305790\n",
      "Iteration 97, loss = 0.24209064\n",
      "Iteration 98, loss = 0.24111366\n",
      "Iteration 99, loss = 0.24015300\n",
      "Iteration 100, loss = 0.23923493\n",
      "Iteration 101, loss = 0.23830268\n",
      "Iteration 102, loss = 0.23738381\n",
      "Iteration 103, loss = 0.23644405\n",
      "Iteration 104, loss = 0.23554029\n",
      "Iteration 105, loss = 0.23463065\n",
      "Iteration 106, loss = 0.23380870\n",
      "Iteration 107, loss = 0.23288173\n",
      "Iteration 108, loss = 0.23203510\n",
      "Iteration 109, loss = 0.23121855\n",
      "Iteration 110, loss = 0.23033913\n",
      "Iteration 111, loss = 0.22954296\n",
      "Iteration 112, loss = 0.22870384\n",
      "Iteration 113, loss = 0.22790822\n",
      "Iteration 114, loss = 0.22712833\n",
      "Iteration 115, loss = 0.22632754\n",
      "Iteration 116, loss = 0.22559777\n",
      "Iteration 117, loss = 0.22490012\n",
      "Iteration 118, loss = 0.22412328\n",
      "Iteration 119, loss = 0.22345097\n",
      "Iteration 120, loss = 0.22272815\n",
      "Iteration 121, loss = 0.22201370\n",
      "Iteration 122, loss = 0.22133160\n",
      "Iteration 123, loss = 0.22066211\n",
      "Iteration 124, loss = 0.21997236\n",
      "Iteration 125, loss = 0.21934714\n",
      "Iteration 126, loss = 0.21867019\n",
      "Iteration 127, loss = 0.21807707\n",
      "Iteration 128, loss = 0.21735835\n",
      "Iteration 129, loss = 0.21675061\n",
      "Iteration 130, loss = 0.21611622\n",
      "Iteration 131, loss = 0.21547589\n",
      "Iteration 132, loss = 0.21492048\n",
      "Iteration 133, loss = 0.21422919\n",
      "Iteration 134, loss = 0.21364969\n",
      "Iteration 135, loss = 0.21307314\n",
      "Iteration 136, loss = 0.21242564\n",
      "Iteration 137, loss = 0.21185796\n",
      "Iteration 138, loss = 0.21129678\n",
      "Iteration 139, loss = 0.21067808\n",
      "Iteration 140, loss = 0.21013673\n",
      "Iteration 141, loss = 0.20957607\n",
      "Iteration 142, loss = 0.20898974\n",
      "Iteration 143, loss = 0.20840505\n",
      "Iteration 144, loss = 0.20781234\n",
      "Iteration 145, loss = 0.20724248\n",
      "Iteration 146, loss = 0.20669661\n",
      "Iteration 147, loss = 0.20612106\n",
      "Iteration 148, loss = 0.20553555\n",
      "Iteration 149, loss = 0.20499372\n",
      "Iteration 150, loss = 0.20448375\n",
      "Iteration 151, loss = 0.20392010\n",
      "Iteration 152, loss = 0.20338040\n",
      "Iteration 153, loss = 0.20286078\n",
      "Iteration 154, loss = 0.20235831\n",
      "Iteration 155, loss = 0.20186734\n",
      "Iteration 156, loss = 0.20138052\n",
      "Iteration 157, loss = 0.20082697\n",
      "Iteration 158, loss = 0.20032817\n",
      "Iteration 159, loss = 0.19986512\n",
      "Iteration 160, loss = 0.19934556\n",
      "Iteration 161, loss = 0.19890793\n",
      "Iteration 162, loss = 0.19842273\n",
      "Iteration 163, loss = 0.19795378\n",
      "Iteration 164, loss = 0.19747424\n",
      "Iteration 165, loss = 0.19703480\n",
      "Iteration 166, loss = 0.19658380\n",
      "Iteration 167, loss = 0.19615089\n",
      "Iteration 168, loss = 0.19572666\n",
      "Iteration 169, loss = 0.19527196\n",
      "Iteration 170, loss = 0.19490205\n",
      "Iteration 171, loss = 0.19444402\n",
      "Iteration 172, loss = 0.19404280\n",
      "Iteration 173, loss = 0.19367163\n",
      "Iteration 174, loss = 0.19326317\n",
      "Iteration 175, loss = 0.19290878\n",
      "Iteration 176, loss = 0.19248677\n",
      "Iteration 177, loss = 0.19210562\n",
      "Iteration 178, loss = 0.19175635\n",
      "Iteration 179, loss = 0.19138815\n",
      "Iteration 180, loss = 0.19103945\n",
      "Iteration 181, loss = 0.19065264\n",
      "Iteration 182, loss = 0.19037451\n",
      "Iteration 183, loss = 0.19001392\n",
      "Iteration 184, loss = 0.18963746\n",
      "Iteration 185, loss = 0.18931310\n",
      "Iteration 186, loss = 0.18899519\n",
      "Iteration 187, loss = 0.18868168\n",
      "Iteration 188, loss = 0.18836246\n",
      "Iteration 189, loss = 0.18804461\n",
      "Iteration 190, loss = 0.18770827\n",
      "Iteration 191, loss = 0.18741004\n",
      "Iteration 192, loss = 0.18710456\n",
      "Iteration 193, loss = 0.18680499\n",
      "Iteration 194, loss = 0.18646386\n",
      "Iteration 195, loss = 0.18618694\n",
      "Iteration 196, loss = 0.18587950\n",
      "Iteration 197, loss = 0.18563954\n",
      "Iteration 198, loss = 0.18531919\n",
      "Iteration 199, loss = 0.18501462\n",
      "Iteration 200, loss = 0.18474072\n",
      "Iteration 201, loss = 0.18447139\n",
      "Iteration 202, loss = 0.18421017\n",
      "Iteration 203, loss = 0.18394542\n",
      "Iteration 204, loss = 0.18368489\n",
      "Iteration 205, loss = 0.18341888\n",
      "Iteration 206, loss = 0.18315653\n",
      "Iteration 207, loss = 0.18292462\n",
      "Iteration 208, loss = 0.18264458\n",
      "Iteration 209, loss = 0.18238631\n",
      "Iteration 210, loss = 0.18212176\n",
      "Iteration 211, loss = 0.18188073\n",
      "Iteration 212, loss = 0.18164046\n",
      "Iteration 213, loss = 0.18138673\n",
      "Iteration 214, loss = 0.18114623\n",
      "Iteration 215, loss = 0.18091614\n",
      "Iteration 216, loss = 0.18067073\n",
      "Iteration 217, loss = 0.18045947\n",
      "Iteration 218, loss = 0.18020239\n",
      "Iteration 219, loss = 0.17996191\n",
      "Iteration 220, loss = 0.17974225\n",
      "Iteration 221, loss = 0.17954857\n",
      "Iteration 222, loss = 0.17932133\n",
      "Iteration 223, loss = 0.17908993\n",
      "Iteration 224, loss = 0.17889722\n",
      "Iteration 225, loss = 0.17868305\n",
      "Iteration 226, loss = 0.17844252\n",
      "Iteration 227, loss = 0.17824999\n",
      "Iteration 228, loss = 0.17804098\n",
      "Iteration 229, loss = 0.17782203\n",
      "Iteration 230, loss = 0.17760792\n",
      "Iteration 231, loss = 0.17742014\n",
      "Iteration 232, loss = 0.17717369\n",
      "Iteration 233, loss = 0.17700657\n",
      "Iteration 234, loss = 0.17677624\n",
      "Iteration 235, loss = 0.17661621\n",
      "Iteration 236, loss = 0.17636177\n",
      "Iteration 237, loss = 0.17615814\n",
      "Iteration 238, loss = 0.17598202\n",
      "Iteration 239, loss = 0.17575198\n",
      "Iteration 240, loss = 0.17557058\n",
      "Iteration 241, loss = 0.17537359\n",
      "Iteration 242, loss = 0.17516408\n",
      "Iteration 243, loss = 0.17500509\n",
      "Iteration 244, loss = 0.17477917\n",
      "Iteration 245, loss = 0.17461644\n",
      "Iteration 246, loss = 0.17442080\n",
      "Iteration 247, loss = 0.17420576\n",
      "Iteration 248, loss = 0.17401848\n",
      "Iteration 249, loss = 0.17382738\n",
      "Iteration 250, loss = 0.17364268\n",
      "Iteration 251, loss = 0.17341854\n",
      "Iteration 252, loss = 0.17323944\n",
      "Iteration 253, loss = 0.17302811\n",
      "Iteration 254, loss = 0.17280762\n",
      "Iteration 255, loss = 0.17260907\n",
      "Iteration 256, loss = 0.17240868\n",
      "Iteration 257, loss = 0.17222277\n",
      "Iteration 258, loss = 0.17202251\n",
      "Iteration 259, loss = 0.17181787\n",
      "Iteration 260, loss = 0.17165488\n",
      "Iteration 261, loss = 0.17150320\n",
      "Iteration 262, loss = 0.17127516\n",
      "Iteration 263, loss = 0.17108901\n",
      "Iteration 264, loss = 0.17096931\n",
      "Iteration 265, loss = 0.17073098\n",
      "Iteration 266, loss = 0.17054288\n",
      "Iteration 267, loss = 0.17034545\n",
      "Iteration 268, loss = 0.17016963\n",
      "Iteration 269, loss = 0.17001674\n",
      "Iteration 270, loss = 0.16981183\n",
      "Iteration 271, loss = 0.16965839\n",
      "Iteration 272, loss = 0.16949103\n",
      "Iteration 273, loss = 0.16930504\n",
      "Iteration 274, loss = 0.16913393\n",
      "Iteration 275, loss = 0.16897466\n",
      "Iteration 276, loss = 0.16879302\n",
      "Iteration 277, loss = 0.16859423\n",
      "Iteration 278, loss = 0.16841993\n",
      "Iteration 279, loss = 0.16823806\n",
      "Iteration 280, loss = 0.16805669\n",
      "Iteration 281, loss = 0.16789491\n",
      "Iteration 282, loss = 0.16772288\n",
      "Iteration 283, loss = 0.16757711\n",
      "Iteration 284, loss = 0.16741253\n",
      "Iteration 285, loss = 0.16728308\n",
      "Iteration 286, loss = 0.16710315\n",
      "Iteration 287, loss = 0.16693217\n",
      "Iteration 288, loss = 0.16678939\n",
      "Iteration 289, loss = 0.16662741\n",
      "Iteration 290, loss = 0.16645969\n",
      "Iteration 291, loss = 0.16629918\n",
      "Iteration 292, loss = 0.16617491\n",
      "Iteration 293, loss = 0.16601659\n",
      "Iteration 294, loss = 0.16586778\n",
      "Iteration 295, loss = 0.16576071\n",
      "Iteration 296, loss = 0.16559008\n",
      "Iteration 297, loss = 0.16543749\n",
      "Iteration 298, loss = 0.16530846\n",
      "Iteration 299, loss = 0.16516935\n",
      "Iteration 300, loss = 0.16502616\n",
      "Iteration 301, loss = 0.16493220\n",
      "Iteration 302, loss = 0.16478602\n",
      "Iteration 303, loss = 0.16469397\n",
      "Iteration 304, loss = 0.16452332\n",
      "Iteration 305, loss = 0.16441045\n",
      "Iteration 306, loss = 0.16431044\n",
      "Iteration 307, loss = 0.16417177\n",
      "Iteration 308, loss = 0.16405958\n",
      "Iteration 309, loss = 0.16397281\n",
      "Iteration 310, loss = 0.16383230\n",
      "Iteration 311, loss = 0.16371905\n",
      "Iteration 312, loss = 0.16359884\n",
      "Iteration 313, loss = 0.16352475\n",
      "Iteration 314, loss = 0.16342720\n",
      "Iteration 315, loss = 0.16333192\n",
      "Iteration 316, loss = 0.16319897\n",
      "Iteration 317, loss = 0.16313756\n",
      "Iteration 318, loss = 0.16304803\n",
      "Iteration 319, loss = 0.16290849\n",
      "Iteration 320, loss = 0.16283761\n",
      "Iteration 321, loss = 0.16274715\n",
      "Iteration 322, loss = 0.16264375\n",
      "Iteration 323, loss = 0.16257110\n",
      "Iteration 324, loss = 0.16253522\n",
      "Iteration 325, loss = 0.16236897\n",
      "Iteration 326, loss = 0.16230132\n",
      "Iteration 327, loss = 0.16221394\n",
      "Iteration 328, loss = 0.16215126\n",
      "Iteration 329, loss = 0.16206003\n",
      "Iteration 330, loss = 0.16200187\n",
      "Iteration 331, loss = 0.16191698\n",
      "Iteration 332, loss = 0.16177699\n",
      "Iteration 333, loss = 0.16170427\n",
      "Iteration 334, loss = 0.16161939\n",
      "Iteration 335, loss = 0.16154102\n",
      "Iteration 336, loss = 0.16143939\n",
      "Iteration 337, loss = 0.16137049\n",
      "Iteration 338, loss = 0.16128739\n",
      "Iteration 339, loss = 0.16119739\n",
      "Iteration 340, loss = 0.16113299\n",
      "Iteration 341, loss = 0.16106248\n",
      "Iteration 342, loss = 0.16101302\n",
      "Iteration 343, loss = 0.16092441\n",
      "Iteration 344, loss = 0.16082648\n",
      "Iteration 345, loss = 0.16076560\n",
      "Iteration 346, loss = 0.16071183\n",
      "Iteration 347, loss = 0.16061044\n",
      "Iteration 348, loss = 0.16056852\n",
      "Iteration 349, loss = 0.16048734\n",
      "Iteration 350, loss = 0.16045069\n",
      "Iteration 351, loss = 0.16033947\n",
      "Iteration 352, loss = 0.16027647\n",
      "Iteration 353, loss = 0.16021034\n",
      "Iteration 354, loss = 0.16017536\n",
      "Iteration 355, loss = 0.16008753\n",
      "Iteration 356, loss = 0.16000806\n",
      "Iteration 357, loss = 0.15996559\n",
      "Iteration 358, loss = 0.15993103\n",
      "Iteration 359, loss = 0.15980952\n",
      "Iteration 360, loss = 0.15977232\n",
      "Iteration 361, loss = 0.15971493\n",
      "Iteration 362, loss = 0.15963879\n",
      "Iteration 363, loss = 0.15958949\n",
      "Iteration 364, loss = 0.15955646\n",
      "Iteration 365, loss = 0.15949679\n",
      "Iteration 366, loss = 0.15940853\n",
      "Iteration 367, loss = 0.15935696\n",
      "Iteration 368, loss = 0.15928915\n",
      "Iteration 369, loss = 0.15927372\n",
      "Iteration 370, loss = 0.15922519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.62099949\n",
      "Iteration 2, loss = 0.60415317\n",
      "Iteration 3, loss = 0.58897209\n",
      "Iteration 4, loss = 0.57415890\n",
      "Iteration 5, loss = 0.56018428\n",
      "Iteration 6, loss = 0.54678000\n",
      "Iteration 7, loss = 0.53434325\n",
      "Iteration 8, loss = 0.52233292\n",
      "Iteration 9, loss = 0.51072115\n",
      "Iteration 10, loss = 0.49982535\n",
      "Iteration 11, loss = 0.48988990\n",
      "Iteration 12, loss = 0.47994846\n",
      "Iteration 13, loss = 0.47122377\n",
      "Iteration 14, loss = 0.46217848\n",
      "Iteration 15, loss = 0.45401244\n",
      "Iteration 16, loss = 0.44608749\n",
      "Iteration 17, loss = 0.43879707\n",
      "Iteration 18, loss = 0.43181106\n",
      "Iteration 19, loss = 0.42478756\n",
      "Iteration 20, loss = 0.41809346\n",
      "Iteration 21, loss = 0.41207278\n",
      "Iteration 22, loss = 0.40615796\n",
      "Iteration 23, loss = 0.40046209\n",
      "Iteration 24, loss = 0.39502967\n",
      "Iteration 25, loss = 0.38976524\n",
      "Iteration 26, loss = 0.38476213\n",
      "Iteration 27, loss = 0.37978681\n",
      "Iteration 28, loss = 0.37516694\n",
      "Iteration 29, loss = 0.37074536\n",
      "Iteration 30, loss = 0.36631977\n",
      "Iteration 31, loss = 0.36218283\n",
      "Iteration 32, loss = 0.35818548\n",
      "Iteration 33, loss = 0.35442276\n",
      "Iteration 34, loss = 0.35062207\n",
      "Iteration 35, loss = 0.34709329\n",
      "Iteration 36, loss = 0.34359767\n",
      "Iteration 37, loss = 0.34026014\n",
      "Iteration 38, loss = 0.33703546\n",
      "Iteration 39, loss = 0.33388419\n",
      "Iteration 40, loss = 0.33081165\n",
      "Iteration 41, loss = 0.32790106\n",
      "Iteration 42, loss = 0.32507435\n",
      "Iteration 43, loss = 0.32232839\n",
      "Iteration 44, loss = 0.31960390\n",
      "Iteration 45, loss = 0.31703106\n",
      "Iteration 46, loss = 0.31448756\n",
      "Iteration 47, loss = 0.31204855\n",
      "Iteration 48, loss = 0.30956961\n",
      "Iteration 49, loss = 0.30716789\n",
      "Iteration 50, loss = 0.30501033\n",
      "Iteration 51, loss = 0.30277318\n",
      "Iteration 52, loss = 0.30055291\n",
      "Iteration 53, loss = 0.29847588\n",
      "Iteration 54, loss = 0.29639594\n",
      "Iteration 55, loss = 0.29443970\n",
      "Iteration 56, loss = 0.29250574\n",
      "Iteration 57, loss = 0.29055858\n",
      "Iteration 58, loss = 0.28865013\n",
      "Iteration 59, loss = 0.28687186\n",
      "Iteration 60, loss = 0.28509381\n",
      "Iteration 61, loss = 0.28325953\n",
      "Iteration 62, loss = 0.28160871\n",
      "Iteration 63, loss = 0.27997047\n",
      "Iteration 64, loss = 0.27833419\n",
      "Iteration 65, loss = 0.27673516\n",
      "Iteration 66, loss = 0.27515655\n",
      "Iteration 67, loss = 0.27358157\n",
      "Iteration 68, loss = 0.27213141\n",
      "Iteration 69, loss = 0.27066320\n",
      "Iteration 70, loss = 0.26921827\n",
      "Iteration 71, loss = 0.26775740\n",
      "Iteration 72, loss = 0.26641590\n",
      "Iteration 73, loss = 0.26494184\n",
      "Iteration 74, loss = 0.26369365\n",
      "Iteration 75, loss = 0.26228567\n",
      "Iteration 76, loss = 0.26096529\n",
      "Iteration 77, loss = 0.25966865\n",
      "Iteration 78, loss = 0.25837346\n",
      "Iteration 79, loss = 0.25711520\n",
      "Iteration 80, loss = 0.25591631\n",
      "Iteration 81, loss = 0.25470033\n",
      "Iteration 82, loss = 0.25348370\n",
      "Iteration 83, loss = 0.25230677\n",
      "Iteration 84, loss = 0.25114144\n",
      "Iteration 85, loss = 0.24997721\n",
      "Iteration 86, loss = 0.24882692\n",
      "Iteration 87, loss = 0.24779185\n",
      "Iteration 88, loss = 0.24663275\n",
      "Iteration 89, loss = 0.24554415\n",
      "Iteration 90, loss = 0.24446791\n",
      "Iteration 91, loss = 0.24346022\n",
      "Iteration 92, loss = 0.24233167\n",
      "Iteration 93, loss = 0.24130445\n",
      "Iteration 94, loss = 0.24021600\n",
      "Iteration 95, loss = 0.23924560\n",
      "Iteration 96, loss = 0.23823644\n",
      "Iteration 97, loss = 0.23724104\n",
      "Iteration 98, loss = 0.23626217\n",
      "Iteration 99, loss = 0.23527199\n",
      "Iteration 100, loss = 0.23434680\n",
      "Iteration 101, loss = 0.23339763\n",
      "Iteration 102, loss = 0.23248139\n",
      "Iteration 103, loss = 0.23151746\n",
      "Iteration 104, loss = 0.23060615\n",
      "Iteration 105, loss = 0.22968341\n",
      "Iteration 106, loss = 0.22888573\n",
      "Iteration 107, loss = 0.22795999\n",
      "Iteration 108, loss = 0.22711001\n",
      "Iteration 109, loss = 0.22629877\n",
      "Iteration 110, loss = 0.22542037\n",
      "Iteration 111, loss = 0.22464825\n",
      "Iteration 112, loss = 0.22383009\n",
      "Iteration 113, loss = 0.22304955\n",
      "Iteration 114, loss = 0.22224944\n",
      "Iteration 115, loss = 0.22146752\n",
      "Iteration 116, loss = 0.22073640\n",
      "Iteration 117, loss = 0.22001397\n",
      "Iteration 118, loss = 0.21923074\n",
      "Iteration 119, loss = 0.21853740\n",
      "Iteration 120, loss = 0.21778422\n",
      "Iteration 121, loss = 0.21706561\n",
      "Iteration 122, loss = 0.21633994\n",
      "Iteration 123, loss = 0.21564043\n",
      "Iteration 124, loss = 0.21493785\n",
      "Iteration 125, loss = 0.21427314\n",
      "Iteration 126, loss = 0.21358351\n",
      "Iteration 127, loss = 0.21296618\n",
      "Iteration 128, loss = 0.21225163\n",
      "Iteration 129, loss = 0.21160879\n",
      "Iteration 130, loss = 0.21095162\n",
      "Iteration 131, loss = 0.21031956\n",
      "Iteration 132, loss = 0.20973177\n",
      "Iteration 133, loss = 0.20903446\n",
      "Iteration 134, loss = 0.20844216\n",
      "Iteration 135, loss = 0.20783133\n",
      "Iteration 136, loss = 0.20715899\n",
      "Iteration 137, loss = 0.20658119\n",
      "Iteration 138, loss = 0.20597146\n",
      "Iteration 139, loss = 0.20533165\n",
      "Iteration 140, loss = 0.20472966\n",
      "Iteration 141, loss = 0.20411990\n",
      "Iteration 142, loss = 0.20348586\n",
      "Iteration 143, loss = 0.20285391\n",
      "Iteration 144, loss = 0.20223221\n",
      "Iteration 145, loss = 0.20162412\n",
      "Iteration 146, loss = 0.20104373\n",
      "Iteration 147, loss = 0.20045943\n",
      "Iteration 148, loss = 0.19985717\n",
      "Iteration 149, loss = 0.19929940\n",
      "Iteration 150, loss = 0.19877037\n",
      "Iteration 151, loss = 0.19818926\n",
      "Iteration 152, loss = 0.19766128\n",
      "Iteration 153, loss = 0.19710957\n",
      "Iteration 154, loss = 0.19658995\n",
      "Iteration 155, loss = 0.19607801\n",
      "Iteration 156, loss = 0.19559365\n",
      "Iteration 157, loss = 0.19503011\n",
      "Iteration 158, loss = 0.19452623\n",
      "Iteration 159, loss = 0.19405702\n",
      "Iteration 160, loss = 0.19354737\n",
      "Iteration 161, loss = 0.19310385\n",
      "Iteration 162, loss = 0.19261095\n",
      "Iteration 163, loss = 0.19213332\n",
      "Iteration 164, loss = 0.19164832\n",
      "Iteration 165, loss = 0.19118974\n",
      "Iteration 166, loss = 0.19070937\n",
      "Iteration 167, loss = 0.19023256\n",
      "Iteration 168, loss = 0.18977042\n",
      "Iteration 169, loss = 0.18928242\n",
      "Iteration 170, loss = 0.18890247\n",
      "Iteration 171, loss = 0.18835921\n",
      "Iteration 172, loss = 0.18792688\n",
      "Iteration 173, loss = 0.18748576\n",
      "Iteration 174, loss = 0.18703968\n",
      "Iteration 175, loss = 0.18662919\n",
      "Iteration 176, loss = 0.18617400\n",
      "Iteration 177, loss = 0.18576033\n",
      "Iteration 178, loss = 0.18534490\n",
      "Iteration 179, loss = 0.18496245\n",
      "Iteration 180, loss = 0.18454299\n",
      "Iteration 181, loss = 0.18411808\n",
      "Iteration 182, loss = 0.18377433\n",
      "Iteration 183, loss = 0.18339780\n",
      "Iteration 184, loss = 0.18294708\n",
      "Iteration 185, loss = 0.18257769\n",
      "Iteration 186, loss = 0.18220455\n",
      "Iteration 187, loss = 0.18182712\n",
      "Iteration 188, loss = 0.18145686\n",
      "Iteration 189, loss = 0.18108915\n",
      "Iteration 190, loss = 0.18071569\n",
      "Iteration 191, loss = 0.18035598\n",
      "Iteration 192, loss = 0.18000268\n",
      "Iteration 193, loss = 0.17964841\n",
      "Iteration 194, loss = 0.17925693\n",
      "Iteration 195, loss = 0.17892304\n",
      "Iteration 196, loss = 0.17857317\n",
      "Iteration 197, loss = 0.17826629\n",
      "Iteration 198, loss = 0.17790783\n",
      "Iteration 199, loss = 0.17755150\n",
      "Iteration 200, loss = 0.17722161\n",
      "Iteration 201, loss = 0.17688850\n",
      "Iteration 202, loss = 0.17658183\n",
      "Iteration 203, loss = 0.17626563\n",
      "Iteration 204, loss = 0.17594260\n",
      "Iteration 205, loss = 0.17564275\n",
      "Iteration 206, loss = 0.17533216\n",
      "Iteration 207, loss = 0.17505230\n",
      "Iteration 208, loss = 0.17474571\n",
      "Iteration 209, loss = 0.17444206\n",
      "Iteration 210, loss = 0.17416441\n",
      "Iteration 211, loss = 0.17389091\n",
      "Iteration 212, loss = 0.17362281\n",
      "Iteration 213, loss = 0.17334156\n",
      "Iteration 214, loss = 0.17307550\n",
      "Iteration 215, loss = 0.17281488\n",
      "Iteration 216, loss = 0.17253147\n",
      "Iteration 217, loss = 0.17230071\n",
      "Iteration 218, loss = 0.17201311\n",
      "Iteration 219, loss = 0.17175712\n",
      "Iteration 220, loss = 0.17148669\n",
      "Iteration 221, loss = 0.17122863\n",
      "Iteration 222, loss = 0.17097181\n",
      "Iteration 223, loss = 0.17070948\n",
      "Iteration 224, loss = 0.17046375\n",
      "Iteration 225, loss = 0.17020360\n",
      "Iteration 226, loss = 0.16993016\n",
      "Iteration 227, loss = 0.16967590\n",
      "Iteration 228, loss = 0.16941764\n",
      "Iteration 229, loss = 0.16916198\n",
      "Iteration 230, loss = 0.16891094\n",
      "Iteration 231, loss = 0.16866149\n",
      "Iteration 232, loss = 0.16836924\n",
      "Iteration 233, loss = 0.16815225\n",
      "Iteration 234, loss = 0.16786002\n",
      "Iteration 235, loss = 0.16764871\n",
      "Iteration 236, loss = 0.16733928\n",
      "Iteration 237, loss = 0.16705332\n",
      "Iteration 238, loss = 0.16680847\n",
      "Iteration 239, loss = 0.16652302\n",
      "Iteration 240, loss = 0.16628826\n",
      "Iteration 241, loss = 0.16601459\n",
      "Iteration 242, loss = 0.16574875\n",
      "Iteration 243, loss = 0.16553297\n",
      "Iteration 244, loss = 0.16525507\n",
      "Iteration 245, loss = 0.16502466\n",
      "Iteration 246, loss = 0.16475232\n",
      "Iteration 247, loss = 0.16448201\n",
      "Iteration 248, loss = 0.16420704\n",
      "Iteration 249, loss = 0.16395600\n",
      "Iteration 250, loss = 0.16372941\n",
      "Iteration 251, loss = 0.16345189\n",
      "Iteration 252, loss = 0.16322282\n",
      "Iteration 253, loss = 0.16300982\n",
      "Iteration 254, loss = 0.16275301\n",
      "Iteration 255, loss = 0.16251486\n",
      "Iteration 256, loss = 0.16229391\n",
      "Iteration 257, loss = 0.16207809\n",
      "Iteration 258, loss = 0.16184126\n",
      "Iteration 259, loss = 0.16161247\n",
      "Iteration 260, loss = 0.16141920\n",
      "Iteration 261, loss = 0.16120837\n",
      "Iteration 262, loss = 0.16097495\n",
      "Iteration 263, loss = 0.16075030\n",
      "Iteration 264, loss = 0.16059063\n",
      "Iteration 265, loss = 0.16033192\n",
      "Iteration 266, loss = 0.16013194\n",
      "Iteration 267, loss = 0.15993343\n",
      "Iteration 268, loss = 0.15974681\n",
      "Iteration 269, loss = 0.15960508\n",
      "Iteration 270, loss = 0.15939965\n",
      "Iteration 271, loss = 0.15922533\n",
      "Iteration 272, loss = 0.15906770\n",
      "Iteration 273, loss = 0.15888504\n",
      "Iteration 274, loss = 0.15873215\n",
      "Iteration 275, loss = 0.15858840\n",
      "Iteration 276, loss = 0.15841415\n",
      "Iteration 277, loss = 0.15826286\n",
      "Iteration 278, loss = 0.15811470\n",
      "Iteration 279, loss = 0.15795526\n",
      "Iteration 280, loss = 0.15778969\n",
      "Iteration 281, loss = 0.15763973\n",
      "Iteration 282, loss = 0.15751102\n",
      "Iteration 283, loss = 0.15737804\n",
      "Iteration 284, loss = 0.15724402\n",
      "Iteration 285, loss = 0.15714373\n",
      "Iteration 286, loss = 0.15696987\n",
      "Iteration 287, loss = 0.15684552\n",
      "Iteration 288, loss = 0.15675115\n",
      "Iteration 289, loss = 0.15659824\n",
      "Iteration 290, loss = 0.15648333\n",
      "Iteration 291, loss = 0.15634550\n",
      "Iteration 292, loss = 0.15624989\n",
      "Iteration 293, loss = 0.15613918\n",
      "Iteration 294, loss = 0.15598612\n",
      "Iteration 295, loss = 0.15590076\n",
      "Iteration 296, loss = 0.15576887\n",
      "Iteration 297, loss = 0.15565000\n",
      "Iteration 298, loss = 0.15553515\n",
      "Iteration 299, loss = 0.15541695\n",
      "Iteration 300, loss = 0.15529415\n",
      "Iteration 301, loss = 0.15522610\n",
      "Iteration 302, loss = 0.15508928\n",
      "Iteration 303, loss = 0.15501558\n",
      "Iteration 304, loss = 0.15486907\n",
      "Iteration 305, loss = 0.15478211\n",
      "Iteration 306, loss = 0.15467637\n",
      "Iteration 307, loss = 0.15456436\n",
      "Iteration 308, loss = 0.15447870\n",
      "Iteration 309, loss = 0.15440563\n",
      "Iteration 310, loss = 0.15427163\n",
      "Iteration 311, loss = 0.15417489\n",
      "Iteration 312, loss = 0.15408045\n",
      "Iteration 313, loss = 0.15400953\n",
      "Iteration 314, loss = 0.15392409\n",
      "Iteration 315, loss = 0.15384136\n",
      "Iteration 316, loss = 0.15372726\n",
      "Iteration 317, loss = 0.15367473\n",
      "Iteration 318, loss = 0.15358300\n",
      "Iteration 319, loss = 0.15346251\n",
      "Iteration 320, loss = 0.15339631\n",
      "Iteration 321, loss = 0.15330913\n",
      "Iteration 322, loss = 0.15322989\n",
      "Iteration 323, loss = 0.15314982\n",
      "Iteration 324, loss = 0.15310715\n",
      "Iteration 325, loss = 0.15296829\n",
      "Iteration 326, loss = 0.15292238\n",
      "Iteration 327, loss = 0.15284826\n",
      "Iteration 328, loss = 0.15277353\n",
      "Iteration 329, loss = 0.15269845\n",
      "Iteration 330, loss = 0.15262172\n",
      "Iteration 331, loss = 0.15255706\n",
      "Iteration 332, loss = 0.15244961\n",
      "Iteration 333, loss = 0.15239042\n",
      "Iteration 334, loss = 0.15230473\n",
      "Iteration 335, loss = 0.15224193\n",
      "Iteration 336, loss = 0.15215728\n",
      "Iteration 337, loss = 0.15209874\n",
      "Iteration 338, loss = 0.15200793\n",
      "Iteration 339, loss = 0.15191253\n",
      "Iteration 340, loss = 0.15183756\n",
      "Iteration 341, loss = 0.15175521\n",
      "Iteration 342, loss = 0.15170350\n",
      "Iteration 343, loss = 0.15160597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.57577744\n",
      "Iteration 2, loss = 0.45493591\n",
      "Iteration 3, loss = 0.38040574\n",
      "Iteration 4, loss = 0.33545881\n",
      "Iteration 5, loss = 0.30206219\n",
      "Iteration 6, loss = 0.28073343\n",
      "Iteration 7, loss = 0.26551449\n",
      "Iteration 8, loss = 0.25230899\n",
      "Iteration 9, loss = 0.24147319\n",
      "Iteration 10, loss = 0.23217141\n",
      "Iteration 11, loss = 0.22366131\n",
      "Iteration 12, loss = 0.21685030\n",
      "Iteration 13, loss = 0.21034257\n",
      "Iteration 14, loss = 0.20488144\n",
      "Iteration 15, loss = 0.19893276\n",
      "Iteration 16, loss = 0.19390722\n",
      "Iteration 17, loss = 0.18937056\n",
      "Iteration 18, loss = 0.18550634\n",
      "Iteration 19, loss = 0.18164307\n",
      "Iteration 20, loss = 0.17821851\n",
      "Iteration 21, loss = 0.17550082\n",
      "Iteration 22, loss = 0.17265929\n",
      "Iteration 23, loss = 0.17013090\n",
      "Iteration 24, loss = 0.16820896\n",
      "Iteration 25, loss = 0.16631966\n",
      "Iteration 26, loss = 0.16473586\n",
      "Iteration 27, loss = 0.16291021\n",
      "Iteration 28, loss = 0.16180885\n",
      "Iteration 29, loss = 0.16024987\n",
      "Iteration 30, loss = 0.15903696\n",
      "Iteration 31, loss = 0.15778472\n",
      "Iteration 32, loss = 0.15674788\n",
      "Iteration 33, loss = 0.15575296\n",
      "Iteration 34, loss = 0.15458266\n",
      "Iteration 35, loss = 0.15363784\n",
      "Iteration 36, loss = 0.15281264\n",
      "Iteration 37, loss = 0.15205752\n",
      "Iteration 38, loss = 0.15162629\n",
      "Iteration 39, loss = 0.15085096\n",
      "Iteration 40, loss = 0.15026516\n",
      "Iteration 41, loss = 0.14990880\n",
      "Iteration 42, loss = 0.14943466\n",
      "Iteration 43, loss = 0.14933328\n",
      "Iteration 44, loss = 0.14907628\n",
      "Iteration 45, loss = 0.14849210\n",
      "Iteration 46, loss = 0.14860429\n",
      "Iteration 47, loss = 0.14816910\n",
      "Iteration 48, loss = 0.14792867\n",
      "Iteration 49, loss = 0.14790881\n",
      "Iteration 50, loss = 0.14758087\n",
      "Iteration 51, loss = 0.14744567\n",
      "Iteration 52, loss = 0.14738059\n",
      "Iteration 53, loss = 0.14721787\n",
      "Iteration 54, loss = 0.14740877\n",
      "Iteration 55, loss = 0.14704584\n",
      "Iteration 56, loss = 0.14749903\n",
      "Iteration 57, loss = 0.14704722\n",
      "Iteration 58, loss = 0.14692863\n",
      "Iteration 59, loss = 0.14656548\n",
      "Iteration 60, loss = 0.14663193\n",
      "Iteration 61, loss = 0.14632360\n",
      "Iteration 62, loss = 0.14612798\n",
      "Iteration 63, loss = 0.14620054\n",
      "Iteration 64, loss = 0.14598541\n",
      "Iteration 65, loss = 0.14641417\n",
      "Iteration 66, loss = 0.14622131\n",
      "Iteration 67, loss = 0.14657264\n",
      "Iteration 68, loss = 0.14603207\n",
      "Iteration 69, loss = 0.14614689\n",
      "Iteration 70, loss = 0.14570936\n",
      "Iteration 71, loss = 0.14563104\n",
      "Iteration 72, loss = 0.14575990\n",
      "Iteration 73, loss = 0.14564100\n",
      "Iteration 74, loss = 0.14593451\n",
      "Iteration 75, loss = 0.14531414\n",
      "Iteration 76, loss = 0.14539596\n",
      "Iteration 77, loss = 0.14551059\n",
      "Iteration 78, loss = 0.14549938\n",
      "Iteration 79, loss = 0.14554655\n",
      "Iteration 80, loss = 0.14523703\n",
      "Iteration 81, loss = 0.14540366\n",
      "Iteration 82, loss = 0.14540127\n",
      "Iteration 83, loss = 0.14537621\n",
      "Iteration 84, loss = 0.14509346\n",
      "Iteration 85, loss = 0.14519687\n",
      "Iteration 86, loss = 0.14514948\n",
      "Iteration 87, loss = 0.14526557\n",
      "Iteration 88, loss = 0.14531893\n",
      "Iteration 89, loss = 0.14560138\n",
      "Iteration 90, loss = 0.14551440\n",
      "Iteration 91, loss = 0.14612222\n",
      "Iteration 92, loss = 0.14528832\n",
      "Iteration 93, loss = 0.14506333\n",
      "Iteration 94, loss = 0.14555918\n",
      "Iteration 95, loss = 0.14495654\n",
      "Iteration 96, loss = 0.14503326\n",
      "Iteration 97, loss = 0.14520822\n",
      "Iteration 98, loss = 0.14507088\n",
      "Iteration 99, loss = 0.14519436\n",
      "Iteration 100, loss = 0.14553155\n",
      "Iteration 101, loss = 0.14537204\n",
      "Iteration 102, loss = 0.14505621\n",
      "Iteration 103, loss = 0.14494043\n",
      "Iteration 104, loss = 0.14518367\n",
      "Iteration 105, loss = 0.14510611\n",
      "Iteration 106, loss = 0.14480158\n",
      "Iteration 107, loss = 0.14493697\n",
      "Iteration 108, loss = 0.14497754\n",
      "Iteration 109, loss = 0.14505952\n",
      "Iteration 110, loss = 0.14492072\n",
      "Iteration 111, loss = 0.14478327\n",
      "Iteration 112, loss = 0.14487180\n",
      "Iteration 113, loss = 0.14500664\n",
      "Iteration 114, loss = 0.14496084\n",
      "Iteration 115, loss = 0.14494804\n",
      "Iteration 116, loss = 0.14505330\n",
      "Iteration 117, loss = 0.14485997\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.57513278\n",
      "Iteration 2, loss = 0.45501489\n",
      "Iteration 3, loss = 0.37964030\n",
      "Iteration 4, loss = 0.33523165\n",
      "Iteration 5, loss = 0.30312738\n",
      "Iteration 6, loss = 0.28264700\n",
      "Iteration 7, loss = 0.26742367\n",
      "Iteration 8, loss = 0.25451607\n",
      "Iteration 9, loss = 0.24435140\n",
      "Iteration 10, loss = 0.23515882\n",
      "Iteration 11, loss = 0.22693150\n",
      "Iteration 12, loss = 0.22030863\n",
      "Iteration 13, loss = 0.21360160\n",
      "Iteration 14, loss = 0.20835963\n",
      "Iteration 15, loss = 0.20209037\n",
      "Iteration 16, loss = 0.19679338\n",
      "Iteration 17, loss = 0.19254266\n",
      "Iteration 18, loss = 0.18866247\n",
      "Iteration 19, loss = 0.18514586\n",
      "Iteration 20, loss = 0.18131850\n",
      "Iteration 21, loss = 0.17829718\n",
      "Iteration 22, loss = 0.17562592\n",
      "Iteration 23, loss = 0.17300362\n",
      "Iteration 24, loss = 0.17108996\n",
      "Iteration 25, loss = 0.16911325\n",
      "Iteration 26, loss = 0.16735445\n",
      "Iteration 27, loss = 0.16532800\n",
      "Iteration 28, loss = 0.16394642\n",
      "Iteration 29, loss = 0.16222099\n",
      "Iteration 30, loss = 0.16059307\n",
      "Iteration 31, loss = 0.15918508\n",
      "Iteration 32, loss = 0.15803933\n",
      "Iteration 33, loss = 0.15707885\n",
      "Iteration 34, loss = 0.15591534\n",
      "Iteration 35, loss = 0.15501123\n",
      "Iteration 36, loss = 0.15432903\n",
      "Iteration 37, loss = 0.15366006\n",
      "Iteration 38, loss = 0.15325622\n",
      "Iteration 39, loss = 0.15237614\n",
      "Iteration 40, loss = 0.15166297\n",
      "Iteration 41, loss = 0.15133844\n",
      "Iteration 42, loss = 0.15073967\n",
      "Iteration 43, loss = 0.15048642\n",
      "Iteration 44, loss = 0.15028996\n",
      "Iteration 45, loss = 0.14986436\n",
      "Iteration 46, loss = 0.14980733\n",
      "Iteration 47, loss = 0.14946449\n",
      "Iteration 48, loss = 0.14903782\n",
      "Iteration 49, loss = 0.14883998\n",
      "Iteration 50, loss = 0.14877147\n",
      "Iteration 51, loss = 0.14884563\n",
      "Iteration 52, loss = 0.14885006\n",
      "Iteration 53, loss = 0.14845503\n",
      "Iteration 54, loss = 0.14837634\n",
      "Iteration 55, loss = 0.14834411\n",
      "Iteration 56, loss = 0.14821243\n",
      "Iteration 57, loss = 0.14860660\n",
      "Iteration 58, loss = 0.14803104\n",
      "Iteration 59, loss = 0.14789877\n",
      "Iteration 60, loss = 0.14785983\n",
      "Iteration 61, loss = 0.14779944\n",
      "Iteration 62, loss = 0.14777415\n",
      "Iteration 63, loss = 0.14811854\n",
      "Iteration 64, loss = 0.14773443\n",
      "Iteration 65, loss = 0.14810826\n",
      "Iteration 66, loss = 0.14782772\n",
      "Iteration 67, loss = 0.14798620\n",
      "Iteration 68, loss = 0.14773236\n",
      "Iteration 69, loss = 0.14781463\n",
      "Iteration 70, loss = 0.14763162\n",
      "Iteration 71, loss = 0.14760674\n",
      "Iteration 72, loss = 0.14734020\n",
      "Iteration 73, loss = 0.14732381\n",
      "Iteration 74, loss = 0.14786532\n",
      "Iteration 75, loss = 0.14726176\n",
      "Iteration 76, loss = 0.14735305\n",
      "Iteration 77, loss = 0.14735557\n",
      "Iteration 78, loss = 0.14723680\n",
      "Iteration 79, loss = 0.14731282\n",
      "Iteration 80, loss = 0.14728719\n",
      "Iteration 81, loss = 0.14734112\n",
      "Iteration 82, loss = 0.14736060\n",
      "Iteration 83, loss = 0.14735926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.62563302\n",
      "Iteration 2, loss = 0.60893786\n",
      "Iteration 3, loss = 0.59327983\n",
      "Iteration 4, loss = 0.57830863\n",
      "Iteration 5, loss = 0.56398640\n",
      "Iteration 6, loss = 0.55066791\n",
      "Iteration 7, loss = 0.53755207\n",
      "Iteration 8, loss = 0.52555404\n",
      "Iteration 9, loss = 0.51377073\n",
      "Iteration 10, loss = 0.50302447\n",
      "Iteration 11, loss = 0.49249506\n",
      "Iteration 12, loss = 0.48250340\n",
      "Iteration 13, loss = 0.47338455\n",
      "Iteration 14, loss = 0.46445906\n",
      "Iteration 15, loss = 0.45618406\n",
      "Iteration 16, loss = 0.44834804\n",
      "Iteration 17, loss = 0.44072343\n",
      "Iteration 18, loss = 0.43333810\n",
      "Iteration 19, loss = 0.42658159\n",
      "Iteration 20, loss = 0.41980315\n",
      "Iteration 21, loss = 0.41371099\n",
      "Iteration 22, loss = 0.40760547\n",
      "Iteration 23, loss = 0.40190017\n",
      "Iteration 24, loss = 0.39632737\n",
      "Iteration 25, loss = 0.39086280\n",
      "Iteration 26, loss = 0.38574812\n",
      "Iteration 27, loss = 0.38080390\n",
      "Iteration 28, loss = 0.37613406\n",
      "Iteration 29, loss = 0.37151223\n",
      "Iteration 30, loss = 0.36701862\n",
      "Iteration 31, loss = 0.36282853\n",
      "Iteration 32, loss = 0.35878744\n",
      "Iteration 33, loss = 0.35492122\n",
      "Iteration 34, loss = 0.35114465\n",
      "Iteration 35, loss = 0.34728426\n",
      "Iteration 36, loss = 0.34388898\n",
      "Iteration 37, loss = 0.34046032\n",
      "Iteration 38, loss = 0.33723877\n",
      "Iteration 39, loss = 0.33398061\n",
      "Iteration 40, loss = 0.33093789\n",
      "Iteration 41, loss = 0.32793628\n",
      "Iteration 42, loss = 0.32509464\n",
      "Iteration 43, loss = 0.32230048\n",
      "Iteration 44, loss = 0.31958893\n",
      "Iteration 45, loss = 0.31698040\n",
      "Iteration 46, loss = 0.31445229\n",
      "Iteration 47, loss = 0.31205336\n",
      "Iteration 48, loss = 0.30961047\n",
      "Iteration 49, loss = 0.30706878\n",
      "Iteration 50, loss = 0.30490709\n",
      "Iteration 51, loss = 0.30262321\n",
      "Iteration 52, loss = 0.30050524\n",
      "Iteration 53, loss = 0.29834746\n",
      "Iteration 54, loss = 0.29627134\n",
      "Iteration 55, loss = 0.29425133\n",
      "Iteration 56, loss = 0.29227901\n",
      "Iteration 57, loss = 0.29036674\n",
      "Iteration 58, loss = 0.28852609\n",
      "Iteration 59, loss = 0.28670552\n",
      "Iteration 60, loss = 0.28492985\n",
      "Iteration 61, loss = 0.28322037\n",
      "Iteration 62, loss = 0.28148894\n",
      "Iteration 63, loss = 0.27979638\n",
      "Iteration 64, loss = 0.27816935\n",
      "Iteration 65, loss = 0.27655639\n",
      "Iteration 66, loss = 0.27504742\n",
      "Iteration 67, loss = 0.27341797\n",
      "Iteration 68, loss = 0.27196409\n",
      "Iteration 69, loss = 0.27042293\n",
      "Iteration 70, loss = 0.26902111\n",
      "Iteration 71, loss = 0.26758084\n",
      "Iteration 72, loss = 0.26614827\n",
      "Iteration 73, loss = 0.26482117\n",
      "Iteration 74, loss = 0.26347006\n",
      "Iteration 75, loss = 0.26219835\n",
      "Iteration 76, loss = 0.26086830\n",
      "Iteration 77, loss = 0.25961178\n",
      "Iteration 78, loss = 0.25833840\n",
      "Iteration 79, loss = 0.25709426\n",
      "Iteration 80, loss = 0.25589698\n",
      "Iteration 81, loss = 0.25467376\n",
      "Iteration 82, loss = 0.25355121\n",
      "Iteration 83, loss = 0.25235211\n",
      "Iteration 84, loss = 0.25120999\n",
      "Iteration 85, loss = 0.25006051\n",
      "Iteration 86, loss = 0.24894810\n",
      "Iteration 87, loss = 0.24789819\n",
      "Iteration 88, loss = 0.24675846\n",
      "Iteration 89, loss = 0.24575336\n",
      "Iteration 90, loss = 0.24464391\n",
      "Iteration 91, loss = 0.24360382\n",
      "Iteration 92, loss = 0.24256852\n",
      "Iteration 93, loss = 0.24156749\n",
      "Iteration 94, loss = 0.24056040\n",
      "Iteration 95, loss = 0.23958705\n",
      "Iteration 96, loss = 0.23857314\n",
      "Iteration 97, loss = 0.23760750\n",
      "Iteration 98, loss = 0.23661214\n",
      "Iteration 99, loss = 0.23565942\n",
      "Iteration 100, loss = 0.23467617\n",
      "Iteration 101, loss = 0.23376198\n",
      "Iteration 102, loss = 0.23279298\n",
      "Iteration 103, loss = 0.23187762\n",
      "Iteration 104, loss = 0.23103418\n",
      "Iteration 105, loss = 0.23015167\n",
      "Iteration 106, loss = 0.22922701\n",
      "Iteration 107, loss = 0.22839300\n",
      "Iteration 108, loss = 0.22755699\n",
      "Iteration 109, loss = 0.22669386\n",
      "Iteration 110, loss = 0.22595062\n",
      "Iteration 111, loss = 0.22515832\n",
      "Iteration 112, loss = 0.22431525\n",
      "Iteration 113, loss = 0.22355314\n",
      "Iteration 114, loss = 0.22277686\n",
      "Iteration 115, loss = 0.22200971\n",
      "Iteration 116, loss = 0.22127598\n",
      "Iteration 117, loss = 0.22061501\n",
      "Iteration 118, loss = 0.21981991\n",
      "Iteration 119, loss = 0.21914867\n",
      "Iteration 120, loss = 0.21843092\n",
      "Iteration 121, loss = 0.21776583\n",
      "Iteration 122, loss = 0.21707374\n",
      "Iteration 123, loss = 0.21645198\n",
      "Iteration 124, loss = 0.21579145\n",
      "Iteration 125, loss = 0.21511370\n",
      "Iteration 126, loss = 0.21448708\n",
      "Iteration 127, loss = 0.21389127\n",
      "Iteration 128, loss = 0.21326670\n",
      "Iteration 129, loss = 0.21263981\n",
      "Iteration 130, loss = 0.21205351\n",
      "Iteration 131, loss = 0.21146698\n",
      "Iteration 132, loss = 0.21085356\n",
      "Iteration 133, loss = 0.21029103\n",
      "Iteration 134, loss = 0.20968290\n",
      "Iteration 135, loss = 0.20914053\n",
      "Iteration 136, loss = 0.20853808\n",
      "Iteration 137, loss = 0.20796776\n",
      "Iteration 138, loss = 0.20741560\n",
      "Iteration 139, loss = 0.20682209\n",
      "Iteration 140, loss = 0.20624423\n",
      "Iteration 141, loss = 0.20569806\n",
      "Iteration 142, loss = 0.20516865\n",
      "Iteration 143, loss = 0.20458600\n",
      "Iteration 144, loss = 0.20400152\n",
      "Iteration 145, loss = 0.20347990\n",
      "Iteration 146, loss = 0.20290905\n",
      "Iteration 147, loss = 0.20235603\n",
      "Iteration 148, loss = 0.20181264\n",
      "Iteration 149, loss = 0.20125360\n",
      "Iteration 150, loss = 0.20074550\n",
      "Iteration 151, loss = 0.20017598\n",
      "Iteration 152, loss = 0.19966056\n",
      "Iteration 153, loss = 0.19912643\n",
      "Iteration 154, loss = 0.19863418\n",
      "Iteration 155, loss = 0.19809708\n",
      "Iteration 156, loss = 0.19762483\n",
      "Iteration 157, loss = 0.19714017\n",
      "Iteration 158, loss = 0.19662370\n",
      "Iteration 159, loss = 0.19614973\n",
      "Iteration 160, loss = 0.19569153\n",
      "Iteration 161, loss = 0.19526299\n",
      "Iteration 162, loss = 0.19482425\n",
      "Iteration 163, loss = 0.19434750\n",
      "Iteration 164, loss = 0.19394832\n",
      "Iteration 165, loss = 0.19352273\n",
      "Iteration 166, loss = 0.19307511\n",
      "Iteration 167, loss = 0.19265631\n",
      "Iteration 168, loss = 0.19226927\n",
      "Iteration 169, loss = 0.19186892\n",
      "Iteration 170, loss = 0.19143452\n",
      "Iteration 171, loss = 0.19106539\n",
      "Iteration 172, loss = 0.19065936\n",
      "Iteration 173, loss = 0.19031231\n",
      "Iteration 174, loss = 0.18991881\n",
      "Iteration 175, loss = 0.18952154\n",
      "Iteration 176, loss = 0.18918342\n",
      "Iteration 177, loss = 0.18883991\n",
      "Iteration 178, loss = 0.18858749\n",
      "Iteration 179, loss = 0.18813329\n",
      "Iteration 180, loss = 0.18777934\n",
      "Iteration 181, loss = 0.18744167\n",
      "Iteration 182, loss = 0.18709324\n",
      "Iteration 183, loss = 0.18678029\n",
      "Iteration 184, loss = 0.18646144\n",
      "Iteration 185, loss = 0.18614867\n",
      "Iteration 186, loss = 0.18582467\n",
      "Iteration 187, loss = 0.18554482\n",
      "Iteration 188, loss = 0.18521035\n",
      "Iteration 189, loss = 0.18491534\n",
      "Iteration 190, loss = 0.18462290\n",
      "Iteration 191, loss = 0.18432322\n",
      "Iteration 192, loss = 0.18404543\n",
      "Iteration 193, loss = 0.18374022\n",
      "Iteration 194, loss = 0.18345970\n",
      "Iteration 195, loss = 0.18319948\n",
      "Iteration 196, loss = 0.18291124\n",
      "Iteration 197, loss = 0.18265030\n",
      "Iteration 198, loss = 0.18238326\n",
      "Iteration 199, loss = 0.18209861\n",
      "Iteration 200, loss = 0.18183286\n",
      "Iteration 201, loss = 0.18155951\n",
      "Iteration 202, loss = 0.18129729\n",
      "Iteration 203, loss = 0.18104384\n",
      "Iteration 204, loss = 0.18080475\n",
      "Iteration 205, loss = 0.18050708\n",
      "Iteration 206, loss = 0.18026442\n",
      "Iteration 207, loss = 0.18000529\n",
      "Iteration 208, loss = 0.17977142\n",
      "Iteration 209, loss = 0.17953576\n",
      "Iteration 210, loss = 0.17927032\n",
      "Iteration 211, loss = 0.17904670\n",
      "Iteration 212, loss = 0.17877003\n",
      "Iteration 213, loss = 0.17857915\n",
      "Iteration 214, loss = 0.17831440\n",
      "Iteration 215, loss = 0.17809265\n",
      "Iteration 216, loss = 0.17784471\n",
      "Iteration 217, loss = 0.17760495\n",
      "Iteration 218, loss = 0.17737535\n",
      "Iteration 219, loss = 0.17713470\n",
      "Iteration 220, loss = 0.17692892\n",
      "Iteration 221, loss = 0.17668028\n",
      "Iteration 222, loss = 0.17648076\n",
      "Iteration 223, loss = 0.17626381\n",
      "Iteration 224, loss = 0.17604249\n",
      "Iteration 225, loss = 0.17580201\n",
      "Iteration 226, loss = 0.17558210\n",
      "Iteration 227, loss = 0.17537554\n",
      "Iteration 228, loss = 0.17517574\n",
      "Iteration 229, loss = 0.17495598\n",
      "Iteration 230, loss = 0.17476606\n",
      "Iteration 231, loss = 0.17455481\n",
      "Iteration 232, loss = 0.17437067\n",
      "Iteration 233, loss = 0.17416648\n",
      "Iteration 234, loss = 0.17400924\n",
      "Iteration 235, loss = 0.17383829\n",
      "Iteration 236, loss = 0.17359911\n",
      "Iteration 237, loss = 0.17342590\n",
      "Iteration 238, loss = 0.17321677\n",
      "Iteration 239, loss = 0.17303076\n",
      "Iteration 240, loss = 0.17286579\n",
      "Iteration 241, loss = 0.17270696\n",
      "Iteration 242, loss = 0.17249936\n",
      "Iteration 243, loss = 0.17233278\n",
      "Iteration 244, loss = 0.17214246\n",
      "Iteration 245, loss = 0.17198293\n",
      "Iteration 246, loss = 0.17179305\n",
      "Iteration 247, loss = 0.17161309\n",
      "Iteration 248, loss = 0.17142111\n",
      "Iteration 249, loss = 0.17123923\n",
      "Iteration 250, loss = 0.17110311\n",
      "Iteration 251, loss = 0.17090186\n",
      "Iteration 252, loss = 0.17073936\n",
      "Iteration 253, loss = 0.17060618\n",
      "Iteration 254, loss = 0.17040744\n",
      "Iteration 255, loss = 0.17022584\n",
      "Iteration 256, loss = 0.17007419\n",
      "Iteration 257, loss = 0.16990515\n",
      "Iteration 258, loss = 0.16973896\n",
      "Iteration 259, loss = 0.16960180\n",
      "Iteration 260, loss = 0.16942486\n",
      "Iteration 261, loss = 0.16928624\n",
      "Iteration 262, loss = 0.16914683\n",
      "Iteration 263, loss = 0.16891551\n",
      "Iteration 264, loss = 0.16875470\n",
      "Iteration 265, loss = 0.16857159\n",
      "Iteration 266, loss = 0.16843269\n",
      "Iteration 267, loss = 0.16823638\n",
      "Iteration 268, loss = 0.16804497\n",
      "Iteration 269, loss = 0.16789836\n",
      "Iteration 270, loss = 0.16770804\n",
      "Iteration 271, loss = 0.16751717\n",
      "Iteration 272, loss = 0.16733063\n",
      "Iteration 273, loss = 0.16714592\n",
      "Iteration 274, loss = 0.16697599\n",
      "Iteration 275, loss = 0.16678225\n",
      "Iteration 276, loss = 0.16661903\n",
      "Iteration 277, loss = 0.16643691\n",
      "Iteration 278, loss = 0.16624871\n",
      "Iteration 279, loss = 0.16611901\n",
      "Iteration 280, loss = 0.16588497\n",
      "Iteration 281, loss = 0.16573996\n",
      "Iteration 282, loss = 0.16554116\n",
      "Iteration 283, loss = 0.16536980\n",
      "Iteration 284, loss = 0.16518892\n",
      "Iteration 285, loss = 0.16503208\n",
      "Iteration 286, loss = 0.16483346\n",
      "Iteration 287, loss = 0.16467147\n",
      "Iteration 288, loss = 0.16449721\n",
      "Iteration 289, loss = 0.16438384\n",
      "Iteration 290, loss = 0.16418510\n",
      "Iteration 291, loss = 0.16401804\n",
      "Iteration 292, loss = 0.16391311\n",
      "Iteration 293, loss = 0.16372230\n",
      "Iteration 294, loss = 0.16359840\n",
      "Iteration 295, loss = 0.16345436\n",
      "Iteration 296, loss = 0.16329592\n",
      "Iteration 297, loss = 0.16318107\n",
      "Iteration 298, loss = 0.16306191\n",
      "Iteration 299, loss = 0.16294141\n",
      "Iteration 300, loss = 0.16281930\n",
      "Iteration 301, loss = 0.16269590\n",
      "Iteration 302, loss = 0.16260188\n",
      "Iteration 303, loss = 0.16248651\n",
      "Iteration 304, loss = 0.16237422\n",
      "Iteration 305, loss = 0.16228628\n",
      "Iteration 306, loss = 0.16215377\n",
      "Iteration 307, loss = 0.16207967\n",
      "Iteration 308, loss = 0.16197842\n",
      "Iteration 309, loss = 0.16185735\n",
      "Iteration 310, loss = 0.16175659\n",
      "Iteration 311, loss = 0.16167201\n",
      "Iteration 312, loss = 0.16153826\n",
      "Iteration 313, loss = 0.16148066\n",
      "Iteration 314, loss = 0.16134714\n",
      "Iteration 315, loss = 0.16128833\n",
      "Iteration 316, loss = 0.16115164\n",
      "Iteration 317, loss = 0.16112843\n",
      "Iteration 318, loss = 0.16097507\n",
      "Iteration 319, loss = 0.16088482\n",
      "Iteration 320, loss = 0.16082405\n",
      "Iteration 321, loss = 0.16071028\n",
      "Iteration 322, loss = 0.16064127\n",
      "Iteration 323, loss = 0.16054508\n",
      "Iteration 324, loss = 0.16049868\n",
      "Iteration 325, loss = 0.16037529\n",
      "Iteration 326, loss = 0.16030559\n",
      "Iteration 327, loss = 0.16019398\n",
      "Iteration 328, loss = 0.16011772\n",
      "Iteration 329, loss = 0.16004294\n",
      "Iteration 330, loss = 0.15993506\n",
      "Iteration 331, loss = 0.15986091\n",
      "Iteration 332, loss = 0.15976381\n",
      "Iteration 333, loss = 0.15969252\n",
      "Iteration 334, loss = 0.15961569\n",
      "Iteration 335, loss = 0.15951566\n",
      "Iteration 336, loss = 0.15943355\n",
      "Iteration 337, loss = 0.15934397\n",
      "Iteration 338, loss = 0.15929166\n",
      "Iteration 339, loss = 0.15920053\n",
      "Iteration 340, loss = 0.15913992\n",
      "Iteration 341, loss = 0.15903767\n",
      "Iteration 342, loss = 0.15896704\n",
      "Iteration 343, loss = 0.15890718\n",
      "Iteration 344, loss = 0.15883721\n",
      "Iteration 345, loss = 0.15878055\n",
      "Iteration 346, loss = 0.15867471\n",
      "Iteration 347, loss = 0.15859199\n",
      "Iteration 348, loss = 0.15853310\n",
      "Iteration 349, loss = 0.15847887\n",
      "Iteration 350, loss = 0.15842975\n",
      "Iteration 351, loss = 0.15831929\n",
      "Iteration 352, loss = 0.15826538\n",
      "Iteration 353, loss = 0.15819205\n",
      "Iteration 354, loss = 0.15811967\n",
      "Iteration 355, loss = 0.15806257\n",
      "Iteration 356, loss = 0.15805392\n",
      "Iteration 357, loss = 0.15792903\n",
      "Iteration 358, loss = 0.15790334\n",
      "Iteration 359, loss = 0.15783323\n",
      "Iteration 360, loss = 0.15775511\n",
      "Iteration 361, loss = 0.15770795\n",
      "Iteration 362, loss = 0.15766730\n",
      "Iteration 363, loss = 0.15757279\n",
      "Iteration 364, loss = 0.15752513\n",
      "Iteration 365, loss = 0.15745886\n",
      "Iteration 366, loss = 0.15741412\n",
      "Iteration 367, loss = 0.15736689\n",
      "Iteration 368, loss = 0.15734445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.62747943\n",
      "Iteration 2, loss = 0.61088809\n",
      "Iteration 3, loss = 0.59545464\n",
      "Iteration 4, loss = 0.58058083\n",
      "Iteration 5, loss = 0.56635368\n",
      "Iteration 6, loss = 0.55311464\n",
      "Iteration 7, loss = 0.54006919\n",
      "Iteration 8, loss = 0.52804355\n",
      "Iteration 9, loss = 0.51627809\n",
      "Iteration 10, loss = 0.50569223\n",
      "Iteration 11, loss = 0.49500940\n",
      "Iteration 12, loss = 0.48515803\n",
      "Iteration 13, loss = 0.47605649\n",
      "Iteration 14, loss = 0.46713055\n",
      "Iteration 15, loss = 0.45876753\n",
      "Iteration 16, loss = 0.45100040\n",
      "Iteration 17, loss = 0.44323286\n",
      "Iteration 18, loss = 0.43578214\n",
      "Iteration 19, loss = 0.42898486\n",
      "Iteration 20, loss = 0.42215700\n",
      "Iteration 21, loss = 0.41602740\n",
      "Iteration 22, loss = 0.40986452\n",
      "Iteration 23, loss = 0.40422957\n",
      "Iteration 24, loss = 0.39866963\n",
      "Iteration 25, loss = 0.39315318\n",
      "Iteration 26, loss = 0.38806916\n",
      "Iteration 27, loss = 0.38323119\n",
      "Iteration 28, loss = 0.37852681\n",
      "Iteration 29, loss = 0.37390931\n",
      "Iteration 30, loss = 0.36939244\n",
      "Iteration 31, loss = 0.36512000\n",
      "Iteration 32, loss = 0.36102944\n",
      "Iteration 33, loss = 0.35711048\n",
      "Iteration 34, loss = 0.35336473\n",
      "Iteration 35, loss = 0.34950743\n",
      "Iteration 36, loss = 0.34607199\n",
      "Iteration 37, loss = 0.34260609\n",
      "Iteration 38, loss = 0.33935179\n",
      "Iteration 39, loss = 0.33603826\n",
      "Iteration 40, loss = 0.33296199\n",
      "Iteration 41, loss = 0.32991327\n",
      "Iteration 42, loss = 0.32699078\n",
      "Iteration 43, loss = 0.32416869\n",
      "Iteration 44, loss = 0.32142015\n",
      "Iteration 45, loss = 0.31876631\n",
      "Iteration 46, loss = 0.31622160\n",
      "Iteration 47, loss = 0.31374810\n",
      "Iteration 48, loss = 0.31124424\n",
      "Iteration 49, loss = 0.30866671\n",
      "Iteration 50, loss = 0.30642553\n",
      "Iteration 51, loss = 0.30414454\n",
      "Iteration 52, loss = 0.30197640\n",
      "Iteration 53, loss = 0.29977284\n",
      "Iteration 54, loss = 0.29765188\n",
      "Iteration 55, loss = 0.29559486\n",
      "Iteration 56, loss = 0.29357481\n",
      "Iteration 57, loss = 0.29161489\n",
      "Iteration 58, loss = 0.28974062\n",
      "Iteration 59, loss = 0.28788179\n",
      "Iteration 60, loss = 0.28605962\n",
      "Iteration 61, loss = 0.28430386\n",
      "Iteration 62, loss = 0.28251313\n",
      "Iteration 63, loss = 0.28081060\n",
      "Iteration 64, loss = 0.27915007\n",
      "Iteration 65, loss = 0.27750576\n",
      "Iteration 66, loss = 0.27596902\n",
      "Iteration 67, loss = 0.27432338\n",
      "Iteration 68, loss = 0.27286574\n",
      "Iteration 69, loss = 0.27130699\n",
      "Iteration 70, loss = 0.26989063\n",
      "Iteration 71, loss = 0.26842247\n",
      "Iteration 72, loss = 0.26697602\n",
      "Iteration 73, loss = 0.26563309\n",
      "Iteration 74, loss = 0.26426119\n",
      "Iteration 75, loss = 0.26294810\n",
      "Iteration 76, loss = 0.26163371\n",
      "Iteration 77, loss = 0.26033499\n",
      "Iteration 78, loss = 0.25905605\n",
      "Iteration 79, loss = 0.25780680\n",
      "Iteration 80, loss = 0.25659138\n",
      "Iteration 81, loss = 0.25537943\n",
      "Iteration 82, loss = 0.25426541\n",
      "Iteration 83, loss = 0.25305903\n",
      "Iteration 84, loss = 0.25192489\n",
      "Iteration 85, loss = 0.25078593\n",
      "Iteration 86, loss = 0.24966910\n",
      "Iteration 87, loss = 0.24864031\n",
      "Iteration 88, loss = 0.24749280\n",
      "Iteration 89, loss = 0.24647733\n",
      "Iteration 90, loss = 0.24539894\n",
      "Iteration 91, loss = 0.24435915\n",
      "Iteration 92, loss = 0.24330422\n",
      "Iteration 93, loss = 0.24230626\n",
      "Iteration 94, loss = 0.24130132\n",
      "Iteration 95, loss = 0.24035341\n",
      "Iteration 96, loss = 0.23934446\n",
      "Iteration 97, loss = 0.23838903\n",
      "Iteration 98, loss = 0.23738275\n",
      "Iteration 99, loss = 0.23645820\n",
      "Iteration 100, loss = 0.23550051\n",
      "Iteration 101, loss = 0.23462421\n",
      "Iteration 102, loss = 0.23370094\n",
      "Iteration 103, loss = 0.23279615\n",
      "Iteration 104, loss = 0.23199972\n",
      "Iteration 105, loss = 0.23114602\n",
      "Iteration 106, loss = 0.23026636\n",
      "Iteration 107, loss = 0.22946972\n",
      "Iteration 108, loss = 0.22865832\n",
      "Iteration 109, loss = 0.22784808\n",
      "Iteration 110, loss = 0.22711646\n",
      "Iteration 111, loss = 0.22632950\n",
      "Iteration 112, loss = 0.22552084\n",
      "Iteration 113, loss = 0.22476902\n",
      "Iteration 114, loss = 0.22400585\n",
      "Iteration 115, loss = 0.22325560\n",
      "Iteration 116, loss = 0.22253277\n",
      "Iteration 117, loss = 0.22187510\n",
      "Iteration 118, loss = 0.22107326\n",
      "Iteration 119, loss = 0.22039801\n",
      "Iteration 120, loss = 0.21966338\n",
      "Iteration 121, loss = 0.21898199\n",
      "Iteration 122, loss = 0.21826004\n",
      "Iteration 123, loss = 0.21760918\n",
      "Iteration 124, loss = 0.21692156\n",
      "Iteration 125, loss = 0.21624095\n",
      "Iteration 126, loss = 0.21558567\n",
      "Iteration 127, loss = 0.21497277\n",
      "Iteration 128, loss = 0.21434857\n",
      "Iteration 129, loss = 0.21370567\n",
      "Iteration 130, loss = 0.21310817\n",
      "Iteration 131, loss = 0.21252308\n",
      "Iteration 132, loss = 0.21189999\n",
      "Iteration 133, loss = 0.21130771\n",
      "Iteration 134, loss = 0.21068292\n",
      "Iteration 135, loss = 0.21013436\n",
      "Iteration 136, loss = 0.20952895\n",
      "Iteration 137, loss = 0.20896462\n",
      "Iteration 138, loss = 0.20841774\n",
      "Iteration 139, loss = 0.20782303\n",
      "Iteration 140, loss = 0.20725224\n",
      "Iteration 141, loss = 0.20671701\n",
      "Iteration 142, loss = 0.20620344\n",
      "Iteration 143, loss = 0.20562600\n",
      "Iteration 144, loss = 0.20504829\n",
      "Iteration 145, loss = 0.20451872\n",
      "Iteration 146, loss = 0.20396069\n",
      "Iteration 147, loss = 0.20341293\n",
      "Iteration 148, loss = 0.20287885\n",
      "Iteration 149, loss = 0.20232586\n",
      "Iteration 150, loss = 0.20186735\n",
      "Iteration 151, loss = 0.20130300\n",
      "Iteration 152, loss = 0.20079233\n",
      "Iteration 153, loss = 0.20029401\n",
      "Iteration 154, loss = 0.19980632\n",
      "Iteration 155, loss = 0.19930380\n",
      "Iteration 156, loss = 0.19882719\n",
      "Iteration 157, loss = 0.19836166\n",
      "Iteration 158, loss = 0.19786841\n",
      "Iteration 159, loss = 0.19739332\n",
      "Iteration 160, loss = 0.19690865\n",
      "Iteration 161, loss = 0.19645286\n",
      "Iteration 162, loss = 0.19600660\n",
      "Iteration 163, loss = 0.19551726\n",
      "Iteration 164, loss = 0.19506347\n",
      "Iteration 165, loss = 0.19464329\n",
      "Iteration 166, loss = 0.19417745\n",
      "Iteration 167, loss = 0.19371300\n",
      "Iteration 168, loss = 0.19328634\n",
      "Iteration 169, loss = 0.19286194\n",
      "Iteration 170, loss = 0.19237925\n",
      "Iteration 171, loss = 0.19196389\n",
      "Iteration 172, loss = 0.19150201\n",
      "Iteration 173, loss = 0.19109982\n",
      "Iteration 174, loss = 0.19068001\n",
      "Iteration 175, loss = 0.19026230\n",
      "Iteration 176, loss = 0.18987468\n",
      "Iteration 177, loss = 0.18947772\n",
      "Iteration 178, loss = 0.18920407\n",
      "Iteration 179, loss = 0.18870274\n",
      "Iteration 180, loss = 0.18833300\n",
      "Iteration 181, loss = 0.18796429\n",
      "Iteration 182, loss = 0.18758761\n",
      "Iteration 183, loss = 0.18723262\n",
      "Iteration 184, loss = 0.18687955\n",
      "Iteration 185, loss = 0.18651317\n",
      "Iteration 186, loss = 0.18617380\n",
      "Iteration 187, loss = 0.18583114\n",
      "Iteration 188, loss = 0.18546842\n",
      "Iteration 189, loss = 0.18513775\n",
      "Iteration 190, loss = 0.18481176\n",
      "Iteration 191, loss = 0.18445249\n",
      "Iteration 192, loss = 0.18416545\n",
      "Iteration 193, loss = 0.18381682\n",
      "Iteration 194, loss = 0.18350028\n",
      "Iteration 195, loss = 0.18320910\n",
      "Iteration 196, loss = 0.18289798\n",
      "Iteration 197, loss = 0.18261100\n",
      "Iteration 198, loss = 0.18230527\n",
      "Iteration 199, loss = 0.18200893\n",
      "Iteration 200, loss = 0.18171943\n",
      "Iteration 201, loss = 0.18142603\n",
      "Iteration 202, loss = 0.18113803\n",
      "Iteration 203, loss = 0.18087456\n",
      "Iteration 204, loss = 0.18060679\n",
      "Iteration 205, loss = 0.18031138\n",
      "Iteration 206, loss = 0.18005987\n",
      "Iteration 207, loss = 0.17977764\n",
      "Iteration 208, loss = 0.17953320\n",
      "Iteration 209, loss = 0.17927054\n",
      "Iteration 210, loss = 0.17900955\n",
      "Iteration 211, loss = 0.17876348\n",
      "Iteration 212, loss = 0.17848837\n",
      "Iteration 213, loss = 0.17827907\n",
      "Iteration 214, loss = 0.17801516\n",
      "Iteration 215, loss = 0.17778947\n",
      "Iteration 216, loss = 0.17752568\n",
      "Iteration 217, loss = 0.17728855\n",
      "Iteration 218, loss = 0.17705291\n",
      "Iteration 219, loss = 0.17682184\n",
      "Iteration 220, loss = 0.17660586\n",
      "Iteration 221, loss = 0.17635385\n",
      "Iteration 222, loss = 0.17616224\n",
      "Iteration 223, loss = 0.17591178\n",
      "Iteration 224, loss = 0.17569619\n",
      "Iteration 225, loss = 0.17545469\n",
      "Iteration 226, loss = 0.17523478\n",
      "Iteration 227, loss = 0.17501060\n",
      "Iteration 228, loss = 0.17480802\n",
      "Iteration 229, loss = 0.17459013\n",
      "Iteration 230, loss = 0.17438674\n",
      "Iteration 231, loss = 0.17415920\n",
      "Iteration 232, loss = 0.17396416\n",
      "Iteration 233, loss = 0.17375973\n",
      "Iteration 234, loss = 0.17359320\n",
      "Iteration 235, loss = 0.17340319\n",
      "Iteration 236, loss = 0.17317644\n",
      "Iteration 237, loss = 0.17300167\n",
      "Iteration 238, loss = 0.17278784\n",
      "Iteration 239, loss = 0.17260092\n",
      "Iteration 240, loss = 0.17242806\n",
      "Iteration 241, loss = 0.17228828\n",
      "Iteration 242, loss = 0.17206271\n",
      "Iteration 243, loss = 0.17188443\n",
      "Iteration 244, loss = 0.17170300\n",
      "Iteration 245, loss = 0.17152769\n",
      "Iteration 246, loss = 0.17134857\n",
      "Iteration 247, loss = 0.17117884\n",
      "Iteration 248, loss = 0.17101151\n",
      "Iteration 249, loss = 0.17082952\n",
      "Iteration 250, loss = 0.17068431\n",
      "Iteration 251, loss = 0.17049041\n",
      "Iteration 252, loss = 0.17032723\n",
      "Iteration 253, loss = 0.17018508\n",
      "Iteration 254, loss = 0.16999596\n",
      "Iteration 255, loss = 0.16980492\n",
      "Iteration 256, loss = 0.16964607\n",
      "Iteration 257, loss = 0.16946387\n",
      "Iteration 258, loss = 0.16926714\n",
      "Iteration 259, loss = 0.16910748\n",
      "Iteration 260, loss = 0.16892650\n",
      "Iteration 261, loss = 0.16876545\n",
      "Iteration 262, loss = 0.16858291\n",
      "Iteration 263, loss = 0.16836741\n",
      "Iteration 264, loss = 0.16819737\n",
      "Iteration 265, loss = 0.16803002\n",
      "Iteration 266, loss = 0.16787500\n",
      "Iteration 267, loss = 0.16767530\n",
      "Iteration 268, loss = 0.16749714\n",
      "Iteration 269, loss = 0.16733998\n",
      "Iteration 270, loss = 0.16716455\n",
      "Iteration 271, loss = 0.16698021\n",
      "Iteration 272, loss = 0.16680571\n",
      "Iteration 273, loss = 0.16663880\n",
      "Iteration 274, loss = 0.16648746\n",
      "Iteration 275, loss = 0.16631671\n",
      "Iteration 276, loss = 0.16616083\n",
      "Iteration 277, loss = 0.16602097\n",
      "Iteration 278, loss = 0.16585332\n",
      "Iteration 279, loss = 0.16575338\n",
      "Iteration 280, loss = 0.16554155\n",
      "Iteration 281, loss = 0.16540773\n",
      "Iteration 282, loss = 0.16522939\n",
      "Iteration 283, loss = 0.16508021\n",
      "Iteration 284, loss = 0.16492487\n",
      "Iteration 285, loss = 0.16475305\n",
      "Iteration 286, loss = 0.16458103\n",
      "Iteration 287, loss = 0.16442881\n",
      "Iteration 288, loss = 0.16426613\n",
      "Iteration 289, loss = 0.16416871\n",
      "Iteration 290, loss = 0.16396201\n",
      "Iteration 291, loss = 0.16380368\n",
      "Iteration 292, loss = 0.16370058\n",
      "Iteration 293, loss = 0.16350288\n",
      "Iteration 294, loss = 0.16338166\n",
      "Iteration 295, loss = 0.16323186\n",
      "Iteration 296, loss = 0.16305658\n",
      "Iteration 297, loss = 0.16292450\n",
      "Iteration 298, loss = 0.16278432\n",
      "Iteration 299, loss = 0.16262946\n",
      "Iteration 300, loss = 0.16249107\n",
      "Iteration 301, loss = 0.16234319\n",
      "Iteration 302, loss = 0.16223061\n",
      "Iteration 303, loss = 0.16209421\n",
      "Iteration 304, loss = 0.16195907\n",
      "Iteration 305, loss = 0.16184328\n",
      "Iteration 306, loss = 0.16169794\n",
      "Iteration 307, loss = 0.16159918\n",
      "Iteration 308, loss = 0.16149488\n",
      "Iteration 309, loss = 0.16137631\n",
      "Iteration 310, loss = 0.16125347\n",
      "Iteration 311, loss = 0.16115811\n",
      "Iteration 312, loss = 0.16102331\n",
      "Iteration 313, loss = 0.16097697\n",
      "Iteration 314, loss = 0.16082422\n",
      "Iteration 315, loss = 0.16075077\n",
      "Iteration 316, loss = 0.16061733\n",
      "Iteration 317, loss = 0.16059144\n",
      "Iteration 318, loss = 0.16042881\n",
      "Iteration 319, loss = 0.16033643\n",
      "Iteration 320, loss = 0.16025353\n",
      "Iteration 321, loss = 0.16014796\n",
      "Iteration 322, loss = 0.16005350\n",
      "Iteration 323, loss = 0.15996253\n",
      "Iteration 324, loss = 0.15989085\n",
      "Iteration 325, loss = 0.15977710\n",
      "Iteration 326, loss = 0.15970589\n",
      "Iteration 327, loss = 0.15959859\n",
      "Iteration 328, loss = 0.15950871\n",
      "Iteration 329, loss = 0.15942819\n",
      "Iteration 330, loss = 0.15932379\n",
      "Iteration 331, loss = 0.15925235\n",
      "Iteration 332, loss = 0.15914400\n",
      "Iteration 333, loss = 0.15907829\n",
      "Iteration 334, loss = 0.15899597\n",
      "Iteration 335, loss = 0.15890320\n",
      "Iteration 336, loss = 0.15880967\n",
      "Iteration 337, loss = 0.15872892\n",
      "Iteration 338, loss = 0.15868677\n",
      "Iteration 339, loss = 0.15857339\n",
      "Iteration 340, loss = 0.15851146\n",
      "Iteration 341, loss = 0.15841897\n",
      "Iteration 342, loss = 0.15834807\n",
      "Iteration 343, loss = 0.15827713\n",
      "Iteration 344, loss = 0.15820846\n",
      "Iteration 345, loss = 0.15813696\n",
      "Iteration 346, loss = 0.15804621\n",
      "Iteration 347, loss = 0.15795950\n",
      "Iteration 348, loss = 0.15790717\n",
      "Iteration 349, loss = 0.15785764\n",
      "Iteration 350, loss = 0.15780427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.57204501\n",
      "Iteration 2, loss = 0.44977884\n",
      "Iteration 3, loss = 0.37329436\n",
      "Iteration 4, loss = 0.32441410\n",
      "Iteration 5, loss = 0.29353614\n",
      "Iteration 6, loss = 0.27131312\n",
      "Iteration 7, loss = 0.25573494\n",
      "Iteration 8, loss = 0.24315546\n",
      "Iteration 9, loss = 0.23314910\n",
      "Iteration 10, loss = 0.22399859\n",
      "Iteration 11, loss = 0.21606056\n",
      "Iteration 12, loss = 0.20901418\n",
      "Iteration 13, loss = 0.20217154\n",
      "Iteration 14, loss = 0.19648302\n",
      "Iteration 15, loss = 0.19130283\n",
      "Iteration 16, loss = 0.18566655\n",
      "Iteration 17, loss = 0.18114086\n",
      "Iteration 18, loss = 0.17644280\n",
      "Iteration 19, loss = 0.17244036\n",
      "Iteration 20, loss = 0.16915310\n",
      "Iteration 21, loss = 0.16525191\n",
      "Iteration 22, loss = 0.16238805\n",
      "Iteration 23, loss = 0.16020896\n",
      "Iteration 24, loss = 0.15792939\n",
      "Iteration 25, loss = 0.15607901\n",
      "Iteration 26, loss = 0.15420439\n",
      "Iteration 27, loss = 0.15273162\n",
      "Iteration 28, loss = 0.15092063\n",
      "Iteration 29, loss = 0.14907090\n",
      "Iteration 30, loss = 0.14781215\n",
      "Iteration 31, loss = 0.14650522\n",
      "Iteration 32, loss = 0.14520724\n",
      "Iteration 33, loss = 0.14405532\n",
      "Iteration 34, loss = 0.14334650\n",
      "Iteration 35, loss = 0.14282150\n",
      "Iteration 36, loss = 0.14173259\n",
      "Iteration 37, loss = 0.14109881\n",
      "Iteration 38, loss = 0.14093784\n",
      "Iteration 39, loss = 0.14028149\n",
      "Iteration 40, loss = 0.13979944\n",
      "Iteration 41, loss = 0.13940686\n",
      "Iteration 42, loss = 0.13917601\n",
      "Iteration 43, loss = 0.13906459\n",
      "Iteration 44, loss = 0.13881933\n",
      "Iteration 45, loss = 0.13830959\n",
      "Iteration 46, loss = 0.13823370\n",
      "Iteration 47, loss = 0.13777785\n",
      "Iteration 48, loss = 0.13763418\n",
      "Iteration 49, loss = 0.13751662\n",
      "Iteration 50, loss = 0.13734352\n",
      "Iteration 51, loss = 0.13729136\n",
      "Iteration 52, loss = 0.13731327\n",
      "Iteration 53, loss = 0.13713798\n",
      "Iteration 54, loss = 0.13722741\n",
      "Iteration 55, loss = 0.13683078\n",
      "Iteration 56, loss = 0.13670686\n",
      "Iteration 57, loss = 0.13649815\n",
      "Iteration 58, loss = 0.13621177\n",
      "Iteration 59, loss = 0.13616140\n",
      "Iteration 60, loss = 0.13598709\n",
      "Iteration 61, loss = 0.13611603\n",
      "Iteration 62, loss = 0.13624605\n",
      "Iteration 63, loss = 0.13586725\n",
      "Iteration 64, loss = 0.13561153\n",
      "Iteration 65, loss = 0.13594345\n",
      "Iteration 66, loss = 0.13592825\n",
      "Iteration 67, loss = 0.13558719\n",
      "Iteration 68, loss = 0.13586024\n",
      "Iteration 69, loss = 0.13545601\n",
      "Iteration 70, loss = 0.13545549\n",
      "Iteration 71, loss = 0.13536002\n",
      "Iteration 72, loss = 0.13527375\n",
      "Iteration 73, loss = 0.13533510\n",
      "Iteration 74, loss = 0.13546772\n",
      "Iteration 75, loss = 0.13537854\n",
      "Iteration 76, loss = 0.13546016\n",
      "Iteration 77, loss = 0.13502944\n",
      "Iteration 78, loss = 0.13502203\n",
      "Iteration 79, loss = 0.13500587\n",
      "Iteration 80, loss = 0.13496823\n",
      "Iteration 81, loss = 0.13515220\n",
      "Iteration 82, loss = 0.13517133\n",
      "Iteration 83, loss = 0.13507382\n",
      "Iteration 84, loss = 0.13492573\n",
      "Iteration 85, loss = 0.13513069\n",
      "Iteration 86, loss = 0.13491396\n",
      "Iteration 87, loss = 0.13530581\n",
      "Iteration 88, loss = 0.13501369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.62057538\n",
      "Iteration 2, loss = 0.60399795\n",
      "Iteration 3, loss = 0.58815205\n",
      "Iteration 4, loss = 0.57349935\n",
      "Iteration 5, loss = 0.55897533\n",
      "Iteration 6, loss = 0.54582421\n",
      "Iteration 7, loss = 0.53270944\n",
      "Iteration 8, loss = 0.52069111\n",
      "Iteration 9, loss = 0.50919737\n",
      "Iteration 10, loss = 0.49830567\n",
      "Iteration 11, loss = 0.48767756\n",
      "Iteration 12, loss = 0.47786961\n",
      "Iteration 13, loss = 0.46881583\n",
      "Iteration 14, loss = 0.45988043\n",
      "Iteration 15, loss = 0.45157379\n",
      "Iteration 16, loss = 0.44389702\n",
      "Iteration 17, loss = 0.43631158\n",
      "Iteration 18, loss = 0.42892045\n",
      "Iteration 19, loss = 0.42212859\n",
      "Iteration 20, loss = 0.41562844\n",
      "Iteration 21, loss = 0.40953228\n",
      "Iteration 22, loss = 0.40346372\n",
      "Iteration 23, loss = 0.39780028\n",
      "Iteration 24, loss = 0.39240168\n",
      "Iteration 25, loss = 0.38704997\n",
      "Iteration 26, loss = 0.38186132\n",
      "Iteration 27, loss = 0.37710935\n",
      "Iteration 28, loss = 0.37237325\n",
      "Iteration 29, loss = 0.36795283\n",
      "Iteration 30, loss = 0.36358821\n",
      "Iteration 31, loss = 0.35944516\n",
      "Iteration 32, loss = 0.35542350\n",
      "Iteration 33, loss = 0.35165211\n",
      "Iteration 34, loss = 0.34793723\n",
      "Iteration 35, loss = 0.34422518\n",
      "Iteration 36, loss = 0.34081091\n",
      "Iteration 37, loss = 0.33744140\n",
      "Iteration 38, loss = 0.33435567\n",
      "Iteration 39, loss = 0.33115117\n",
      "Iteration 40, loss = 0.32821064\n",
      "Iteration 41, loss = 0.32528459\n",
      "Iteration 42, loss = 0.32243000\n",
      "Iteration 43, loss = 0.31963570\n",
      "Iteration 44, loss = 0.31698764\n",
      "Iteration 45, loss = 0.31441253\n",
      "Iteration 46, loss = 0.31198090\n",
      "Iteration 47, loss = 0.30948657\n",
      "Iteration 48, loss = 0.30712442\n",
      "Iteration 49, loss = 0.30465604\n",
      "Iteration 50, loss = 0.30241108\n",
      "Iteration 51, loss = 0.30023240\n",
      "Iteration 52, loss = 0.29804917\n",
      "Iteration 53, loss = 0.29593339\n",
      "Iteration 54, loss = 0.29384055\n",
      "Iteration 55, loss = 0.29181697\n",
      "Iteration 56, loss = 0.28989926\n",
      "Iteration 57, loss = 0.28795683\n",
      "Iteration 58, loss = 0.28607692\n",
      "Iteration 59, loss = 0.28424253\n",
      "Iteration 60, loss = 0.28247617\n",
      "Iteration 61, loss = 0.28074007\n",
      "Iteration 62, loss = 0.27895211\n",
      "Iteration 63, loss = 0.27733142\n",
      "Iteration 64, loss = 0.27567530\n",
      "Iteration 65, loss = 0.27405318\n",
      "Iteration 66, loss = 0.27256873\n",
      "Iteration 67, loss = 0.27097615\n",
      "Iteration 68, loss = 0.26949428\n",
      "Iteration 69, loss = 0.26800276\n",
      "Iteration 70, loss = 0.26663963\n",
      "Iteration 71, loss = 0.26517025\n",
      "Iteration 72, loss = 0.26381166\n",
      "Iteration 73, loss = 0.26247686\n",
      "Iteration 74, loss = 0.26113918\n",
      "Iteration 75, loss = 0.25981664\n",
      "Iteration 76, loss = 0.25852927\n",
      "Iteration 77, loss = 0.25728942\n",
      "Iteration 78, loss = 0.25603591\n",
      "Iteration 79, loss = 0.25479626\n",
      "Iteration 80, loss = 0.25358864\n",
      "Iteration 81, loss = 0.25235750\n",
      "Iteration 82, loss = 0.25123194\n",
      "Iteration 83, loss = 0.25002191\n",
      "Iteration 84, loss = 0.24887970\n",
      "Iteration 85, loss = 0.24774090\n",
      "Iteration 86, loss = 0.24658133\n",
      "Iteration 87, loss = 0.24552612\n",
      "Iteration 88, loss = 0.24439505\n",
      "Iteration 89, loss = 0.24326729\n",
      "Iteration 90, loss = 0.24220631\n",
      "Iteration 91, loss = 0.24116806\n",
      "Iteration 92, loss = 0.24003942\n",
      "Iteration 93, loss = 0.23905319\n",
      "Iteration 94, loss = 0.23797774\n",
      "Iteration 95, loss = 0.23700118\n",
      "Iteration 96, loss = 0.23599055\n",
      "Iteration 97, loss = 0.23497818\n",
      "Iteration 98, loss = 0.23403040\n",
      "Iteration 99, loss = 0.23301884\n",
      "Iteration 100, loss = 0.23208304\n",
      "Iteration 101, loss = 0.23112007\n",
      "Iteration 102, loss = 0.23017929\n",
      "Iteration 103, loss = 0.22923377\n",
      "Iteration 104, loss = 0.22836070\n",
      "Iteration 105, loss = 0.22744760\n",
      "Iteration 106, loss = 0.22652701\n",
      "Iteration 107, loss = 0.22568424\n",
      "Iteration 108, loss = 0.22484904\n",
      "Iteration 109, loss = 0.22399261\n",
      "Iteration 110, loss = 0.22315774\n",
      "Iteration 111, loss = 0.22238061\n",
      "Iteration 112, loss = 0.22156486\n",
      "Iteration 113, loss = 0.22079735\n",
      "Iteration 114, loss = 0.22001820\n",
      "Iteration 115, loss = 0.21922886\n",
      "Iteration 116, loss = 0.21851990\n",
      "Iteration 117, loss = 0.21781891\n",
      "Iteration 118, loss = 0.21705986\n",
      "Iteration 119, loss = 0.21638515\n",
      "Iteration 120, loss = 0.21566260\n",
      "Iteration 121, loss = 0.21499029\n",
      "Iteration 122, loss = 0.21432262\n",
      "Iteration 123, loss = 0.21362485\n",
      "Iteration 124, loss = 0.21296038\n",
      "Iteration 125, loss = 0.21230422\n",
      "Iteration 126, loss = 0.21162653\n",
      "Iteration 127, loss = 0.21102325\n",
      "Iteration 128, loss = 0.21038033\n",
      "Iteration 129, loss = 0.20973634\n",
      "Iteration 130, loss = 0.20910662\n",
      "Iteration 131, loss = 0.20847532\n",
      "Iteration 132, loss = 0.20785199\n",
      "Iteration 133, loss = 0.20723165\n",
      "Iteration 134, loss = 0.20659467\n",
      "Iteration 135, loss = 0.20596813\n",
      "Iteration 136, loss = 0.20532835\n",
      "Iteration 137, loss = 0.20474465\n",
      "Iteration 138, loss = 0.20410787\n",
      "Iteration 139, loss = 0.20348336\n",
      "Iteration 140, loss = 0.20288843\n",
      "Iteration 141, loss = 0.20229488\n",
      "Iteration 142, loss = 0.20169176\n",
      "Iteration 143, loss = 0.20111745\n",
      "Iteration 144, loss = 0.20053098\n",
      "Iteration 145, loss = 0.19995311\n",
      "Iteration 146, loss = 0.19938221\n",
      "Iteration 147, loss = 0.19879188\n",
      "Iteration 148, loss = 0.19824395\n",
      "Iteration 149, loss = 0.19768667\n",
      "Iteration 150, loss = 0.19716592\n",
      "Iteration 151, loss = 0.19652992\n",
      "Iteration 152, loss = 0.19602905\n",
      "Iteration 153, loss = 0.19546839\n",
      "Iteration 154, loss = 0.19492194\n",
      "Iteration 155, loss = 0.19436711\n",
      "Iteration 156, loss = 0.19387024\n",
      "Iteration 157, loss = 0.19339812\n",
      "Iteration 158, loss = 0.19284036\n",
      "Iteration 159, loss = 0.19233331\n",
      "Iteration 160, loss = 0.19184403\n",
      "Iteration 161, loss = 0.19139401\n",
      "Iteration 162, loss = 0.19091513\n",
      "Iteration 163, loss = 0.19043620\n",
      "Iteration 164, loss = 0.18996842\n",
      "Iteration 165, loss = 0.18950927\n",
      "Iteration 166, loss = 0.18904736\n",
      "Iteration 167, loss = 0.18859719\n",
      "Iteration 168, loss = 0.18816687\n",
      "Iteration 169, loss = 0.18767175\n",
      "Iteration 170, loss = 0.18724777\n",
      "Iteration 171, loss = 0.18683265\n",
      "Iteration 172, loss = 0.18638775\n",
      "Iteration 173, loss = 0.18598152\n",
      "Iteration 174, loss = 0.18552987\n",
      "Iteration 175, loss = 0.18512219\n",
      "Iteration 176, loss = 0.18472148\n",
      "Iteration 177, loss = 0.18433059\n",
      "Iteration 178, loss = 0.18400986\n",
      "Iteration 179, loss = 0.18355758\n",
      "Iteration 180, loss = 0.18317799\n",
      "Iteration 181, loss = 0.18282698\n",
      "Iteration 182, loss = 0.18244758\n",
      "Iteration 183, loss = 0.18207098\n",
      "Iteration 184, loss = 0.18173424\n",
      "Iteration 185, loss = 0.18136571\n",
      "Iteration 186, loss = 0.18102661\n",
      "Iteration 187, loss = 0.18070645\n",
      "Iteration 188, loss = 0.18034237\n",
      "Iteration 189, loss = 0.18004286\n",
      "Iteration 190, loss = 0.17967583\n",
      "Iteration 191, loss = 0.17934195\n",
      "Iteration 192, loss = 0.17904045\n",
      "Iteration 193, loss = 0.17872736\n",
      "Iteration 194, loss = 0.17840224\n",
      "Iteration 195, loss = 0.17809979\n",
      "Iteration 196, loss = 0.17780181\n",
      "Iteration 197, loss = 0.17750025\n",
      "Iteration 198, loss = 0.17720548\n",
      "Iteration 199, loss = 0.17689416\n",
      "Iteration 200, loss = 0.17662715\n",
      "Iteration 201, loss = 0.17632842\n",
      "Iteration 202, loss = 0.17603399\n",
      "Iteration 203, loss = 0.17575823\n",
      "Iteration 204, loss = 0.17548801\n",
      "Iteration 205, loss = 0.17516455\n",
      "Iteration 206, loss = 0.17489655\n",
      "Iteration 207, loss = 0.17461913\n",
      "Iteration 208, loss = 0.17435175\n",
      "Iteration 209, loss = 0.17410415\n",
      "Iteration 210, loss = 0.17384626\n",
      "Iteration 211, loss = 0.17358201\n",
      "Iteration 212, loss = 0.17333304\n",
      "Iteration 213, loss = 0.17313325\n",
      "Iteration 214, loss = 0.17283421\n",
      "Iteration 215, loss = 0.17263888\n",
      "Iteration 216, loss = 0.17235454\n",
      "Iteration 217, loss = 0.17210804\n",
      "Iteration 218, loss = 0.17186373\n",
      "Iteration 219, loss = 0.17163999\n",
      "Iteration 220, loss = 0.17139959\n",
      "Iteration 221, loss = 0.17116219\n",
      "Iteration 222, loss = 0.17095290\n",
      "Iteration 223, loss = 0.17072244\n",
      "Iteration 224, loss = 0.17050588\n",
      "Iteration 225, loss = 0.17026899\n",
      "Iteration 226, loss = 0.17001169\n",
      "Iteration 227, loss = 0.16978993\n",
      "Iteration 228, loss = 0.16958136\n",
      "Iteration 229, loss = 0.16936390\n",
      "Iteration 230, loss = 0.16914718\n",
      "Iteration 231, loss = 0.16890353\n",
      "Iteration 232, loss = 0.16869436\n",
      "Iteration 233, loss = 0.16848524\n",
      "Iteration 234, loss = 0.16831518\n",
      "Iteration 235, loss = 0.16806672\n",
      "Iteration 236, loss = 0.16786778\n",
      "Iteration 237, loss = 0.16765395\n",
      "Iteration 238, loss = 0.16743751\n",
      "Iteration 239, loss = 0.16723989\n",
      "Iteration 240, loss = 0.16700524\n",
      "Iteration 241, loss = 0.16681386\n",
      "Iteration 242, loss = 0.16658399\n",
      "Iteration 243, loss = 0.16637652\n",
      "Iteration 244, loss = 0.16618325\n",
      "Iteration 245, loss = 0.16595600\n",
      "Iteration 246, loss = 0.16574433\n",
      "Iteration 247, loss = 0.16553048\n",
      "Iteration 248, loss = 0.16534823\n",
      "Iteration 249, loss = 0.16513678\n",
      "Iteration 250, loss = 0.16495005\n",
      "Iteration 251, loss = 0.16474874\n",
      "Iteration 252, loss = 0.16454038\n",
      "Iteration 253, loss = 0.16437083\n",
      "Iteration 254, loss = 0.16413926\n",
      "Iteration 255, loss = 0.16388895\n",
      "Iteration 256, loss = 0.16368429\n",
      "Iteration 257, loss = 0.16348192\n",
      "Iteration 258, loss = 0.16325501\n",
      "Iteration 259, loss = 0.16305153\n",
      "Iteration 260, loss = 0.16286865\n",
      "Iteration 261, loss = 0.16265796\n",
      "Iteration 262, loss = 0.16243504\n",
      "Iteration 263, loss = 0.16221712\n",
      "Iteration 264, loss = 0.16201251\n",
      "Iteration 265, loss = 0.16182725\n",
      "Iteration 266, loss = 0.16163834\n",
      "Iteration 267, loss = 0.16143349\n",
      "Iteration 268, loss = 0.16124288\n",
      "Iteration 269, loss = 0.16104878\n",
      "Iteration 270, loss = 0.16084773\n",
      "Iteration 271, loss = 0.16067458\n",
      "Iteration 272, loss = 0.16048563\n",
      "Iteration 273, loss = 0.16028976\n",
      "Iteration 274, loss = 0.16011910\n",
      "Iteration 275, loss = 0.15993203\n",
      "Iteration 276, loss = 0.15975127\n",
      "Iteration 277, loss = 0.15956577\n",
      "Iteration 278, loss = 0.15941536\n",
      "Iteration 279, loss = 0.15925883\n",
      "Iteration 280, loss = 0.15901980\n",
      "Iteration 281, loss = 0.15887242\n",
      "Iteration 282, loss = 0.15870436\n",
      "Iteration 283, loss = 0.15855708\n",
      "Iteration 284, loss = 0.15841508\n",
      "Iteration 285, loss = 0.15826609\n",
      "Iteration 286, loss = 0.15810383\n",
      "Iteration 287, loss = 0.15798345\n",
      "Iteration 288, loss = 0.15782937\n",
      "Iteration 289, loss = 0.15771764\n",
      "Iteration 290, loss = 0.15756077\n",
      "Iteration 291, loss = 0.15741964\n",
      "Iteration 292, loss = 0.15730869\n",
      "Iteration 293, loss = 0.15716901\n",
      "Iteration 294, loss = 0.15706050\n",
      "Iteration 295, loss = 0.15696281\n",
      "Iteration 296, loss = 0.15681133\n",
      "Iteration 297, loss = 0.15669709\n",
      "Iteration 298, loss = 0.15656844\n",
      "Iteration 299, loss = 0.15645951\n",
      "Iteration 300, loss = 0.15634249\n",
      "Iteration 301, loss = 0.15622800\n",
      "Iteration 302, loss = 0.15611546\n",
      "Iteration 303, loss = 0.15599816\n",
      "Iteration 304, loss = 0.15591059\n",
      "Iteration 305, loss = 0.15579214\n",
      "Iteration 306, loss = 0.15569309\n",
      "Iteration 307, loss = 0.15558931\n",
      "Iteration 308, loss = 0.15548091\n",
      "Iteration 309, loss = 0.15536726\n",
      "Iteration 310, loss = 0.15524761\n",
      "Iteration 311, loss = 0.15517212\n",
      "Iteration 312, loss = 0.15505194\n",
      "Iteration 313, loss = 0.15499140\n",
      "Iteration 314, loss = 0.15489385\n",
      "Iteration 315, loss = 0.15478059\n",
      "Iteration 316, loss = 0.15468195\n",
      "Iteration 317, loss = 0.15469590\n",
      "Iteration 318, loss = 0.15448923\n",
      "Iteration 319, loss = 0.15442158\n",
      "Iteration 320, loss = 0.15433155\n",
      "Iteration 321, loss = 0.15424046\n",
      "Iteration 322, loss = 0.15415358\n",
      "Iteration 323, loss = 0.15407038\n",
      "Iteration 324, loss = 0.15400280\n",
      "Iteration 325, loss = 0.15390832\n",
      "Iteration 326, loss = 0.15385783\n",
      "Iteration 327, loss = 0.15374996\n",
      "Iteration 328, loss = 0.15367486\n",
      "Iteration 329, loss = 0.15360148\n",
      "Iteration 330, loss = 0.15352367\n",
      "Iteration 331, loss = 0.15345725\n",
      "Iteration 332, loss = 0.15339001\n",
      "Iteration 333, loss = 0.15331967\n",
      "Iteration 334, loss = 0.15324398\n",
      "Iteration 335, loss = 0.15315413\n",
      "Iteration 336, loss = 0.15310355\n",
      "Iteration 337, loss = 0.15301117\n",
      "Iteration 338, loss = 0.15295067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.58037614\n",
      "Iteration 2, loss = 0.46121817\n",
      "Iteration 3, loss = 0.38594943\n",
      "Iteration 4, loss = 0.33837717\n",
      "Iteration 5, loss = 0.30550393\n",
      "Iteration 6, loss = 0.28423783\n",
      "Iteration 7, loss = 0.26711419\n",
      "Iteration 8, loss = 0.25401008\n",
      "Iteration 9, loss = 0.24379221\n",
      "Iteration 10, loss = 0.23428171\n",
      "Iteration 11, loss = 0.22624992\n",
      "Iteration 12, loss = 0.22004871\n",
      "Iteration 13, loss = 0.21332962\n",
      "Iteration 14, loss = 0.20776624\n",
      "Iteration 15, loss = 0.20240112\n",
      "Iteration 16, loss = 0.19757309\n",
      "Iteration 17, loss = 0.19366883\n",
      "Iteration 18, loss = 0.18924802\n",
      "Iteration 19, loss = 0.18582829\n",
      "Iteration 20, loss = 0.18301341\n",
      "Iteration 21, loss = 0.17945557\n",
      "Iteration 22, loss = 0.17665712\n",
      "Iteration 23, loss = 0.17400768\n",
      "Iteration 24, loss = 0.17159876\n",
      "Iteration 25, loss = 0.16946546\n",
      "Iteration 26, loss = 0.16751057\n",
      "Iteration 27, loss = 0.16557679\n",
      "Iteration 28, loss = 0.16386799\n",
      "Iteration 29, loss = 0.16217243\n",
      "Iteration 30, loss = 0.16108329\n",
      "Iteration 31, loss = 0.15909264\n",
      "Iteration 32, loss = 0.15795671\n",
      "Iteration 33, loss = 0.15681225\n",
      "Iteration 34, loss = 0.15573507\n",
      "Iteration 35, loss = 0.15495016\n",
      "Iteration 36, loss = 0.15409982\n",
      "Iteration 37, loss = 0.15334020\n",
      "Iteration 38, loss = 0.15280492\n",
      "Iteration 39, loss = 0.15243945\n",
      "Iteration 40, loss = 0.15212135\n",
      "Iteration 41, loss = 0.15129638\n",
      "Iteration 42, loss = 0.15097377\n",
      "Iteration 43, loss = 0.15061839\n",
      "Iteration 44, loss = 0.15043316\n",
      "Iteration 45, loss = 0.15010757\n",
      "Iteration 46, loss = 0.14992436\n",
      "Iteration 47, loss = 0.14953094\n",
      "Iteration 48, loss = 0.14937693\n",
      "Iteration 49, loss = 0.14914315\n",
      "Iteration 50, loss = 0.14922145\n",
      "Iteration 51, loss = 0.14930767\n",
      "Iteration 52, loss = 0.14893537\n",
      "Iteration 53, loss = 0.14862806\n",
      "Iteration 54, loss = 0.14891869\n",
      "Iteration 55, loss = 0.14847502\n",
      "Iteration 56, loss = 0.14873192\n",
      "Iteration 57, loss = 0.14842772\n",
      "Iteration 58, loss = 0.14796040\n",
      "Iteration 59, loss = 0.14813216\n",
      "Iteration 60, loss = 0.14812002\n",
      "Iteration 61, loss = 0.14824086\n",
      "Iteration 62, loss = 0.14794747\n",
      "Iteration 63, loss = 0.14804810\n",
      "Iteration 64, loss = 0.14775082\n",
      "Iteration 65, loss = 0.14794481\n",
      "Iteration 66, loss = 0.14759561\n",
      "Iteration 67, loss = 0.14766124\n",
      "Iteration 68, loss = 0.14766238\n",
      "Iteration 69, loss = 0.14738662\n",
      "Iteration 70, loss = 0.14781303\n",
      "Iteration 71, loss = 0.14732488\n",
      "Iteration 72, loss = 0.14738691\n",
      "Iteration 73, loss = 0.14725686\n",
      "Iteration 74, loss = 0.14758194\n",
      "Iteration 75, loss = 0.14726762\n",
      "Iteration 76, loss = 0.14741714\n",
      "Iteration 77, loss = 0.14707248\n",
      "Iteration 78, loss = 0.14714038\n",
      "Iteration 79, loss = 0.14699913\n",
      "Iteration 80, loss = 0.14712329\n",
      "Iteration 81, loss = 0.14740585\n",
      "Iteration 82, loss = 0.14698313\n",
      "Iteration 83, loss = 0.14696066\n",
      "Iteration 84, loss = 0.14716646\n",
      "Iteration 85, loss = 0.14704494\n",
      "Iteration 86, loss = 0.14669875\n",
      "Iteration 87, loss = 0.14685215\n",
      "Iteration 88, loss = 0.14677598\n",
      "Iteration 89, loss = 0.14677444\n",
      "Iteration 90, loss = 0.14669370\n",
      "Iteration 91, loss = 0.14695473\n",
      "Iteration 92, loss = 0.14664040\n",
      "Iteration 93, loss = 0.14677859\n",
      "Iteration 94, loss = 0.14667074\n",
      "Iteration 95, loss = 0.14683744\n",
      "Iteration 96, loss = 0.14684167\n",
      "Iteration 97, loss = 0.14654170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.62605125\n",
      "Iteration 2, loss = 0.60977401\n",
      "Iteration 3, loss = 0.59400648\n",
      "Iteration 4, loss = 0.57928312\n",
      "Iteration 5, loss = 0.56458202\n",
      "Iteration 6, loss = 0.55123597\n",
      "Iteration 7, loss = 0.53817453\n",
      "Iteration 8, loss = 0.52604722\n",
      "Iteration 9, loss = 0.51452067\n",
      "Iteration 10, loss = 0.50343732\n",
      "Iteration 11, loss = 0.49273988\n",
      "Iteration 12, loss = 0.48294638\n",
      "Iteration 13, loss = 0.47378083\n",
      "Iteration 14, loss = 0.46463534\n",
      "Iteration 15, loss = 0.45626149\n",
      "Iteration 16, loss = 0.44844908\n",
      "Iteration 17, loss = 0.44077092\n",
      "Iteration 18, loss = 0.43325562\n",
      "Iteration 19, loss = 0.42631691\n",
      "Iteration 20, loss = 0.41964217\n",
      "Iteration 21, loss = 0.41346650\n",
      "Iteration 22, loss = 0.40727169\n",
      "Iteration 23, loss = 0.40154839\n",
      "Iteration 24, loss = 0.39599885\n",
      "Iteration 25, loss = 0.39057598\n",
      "Iteration 26, loss = 0.38530253\n",
      "Iteration 27, loss = 0.38036432\n",
      "Iteration 28, loss = 0.37552393\n",
      "Iteration 29, loss = 0.37102423\n",
      "Iteration 30, loss = 0.36667520\n",
      "Iteration 31, loss = 0.36237491\n",
      "Iteration 32, loss = 0.35831757\n",
      "Iteration 33, loss = 0.35442348\n",
      "Iteration 34, loss = 0.35066908\n",
      "Iteration 35, loss = 0.34698356\n",
      "Iteration 36, loss = 0.34347297\n",
      "Iteration 37, loss = 0.34010829\n",
      "Iteration 38, loss = 0.33692162\n",
      "Iteration 39, loss = 0.33375189\n",
      "Iteration 40, loss = 0.33074918\n",
      "Iteration 41, loss = 0.32780594\n",
      "Iteration 42, loss = 0.32493094\n",
      "Iteration 43, loss = 0.32205110\n",
      "Iteration 44, loss = 0.31941092\n",
      "Iteration 45, loss = 0.31688515\n",
      "Iteration 46, loss = 0.31432236\n",
      "Iteration 47, loss = 0.31183324\n",
      "Iteration 48, loss = 0.30942630\n",
      "Iteration 49, loss = 0.30700990\n",
      "Iteration 50, loss = 0.30466719\n",
      "Iteration 51, loss = 0.30250825\n",
      "Iteration 52, loss = 0.30034550\n",
      "Iteration 53, loss = 0.29810911\n",
      "Iteration 54, loss = 0.29603312\n",
      "Iteration 55, loss = 0.29403901\n",
      "Iteration 56, loss = 0.29209484\n",
      "Iteration 57, loss = 0.29019636\n",
      "Iteration 58, loss = 0.28829158\n",
      "Iteration 59, loss = 0.28645129\n",
      "Iteration 60, loss = 0.28472366\n",
      "Iteration 61, loss = 0.28291437\n",
      "Iteration 62, loss = 0.28119286\n",
      "Iteration 63, loss = 0.27954952\n",
      "Iteration 64, loss = 0.27790972\n",
      "Iteration 65, loss = 0.27629997\n",
      "Iteration 66, loss = 0.27476523\n",
      "Iteration 67, loss = 0.27319538\n",
      "Iteration 68, loss = 0.27168096\n",
      "Iteration 69, loss = 0.27022797\n",
      "Iteration 70, loss = 0.26883513\n",
      "Iteration 71, loss = 0.26734559\n",
      "Iteration 72, loss = 0.26597577\n",
      "Iteration 73, loss = 0.26459985\n",
      "Iteration 74, loss = 0.26330803\n",
      "Iteration 75, loss = 0.26194315\n",
      "Iteration 76, loss = 0.26065313\n",
      "Iteration 77, loss = 0.25938987\n",
      "Iteration 78, loss = 0.25813809\n",
      "Iteration 79, loss = 0.25689048\n",
      "Iteration 80, loss = 0.25570424\n",
      "Iteration 81, loss = 0.25449399\n",
      "Iteration 82, loss = 0.25336395\n",
      "Iteration 83, loss = 0.25220961\n",
      "Iteration 84, loss = 0.25108446\n",
      "Iteration 85, loss = 0.24999863\n",
      "Iteration 86, loss = 0.24886212\n",
      "Iteration 87, loss = 0.24779793\n",
      "Iteration 88, loss = 0.24671456\n",
      "Iteration 89, loss = 0.24565659\n",
      "Iteration 90, loss = 0.24461620\n",
      "Iteration 91, loss = 0.24358251\n",
      "Iteration 92, loss = 0.24254795\n",
      "Iteration 93, loss = 0.24161886\n",
      "Iteration 94, loss = 0.24057081\n",
      "Iteration 95, loss = 0.23963477\n",
      "Iteration 96, loss = 0.23866790\n",
      "Iteration 97, loss = 0.23773789\n",
      "Iteration 98, loss = 0.23680730\n",
      "Iteration 99, loss = 0.23584106\n",
      "Iteration 100, loss = 0.23492806\n",
      "Iteration 101, loss = 0.23402573\n",
      "Iteration 102, loss = 0.23311695\n",
      "Iteration 103, loss = 0.23220816\n",
      "Iteration 104, loss = 0.23136921\n",
      "Iteration 105, loss = 0.23045781\n",
      "Iteration 106, loss = 0.22956505\n",
      "Iteration 107, loss = 0.22878094\n",
      "Iteration 108, loss = 0.22797950\n",
      "Iteration 109, loss = 0.22716483\n",
      "Iteration 110, loss = 0.22635608\n",
      "Iteration 111, loss = 0.22559989\n",
      "Iteration 112, loss = 0.22482207\n",
      "Iteration 113, loss = 0.22404837\n",
      "Iteration 114, loss = 0.22328943\n",
      "Iteration 115, loss = 0.22251916\n",
      "Iteration 116, loss = 0.22180680\n",
      "Iteration 117, loss = 0.22106303\n",
      "Iteration 118, loss = 0.22035764\n",
      "Iteration 119, loss = 0.21963546\n",
      "Iteration 120, loss = 0.21891743\n",
      "Iteration 121, loss = 0.21820479\n",
      "Iteration 122, loss = 0.21751903\n",
      "Iteration 123, loss = 0.21680479\n",
      "Iteration 124, loss = 0.21608218\n",
      "Iteration 125, loss = 0.21543363\n",
      "Iteration 126, loss = 0.21473088\n",
      "Iteration 127, loss = 0.21411862\n",
      "Iteration 128, loss = 0.21344471\n",
      "Iteration 129, loss = 0.21280029\n",
      "Iteration 130, loss = 0.21216766\n",
      "Iteration 131, loss = 0.21153776\n",
      "Iteration 132, loss = 0.21090069\n",
      "Iteration 133, loss = 0.21031142\n",
      "Iteration 134, loss = 0.20972988\n",
      "Iteration 135, loss = 0.20909382\n",
      "Iteration 136, loss = 0.20849681\n",
      "Iteration 137, loss = 0.20793562\n",
      "Iteration 138, loss = 0.20735282\n",
      "Iteration 139, loss = 0.20681555\n",
      "Iteration 140, loss = 0.20624105\n",
      "Iteration 141, loss = 0.20569153\n",
      "Iteration 142, loss = 0.20512454\n",
      "Iteration 143, loss = 0.20461978\n",
      "Iteration 144, loss = 0.20408531\n",
      "Iteration 145, loss = 0.20351328\n",
      "Iteration 146, loss = 0.20295894\n",
      "Iteration 147, loss = 0.20242762\n",
      "Iteration 148, loss = 0.20190130\n",
      "Iteration 149, loss = 0.20136741\n",
      "Iteration 150, loss = 0.20085408\n",
      "Iteration 151, loss = 0.20027351\n",
      "Iteration 152, loss = 0.19979917\n",
      "Iteration 153, loss = 0.19925330\n",
      "Iteration 154, loss = 0.19875070\n",
      "Iteration 155, loss = 0.19822857\n",
      "Iteration 156, loss = 0.19773778\n",
      "Iteration 157, loss = 0.19727967\n",
      "Iteration 158, loss = 0.19675017\n",
      "Iteration 159, loss = 0.19624639\n",
      "Iteration 160, loss = 0.19576365\n",
      "Iteration 161, loss = 0.19530468\n",
      "Iteration 162, loss = 0.19483512\n",
      "Iteration 163, loss = 0.19433044\n",
      "Iteration 164, loss = 0.19384250\n",
      "Iteration 165, loss = 0.19340756\n",
      "Iteration 166, loss = 0.19290133\n",
      "Iteration 167, loss = 0.19246276\n",
      "Iteration 168, loss = 0.19199370\n",
      "Iteration 169, loss = 0.19149463\n",
      "Iteration 170, loss = 0.19106984\n",
      "Iteration 171, loss = 0.19063274\n",
      "Iteration 172, loss = 0.19018410\n",
      "Iteration 173, loss = 0.18974656\n",
      "Iteration 174, loss = 0.18929295\n",
      "Iteration 175, loss = 0.18887152\n",
      "Iteration 176, loss = 0.18848332\n",
      "Iteration 177, loss = 0.18807871\n",
      "Iteration 178, loss = 0.18773123\n",
      "Iteration 179, loss = 0.18730519\n",
      "Iteration 180, loss = 0.18692445\n",
      "Iteration 181, loss = 0.18654682\n",
      "Iteration 182, loss = 0.18618606\n",
      "Iteration 183, loss = 0.18581880\n",
      "Iteration 184, loss = 0.18549134\n",
      "Iteration 185, loss = 0.18512550\n",
      "Iteration 186, loss = 0.18482662\n",
      "Iteration 187, loss = 0.18450509\n",
      "Iteration 188, loss = 0.18413864\n",
      "Iteration 189, loss = 0.18387342\n",
      "Iteration 190, loss = 0.18351883\n",
      "Iteration 191, loss = 0.18319562\n",
      "Iteration 192, loss = 0.18288483\n",
      "Iteration 193, loss = 0.18259743\n",
      "Iteration 194, loss = 0.18229740\n",
      "Iteration 195, loss = 0.18197329\n",
      "Iteration 196, loss = 0.18170300\n",
      "Iteration 197, loss = 0.18139188\n",
      "Iteration 198, loss = 0.18110999\n",
      "Iteration 199, loss = 0.18082959\n",
      "Iteration 200, loss = 0.18057210\n",
      "Iteration 201, loss = 0.18028641\n",
      "Iteration 202, loss = 0.18002582\n",
      "Iteration 203, loss = 0.17975458\n",
      "Iteration 204, loss = 0.17949459\n",
      "Iteration 205, loss = 0.17918549\n",
      "Iteration 206, loss = 0.17893979\n",
      "Iteration 207, loss = 0.17867648\n",
      "Iteration 208, loss = 0.17844668\n",
      "Iteration 209, loss = 0.17821838\n",
      "Iteration 210, loss = 0.17792739\n",
      "Iteration 211, loss = 0.17769244\n",
      "Iteration 212, loss = 0.17744549\n",
      "Iteration 213, loss = 0.17724206\n",
      "Iteration 214, loss = 0.17696005\n",
      "Iteration 215, loss = 0.17675949\n",
      "Iteration 216, loss = 0.17648208\n",
      "Iteration 217, loss = 0.17624849\n",
      "Iteration 218, loss = 0.17603853\n",
      "Iteration 219, loss = 0.17581885\n",
      "Iteration 220, loss = 0.17561631\n",
      "Iteration 221, loss = 0.17536915\n",
      "Iteration 222, loss = 0.17517569\n",
      "Iteration 223, loss = 0.17497410\n",
      "Iteration 224, loss = 0.17471828\n",
      "Iteration 225, loss = 0.17449712\n",
      "Iteration 226, loss = 0.17426652\n",
      "Iteration 227, loss = 0.17406389\n",
      "Iteration 228, loss = 0.17385653\n",
      "Iteration 229, loss = 0.17364355\n",
      "Iteration 230, loss = 0.17345628\n",
      "Iteration 231, loss = 0.17322210\n",
      "Iteration 232, loss = 0.17302614\n",
      "Iteration 233, loss = 0.17283533\n",
      "Iteration 234, loss = 0.17266982\n",
      "Iteration 235, loss = 0.17243406\n",
      "Iteration 236, loss = 0.17225226\n",
      "Iteration 237, loss = 0.17206539\n",
      "Iteration 238, loss = 0.17184755\n",
      "Iteration 239, loss = 0.17169061\n",
      "Iteration 240, loss = 0.17145816\n",
      "Iteration 241, loss = 0.17127246\n",
      "Iteration 242, loss = 0.17107804\n",
      "Iteration 243, loss = 0.17088048\n",
      "Iteration 244, loss = 0.17069723\n",
      "Iteration 245, loss = 0.17049785\n",
      "Iteration 246, loss = 0.17030736\n",
      "Iteration 247, loss = 0.17012121\n",
      "Iteration 248, loss = 0.16994784\n",
      "Iteration 249, loss = 0.16974926\n",
      "Iteration 250, loss = 0.16955645\n",
      "Iteration 251, loss = 0.16937756\n",
      "Iteration 252, loss = 0.16917917\n",
      "Iteration 253, loss = 0.16901940\n",
      "Iteration 254, loss = 0.16881290\n",
      "Iteration 255, loss = 0.16858022\n",
      "Iteration 256, loss = 0.16838847\n",
      "Iteration 257, loss = 0.16820247\n",
      "Iteration 258, loss = 0.16800560\n",
      "Iteration 259, loss = 0.16782461\n",
      "Iteration 260, loss = 0.16769331\n",
      "Iteration 261, loss = 0.16748405\n",
      "Iteration 262, loss = 0.16729561\n",
      "Iteration 263, loss = 0.16710773\n",
      "Iteration 264, loss = 0.16692254\n",
      "Iteration 265, loss = 0.16678200\n",
      "Iteration 266, loss = 0.16660155\n",
      "Iteration 267, loss = 0.16641388\n",
      "Iteration 268, loss = 0.16623908\n",
      "Iteration 269, loss = 0.16607289\n",
      "Iteration 270, loss = 0.16588610\n",
      "Iteration 271, loss = 0.16573130\n",
      "Iteration 272, loss = 0.16555796\n",
      "Iteration 273, loss = 0.16538443\n",
      "Iteration 274, loss = 0.16522651\n",
      "Iteration 275, loss = 0.16506923\n",
      "Iteration 276, loss = 0.16491513\n",
      "Iteration 277, loss = 0.16473837\n",
      "Iteration 278, loss = 0.16465450\n",
      "Iteration 279, loss = 0.16451200\n",
      "Iteration 280, loss = 0.16429231\n",
      "Iteration 281, loss = 0.16415712\n",
      "Iteration 282, loss = 0.16402387\n",
      "Iteration 283, loss = 0.16389330\n",
      "Iteration 284, loss = 0.16376418\n",
      "Iteration 285, loss = 0.16360255\n",
      "Iteration 286, loss = 0.16346581\n",
      "Iteration 287, loss = 0.16335515\n",
      "Iteration 288, loss = 0.16319306\n",
      "Iteration 289, loss = 0.16308616\n",
      "Iteration 290, loss = 0.16295524\n",
      "Iteration 291, loss = 0.16280411\n",
      "Iteration 292, loss = 0.16271315\n",
      "Iteration 293, loss = 0.16256686\n",
      "Iteration 294, loss = 0.16247300\n",
      "Iteration 295, loss = 0.16235602\n",
      "Iteration 296, loss = 0.16223203\n",
      "Iteration 297, loss = 0.16211896\n",
      "Iteration 298, loss = 0.16200237\n",
      "Iteration 299, loss = 0.16189283\n",
      "Iteration 300, loss = 0.16180240\n",
      "Iteration 301, loss = 0.16168565\n",
      "Iteration 302, loss = 0.16157838\n",
      "Iteration 303, loss = 0.16148316\n",
      "Iteration 304, loss = 0.16138280\n",
      "Iteration 305, loss = 0.16128314\n",
      "Iteration 306, loss = 0.16119419\n",
      "Iteration 307, loss = 0.16108779\n",
      "Iteration 308, loss = 0.16098455\n",
      "Iteration 309, loss = 0.16088548\n",
      "Iteration 310, loss = 0.16077475\n",
      "Iteration 311, loss = 0.16070431\n",
      "Iteration 312, loss = 0.16059953\n",
      "Iteration 313, loss = 0.16056628\n",
      "Iteration 314, loss = 0.16044382\n",
      "Iteration 315, loss = 0.16033697\n",
      "Iteration 316, loss = 0.16025390\n",
      "Iteration 317, loss = 0.16022935\n",
      "Iteration 318, loss = 0.16007092\n",
      "Iteration 319, loss = 0.16002141\n",
      "Iteration 320, loss = 0.15990818\n",
      "Iteration 321, loss = 0.15984164\n",
      "Iteration 322, loss = 0.15977744\n",
      "Iteration 323, loss = 0.15969342\n",
      "Iteration 324, loss = 0.15964487\n",
      "Iteration 325, loss = 0.15954284\n",
      "Iteration 326, loss = 0.15948122\n",
      "Iteration 327, loss = 0.15940091\n",
      "Iteration 328, loss = 0.15932641\n",
      "Iteration 329, loss = 0.15926980\n",
      "Iteration 330, loss = 0.15918944\n",
      "Iteration 331, loss = 0.15911904\n",
      "Iteration 332, loss = 0.15906170\n",
      "Iteration 333, loss = 0.15899795\n",
      "Iteration 334, loss = 0.15894302\n",
      "Iteration 335, loss = 0.15883644\n",
      "Iteration 336, loss = 0.15879867\n",
      "Iteration 337, loss = 0.15870301\n",
      "Iteration 338, loss = 0.15865172\n",
      "Iteration 339, loss = 0.15855651\n",
      "Iteration 340, loss = 0.15850670\n",
      "Iteration 341, loss = 0.15844455\n",
      "Iteration 342, loss = 0.15838229\n",
      "Iteration 343, loss = 0.15834604\n",
      "Iteration 344, loss = 0.15826865\n",
      "Iteration 345, loss = 0.15821991\n",
      "Iteration 346, loss = 0.15812554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.pdf\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "# https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(14,)] , #[(14,),(100,),(14,4),(7,7,)] \n",
    "    'mlp__activation' : ['relu'], #['logistic','tanh','relu']\n",
    "    'mlp__solver' : ['adam'],\n",
    "    'mlp__alpha' : [1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , *1\n",
    "    'mlp__batch_size' : [99], # ~297 samples for train at inner cv -> 3 equal size batches , #[16,32,64,128,'auto']\n",
    "    'mlp__learning_rate' : ['constant'],\n",
    "    'mlp__learning_rate_init' : [0.1,0.01,0.001],\n",
    "    'mlp__max_iter' : [1000],\n",
    "    'mlp__verbose' : [1]\n",
    "}\n",
    "\n",
    "search_adam = GridSearchCV(clf_mlp, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_adam, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.968     0.986     0.977       212\n",
      "   Malignant      0.986     0.967     0.976       212\n",
      "\n",
      "    accuracy                          0.976       424\n",
      "   macro avg      0.977     0.976     0.976       424\n",
      "weighted avg      0.977     0.976     0.976       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Solver : SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.55962501\n",
      "Iteration 2, loss = 0.52498810\n",
      "Iteration 3, loss = 0.48567734\n",
      "Iteration 4, loss = 0.44951747\n",
      "Iteration 5, loss = 0.41764446\n",
      "Iteration 6, loss = 0.39069424\n",
      "Iteration 7, loss = 0.36853233\n",
      "Iteration 8, loss = 0.34871989\n",
      "Iteration 9, loss = 0.33158988\n",
      "Iteration 10, loss = 0.31694796\n",
      "Iteration 11, loss = 0.30433546\n",
      "Iteration 12, loss = 0.29277844\n",
      "Iteration 13, loss = 0.28290326\n",
      "Iteration 14, loss = 0.27383673\n",
      "Iteration 15, loss = 0.26593811\n",
      "Iteration 16, loss = 0.25853336\n",
      "Iteration 17, loss = 0.25222302\n",
      "Iteration 18, loss = 0.24628894\n",
      "Iteration 19, loss = 0.24073073\n",
      "Iteration 20, loss = 0.23584484\n",
      "Iteration 21, loss = 0.23141642\n",
      "Iteration 22, loss = 0.22738690\n",
      "Iteration 23, loss = 0.22344367\n",
      "Iteration 24, loss = 0.21991796\n",
      "Iteration 25, loss = 0.21651548\n",
      "Iteration 26, loss = 0.21340597\n",
      "Iteration 27, loss = 0.21039532\n",
      "Iteration 28, loss = 0.20754578\n",
      "Iteration 29, loss = 0.20502403\n",
      "Iteration 30, loss = 0.20263586\n",
      "Iteration 31, loss = 0.20018712\n",
      "Iteration 32, loss = 0.19794899\n",
      "Iteration 33, loss = 0.19591310\n",
      "Iteration 34, loss = 0.19381341\n",
      "Iteration 35, loss = 0.19198188\n",
      "Iteration 36, loss = 0.19008201\n",
      "Iteration 37, loss = 0.18834180\n",
      "Iteration 38, loss = 0.18669117\n",
      "Iteration 39, loss = 0.18500978\n",
      "Iteration 40, loss = 0.18338113\n",
      "Iteration 41, loss = 0.18198261\n",
      "Iteration 42, loss = 0.18053629\n",
      "Iteration 43, loss = 0.17904999\n",
      "Iteration 44, loss = 0.17771361\n",
      "Iteration 45, loss = 0.17638397\n",
      "Iteration 46, loss = 0.17513509\n",
      "Iteration 47, loss = 0.17386725\n",
      "Iteration 48, loss = 0.17260853\n",
      "Iteration 49, loss = 0.17146129\n",
      "Iteration 50, loss = 0.17036093\n",
      "Iteration 51, loss = 0.16925949\n",
      "Iteration 52, loss = 0.16821434\n",
      "Iteration 53, loss = 0.16708964\n",
      "Iteration 54, loss = 0.16608220\n",
      "Iteration 55, loss = 0.16505353\n",
      "Iteration 56, loss = 0.16411730\n",
      "Iteration 57, loss = 0.16318844\n",
      "Iteration 58, loss = 0.16225591\n",
      "Iteration 59, loss = 0.16138428\n",
      "Iteration 60, loss = 0.16042778\n",
      "Iteration 61, loss = 0.15959418\n",
      "Iteration 62, loss = 0.15880611\n",
      "Iteration 63, loss = 0.15796207\n",
      "Iteration 64, loss = 0.15707996\n",
      "Iteration 65, loss = 0.15629933\n",
      "Iteration 66, loss = 0.15541101\n",
      "Iteration 67, loss = 0.15456339\n",
      "Iteration 68, loss = 0.15374334\n",
      "Iteration 69, loss = 0.15293902\n",
      "Iteration 70, loss = 0.15214567\n",
      "Iteration 71, loss = 0.15129463\n",
      "Iteration 72, loss = 0.15061270\n",
      "Iteration 73, loss = 0.14976957\n",
      "Iteration 74, loss = 0.14909888\n",
      "Iteration 75, loss = 0.14834507\n",
      "Iteration 76, loss = 0.14754320\n",
      "Iteration 77, loss = 0.14687921\n",
      "Iteration 78, loss = 0.14614574\n",
      "Iteration 79, loss = 0.14550505\n",
      "Iteration 80, loss = 0.14479403\n",
      "Iteration 81, loss = 0.14419597\n",
      "Iteration 82, loss = 0.14351937\n",
      "Iteration 83, loss = 0.14286991\n",
      "Iteration 84, loss = 0.14218364\n",
      "Iteration 85, loss = 0.14154925\n",
      "Iteration 86, loss = 0.14090398\n",
      "Iteration 87, loss = 0.14030540\n",
      "Iteration 88, loss = 0.13966970\n",
      "Iteration 89, loss = 0.13907182\n",
      "Iteration 90, loss = 0.13848126\n",
      "Iteration 91, loss = 0.13792230\n",
      "Iteration 92, loss = 0.13733860\n",
      "Iteration 93, loss = 0.13673171\n",
      "Iteration 94, loss = 0.13612578\n",
      "Iteration 95, loss = 0.13558939\n",
      "Iteration 96, loss = 0.13502130\n",
      "Iteration 97, loss = 0.13450467\n",
      "Iteration 98, loss = 0.13396066\n",
      "Iteration 99, loss = 0.13334841\n",
      "Iteration 100, loss = 0.13283897\n",
      "Iteration 101, loss = 0.13232348\n",
      "Iteration 102, loss = 0.13185170\n",
      "Iteration 103, loss = 0.13128971\n",
      "Iteration 104, loss = 0.13080513\n",
      "Iteration 105, loss = 0.13031076\n",
      "Iteration 106, loss = 0.12984275\n",
      "Iteration 107, loss = 0.12934428\n",
      "Iteration 108, loss = 0.12887994\n",
      "Iteration 109, loss = 0.12844439\n",
      "Iteration 110, loss = 0.12795900\n",
      "Iteration 111, loss = 0.12755895\n",
      "Iteration 112, loss = 0.12706127\n",
      "Iteration 113, loss = 0.12664406\n",
      "Iteration 114, loss = 0.12619572\n",
      "Iteration 115, loss = 0.12579906\n",
      "Iteration 116, loss = 0.12536220\n",
      "Iteration 117, loss = 0.12498359\n",
      "Iteration 118, loss = 0.12451848\n",
      "Iteration 119, loss = 0.12418606\n",
      "Iteration 120, loss = 0.12377539\n",
      "Iteration 121, loss = 0.12336055\n",
      "Iteration 122, loss = 0.12301229\n",
      "Iteration 123, loss = 0.12264822\n",
      "Iteration 124, loss = 0.12227134\n",
      "Iteration 125, loss = 0.12193569\n",
      "Iteration 126, loss = 0.12158180\n",
      "Iteration 127, loss = 0.12129548\n",
      "Iteration 128, loss = 0.12086765\n",
      "Iteration 129, loss = 0.12056992\n",
      "Iteration 130, loss = 0.12025638\n",
      "Iteration 131, loss = 0.11992941\n",
      "Iteration 132, loss = 0.11966047\n",
      "Iteration 133, loss = 0.11927433\n",
      "Iteration 134, loss = 0.11898707\n",
      "Iteration 135, loss = 0.11869734\n",
      "Iteration 136, loss = 0.11837762\n",
      "Iteration 137, loss = 0.11808314\n",
      "Iteration 138, loss = 0.11787660\n",
      "Iteration 139, loss = 0.11752429\n",
      "Iteration 140, loss = 0.11728216\n",
      "Iteration 141, loss = 0.11701594\n",
      "Iteration 142, loss = 0.11672878\n",
      "Iteration 143, loss = 0.11649779\n",
      "Iteration 144, loss = 0.11619580\n",
      "Iteration 145, loss = 0.11593352\n",
      "Iteration 146, loss = 0.11572510\n",
      "Iteration 147, loss = 0.11546953\n",
      "Iteration 148, loss = 0.11519697\n",
      "Iteration 149, loss = 0.11497637\n",
      "Iteration 150, loss = 0.11477191\n",
      "Iteration 151, loss = 0.11450748\n",
      "Iteration 152, loss = 0.11426810\n",
      "Iteration 153, loss = 0.11405196\n",
      "Iteration 154, loss = 0.11383370\n",
      "Iteration 155, loss = 0.11358320\n",
      "Iteration 156, loss = 0.11335589\n",
      "Iteration 157, loss = 0.11308526\n",
      "Iteration 158, loss = 0.11288938\n",
      "Iteration 159, loss = 0.11267740\n",
      "Iteration 160, loss = 0.11245285\n",
      "Iteration 161, loss = 0.11224963\n",
      "Iteration 162, loss = 0.11205026\n",
      "Iteration 163, loss = 0.11186169\n",
      "Iteration 164, loss = 0.11160100\n",
      "Iteration 165, loss = 0.11140262\n",
      "Iteration 166, loss = 0.11121490\n",
      "Iteration 167, loss = 0.11102970\n",
      "Iteration 168, loss = 0.11083644\n",
      "Iteration 169, loss = 0.11063740\n",
      "Iteration 170, loss = 0.11045998\n",
      "Iteration 171, loss = 0.11024485\n",
      "Iteration 172, loss = 0.11003208\n",
      "Iteration 173, loss = 0.10985983\n",
      "Iteration 174, loss = 0.10963249\n",
      "Iteration 175, loss = 0.10943054\n",
      "Iteration 176, loss = 0.10921090\n",
      "Iteration 177, loss = 0.10900254\n",
      "Iteration 178, loss = 0.10883127\n",
      "Iteration 179, loss = 0.10863182\n",
      "Iteration 180, loss = 0.10846763\n",
      "Iteration 181, loss = 0.10823380\n",
      "Iteration 182, loss = 0.10814353\n",
      "Iteration 183, loss = 0.10794614\n",
      "Iteration 184, loss = 0.10768889\n",
      "Iteration 185, loss = 0.10751473\n",
      "Iteration 186, loss = 0.10735164\n",
      "Iteration 187, loss = 0.10718032\n",
      "Iteration 188, loss = 0.10700317\n",
      "Iteration 189, loss = 0.10683088\n",
      "Iteration 190, loss = 0.10665378\n",
      "Iteration 191, loss = 0.10649569\n",
      "Iteration 192, loss = 0.10634955\n",
      "Iteration 193, loss = 0.10620751\n",
      "Iteration 194, loss = 0.10600055\n",
      "Iteration 195, loss = 0.10589094\n",
      "Iteration 196, loss = 0.10570941\n",
      "Iteration 197, loss = 0.10559987\n",
      "Iteration 198, loss = 0.10543082\n",
      "Iteration 199, loss = 0.10528441\n",
      "Iteration 200, loss = 0.10514385\n",
      "Iteration 201, loss = 0.10499379\n",
      "Iteration 202, loss = 0.10489244\n",
      "Iteration 203, loss = 0.10475212\n",
      "Iteration 204, loss = 0.10459873\n",
      "Iteration 205, loss = 0.10446473\n",
      "Iteration 206, loss = 0.10434379\n",
      "Iteration 207, loss = 0.10426908\n",
      "Iteration 208, loss = 0.10410385\n",
      "Iteration 209, loss = 0.10398158\n",
      "Iteration 210, loss = 0.10386017\n",
      "Iteration 211, loss = 0.10371746\n",
      "Iteration 212, loss = 0.10362386\n",
      "Iteration 213, loss = 0.10346875\n",
      "Iteration 214, loss = 0.10336672\n",
      "Iteration 215, loss = 0.10326486\n",
      "Iteration 216, loss = 0.10312747\n",
      "Iteration 217, loss = 0.10304060\n",
      "Iteration 218, loss = 0.10289763\n",
      "Iteration 219, loss = 0.10279190\n",
      "Iteration 220, loss = 0.10267085\n",
      "Iteration 221, loss = 0.10258796\n",
      "Iteration 222, loss = 0.10249037\n",
      "Iteration 223, loss = 0.10234324\n",
      "Iteration 224, loss = 0.10227260\n",
      "Iteration 225, loss = 0.10215560\n",
      "Iteration 226, loss = 0.10204020\n",
      "Iteration 227, loss = 0.10194037\n",
      "Iteration 228, loss = 0.10186521\n",
      "Iteration 229, loss = 0.10176571\n",
      "Iteration 230, loss = 0.10165889\n",
      "Iteration 231, loss = 0.10156085\n",
      "Iteration 232, loss = 0.10145351\n",
      "Iteration 233, loss = 0.10139970\n",
      "Iteration 234, loss = 0.10129735\n",
      "Iteration 235, loss = 0.10122592\n",
      "Iteration 236, loss = 0.10109082\n",
      "Iteration 237, loss = 0.10099436\n",
      "Iteration 238, loss = 0.10094993\n",
      "Iteration 239, loss = 0.10081776\n",
      "Iteration 240, loss = 0.10073609\n",
      "Iteration 241, loss = 0.10064634\n",
      "Iteration 242, loss = 0.10056389\n",
      "Iteration 243, loss = 0.10050210\n",
      "Iteration 244, loss = 0.10040317\n",
      "Iteration 245, loss = 0.10033267\n",
      "Iteration 246, loss = 0.10026421\n",
      "Iteration 247, loss = 0.10015062\n",
      "Iteration 248, loss = 0.10008009\n",
      "Iteration 249, loss = 0.09999663\n",
      "Iteration 250, loss = 0.09992947\n",
      "Iteration 251, loss = 0.09982515\n",
      "Iteration 252, loss = 0.09977217\n",
      "Iteration 253, loss = 0.09970418\n",
      "Iteration 254, loss = 0.09961794\n",
      "Iteration 255, loss = 0.09953914\n",
      "Iteration 256, loss = 0.09946445\n",
      "Iteration 257, loss = 0.09942564\n",
      "Iteration 258, loss = 0.09932349\n",
      "Iteration 259, loss = 0.09924084\n",
      "Iteration 260, loss = 0.09919162\n",
      "Iteration 261, loss = 0.09914226\n",
      "Iteration 262, loss = 0.09903693\n",
      "Iteration 263, loss = 0.09895886\n",
      "Iteration 264, loss = 0.09896948\n",
      "Iteration 265, loss = 0.09883845\n",
      "Iteration 266, loss = 0.09873681\n",
      "Iteration 267, loss = 0.09865854\n",
      "Iteration 268, loss = 0.09860656\n",
      "Iteration 269, loss = 0.09856700\n",
      "Iteration 270, loss = 0.09845277\n",
      "Iteration 271, loss = 0.09843326\n",
      "Iteration 272, loss = 0.09834076\n",
      "Iteration 273, loss = 0.09825676\n",
      "Iteration 274, loss = 0.09820730\n",
      "Iteration 275, loss = 0.09817103\n",
      "Iteration 276, loss = 0.09808859\n",
      "Iteration 277, loss = 0.09803212\n",
      "Iteration 278, loss = 0.09795876\n",
      "Iteration 279, loss = 0.09788888\n",
      "Iteration 280, loss = 0.09780082\n",
      "Iteration 281, loss = 0.09774210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.55871810\n",
      "Iteration 2, loss = 0.52313781\n",
      "Iteration 3, loss = 0.48354561\n",
      "Iteration 4, loss = 0.44654275\n",
      "Iteration 5, loss = 0.41411469\n",
      "Iteration 6, loss = 0.38690269\n",
      "Iteration 7, loss = 0.36450732\n",
      "Iteration 8, loss = 0.34457404\n",
      "Iteration 9, loss = 0.32747426\n",
      "Iteration 10, loss = 0.31277485\n",
      "Iteration 11, loss = 0.30018899\n",
      "Iteration 12, loss = 0.28860538\n",
      "Iteration 13, loss = 0.27874303\n",
      "Iteration 14, loss = 0.26963964\n",
      "Iteration 15, loss = 0.26173835\n",
      "Iteration 16, loss = 0.25433670\n",
      "Iteration 17, loss = 0.24801723\n",
      "Iteration 18, loss = 0.24208522\n",
      "Iteration 19, loss = 0.23644380\n",
      "Iteration 20, loss = 0.23150757\n",
      "Iteration 21, loss = 0.22707520\n",
      "Iteration 22, loss = 0.22288956\n",
      "Iteration 23, loss = 0.21901377\n",
      "Iteration 24, loss = 0.21538254\n",
      "Iteration 25, loss = 0.21191528\n",
      "Iteration 26, loss = 0.20880630\n",
      "Iteration 27, loss = 0.20576136\n",
      "Iteration 28, loss = 0.20290497\n",
      "Iteration 29, loss = 0.20029594\n",
      "Iteration 30, loss = 0.19786210\n",
      "Iteration 31, loss = 0.19541704\n",
      "Iteration 32, loss = 0.19311143\n",
      "Iteration 33, loss = 0.19103727\n",
      "Iteration 34, loss = 0.18890880\n",
      "Iteration 35, loss = 0.18703352\n",
      "Iteration 36, loss = 0.18510314\n",
      "Iteration 37, loss = 0.18331694\n",
      "Iteration 38, loss = 0.18162172\n",
      "Iteration 39, loss = 0.17993125\n",
      "Iteration 40, loss = 0.17825621\n",
      "Iteration 41, loss = 0.17686722\n",
      "Iteration 42, loss = 0.17533644\n",
      "Iteration 43, loss = 0.17383459\n",
      "Iteration 44, loss = 0.17240888\n",
      "Iteration 45, loss = 0.17109942\n",
      "Iteration 46, loss = 0.16982636\n",
      "Iteration 47, loss = 0.16853746\n",
      "Iteration 48, loss = 0.16724801\n",
      "Iteration 49, loss = 0.16610648\n",
      "Iteration 50, loss = 0.16499028\n",
      "Iteration 51, loss = 0.16387207\n",
      "Iteration 52, loss = 0.16281105\n",
      "Iteration 53, loss = 0.16169118\n",
      "Iteration 54, loss = 0.16064472\n",
      "Iteration 55, loss = 0.15962741\n",
      "Iteration 56, loss = 0.15867410\n",
      "Iteration 57, loss = 0.15773479\n",
      "Iteration 58, loss = 0.15679215\n",
      "Iteration 59, loss = 0.15590792\n",
      "Iteration 60, loss = 0.15494787\n",
      "Iteration 61, loss = 0.15410866\n",
      "Iteration 62, loss = 0.15328477\n",
      "Iteration 63, loss = 0.15238532\n",
      "Iteration 64, loss = 0.15147665\n",
      "Iteration 65, loss = 0.15068753\n",
      "Iteration 66, loss = 0.14980069\n",
      "Iteration 67, loss = 0.14897741\n",
      "Iteration 68, loss = 0.14818080\n",
      "Iteration 69, loss = 0.14740124\n",
      "Iteration 70, loss = 0.14657561\n",
      "Iteration 71, loss = 0.14572260\n",
      "Iteration 72, loss = 0.14501060\n",
      "Iteration 73, loss = 0.14414608\n",
      "Iteration 74, loss = 0.14344487\n",
      "Iteration 75, loss = 0.14269108\n",
      "Iteration 76, loss = 0.14189076\n",
      "Iteration 77, loss = 0.14119491\n",
      "Iteration 78, loss = 0.14042068\n",
      "Iteration 79, loss = 0.13971120\n",
      "Iteration 80, loss = 0.13891744\n",
      "Iteration 81, loss = 0.13824264\n",
      "Iteration 82, loss = 0.13752931\n",
      "Iteration 83, loss = 0.13680883\n",
      "Iteration 84, loss = 0.13610892\n",
      "Iteration 85, loss = 0.13543322\n",
      "Iteration 86, loss = 0.13475893\n",
      "Iteration 87, loss = 0.13412136\n",
      "Iteration 88, loss = 0.13339720\n",
      "Iteration 89, loss = 0.13275834\n",
      "Iteration 90, loss = 0.13212097\n",
      "Iteration 91, loss = 0.13154753\n",
      "Iteration 92, loss = 0.13092861\n",
      "Iteration 93, loss = 0.13033451\n",
      "Iteration 94, loss = 0.12966700\n",
      "Iteration 95, loss = 0.12909750\n",
      "Iteration 96, loss = 0.12848705\n",
      "Iteration 97, loss = 0.12793986\n",
      "Iteration 98, loss = 0.12737832\n",
      "Iteration 99, loss = 0.12673112\n",
      "Iteration 100, loss = 0.12619591\n",
      "Iteration 101, loss = 0.12562965\n",
      "Iteration 102, loss = 0.12512870\n",
      "Iteration 103, loss = 0.12453521\n",
      "Iteration 104, loss = 0.12400594\n",
      "Iteration 105, loss = 0.12347440\n",
      "Iteration 106, loss = 0.12298304\n",
      "Iteration 107, loss = 0.12244605\n",
      "Iteration 108, loss = 0.12194630\n",
      "Iteration 109, loss = 0.12149130\n",
      "Iteration 110, loss = 0.12095610\n",
      "Iteration 111, loss = 0.12052379\n",
      "Iteration 112, loss = 0.12000493\n",
      "Iteration 113, loss = 0.11957900\n",
      "Iteration 114, loss = 0.11908645\n",
      "Iteration 115, loss = 0.11866665\n",
      "Iteration 116, loss = 0.11820301\n",
      "Iteration 117, loss = 0.11780675\n",
      "Iteration 118, loss = 0.11732001\n",
      "Iteration 119, loss = 0.11695976\n",
      "Iteration 120, loss = 0.11651913\n",
      "Iteration 121, loss = 0.11611165\n",
      "Iteration 122, loss = 0.11574079\n",
      "Iteration 123, loss = 0.11534079\n",
      "Iteration 124, loss = 0.11496606\n",
      "Iteration 125, loss = 0.11460592\n",
      "Iteration 126, loss = 0.11424663\n",
      "Iteration 127, loss = 0.11393719\n",
      "Iteration 128, loss = 0.11351102\n",
      "Iteration 129, loss = 0.11319710\n",
      "Iteration 130, loss = 0.11286755\n",
      "Iteration 131, loss = 0.11255320\n",
      "Iteration 132, loss = 0.11223254\n",
      "Iteration 133, loss = 0.11186631\n",
      "Iteration 134, loss = 0.11157230\n",
      "Iteration 135, loss = 0.11126673\n",
      "Iteration 136, loss = 0.11093845\n",
      "Iteration 137, loss = 0.11065969\n",
      "Iteration 138, loss = 0.11040552\n",
      "Iteration 139, loss = 0.11005430\n",
      "Iteration 140, loss = 0.10977490\n",
      "Iteration 141, loss = 0.10951774\n",
      "Iteration 142, loss = 0.10922077\n",
      "Iteration 143, loss = 0.10897762\n",
      "Iteration 144, loss = 0.10866870\n",
      "Iteration 145, loss = 0.10840628\n",
      "Iteration 146, loss = 0.10818094\n",
      "Iteration 147, loss = 0.10788987\n",
      "Iteration 148, loss = 0.10758994\n",
      "Iteration 149, loss = 0.10732829\n",
      "Iteration 150, loss = 0.10707018\n",
      "Iteration 151, loss = 0.10678874\n",
      "Iteration 152, loss = 0.10652106\n",
      "Iteration 153, loss = 0.10626742\n",
      "Iteration 154, loss = 0.10601863\n",
      "Iteration 155, loss = 0.10575432\n",
      "Iteration 156, loss = 0.10550907\n",
      "Iteration 157, loss = 0.10521667\n",
      "Iteration 158, loss = 0.10501743\n",
      "Iteration 159, loss = 0.10479330\n",
      "Iteration 160, loss = 0.10454750\n",
      "Iteration 161, loss = 0.10433358\n",
      "Iteration 162, loss = 0.10411572\n",
      "Iteration 163, loss = 0.10391254\n",
      "Iteration 164, loss = 0.10364514\n",
      "Iteration 165, loss = 0.10344380\n",
      "Iteration 166, loss = 0.10323099\n",
      "Iteration 167, loss = 0.10305039\n",
      "Iteration 168, loss = 0.10284449\n",
      "Iteration 169, loss = 0.10263857\n",
      "Iteration 170, loss = 0.10244596\n",
      "Iteration 171, loss = 0.10219739\n",
      "Iteration 172, loss = 0.10200700\n",
      "Iteration 173, loss = 0.10180896\n",
      "Iteration 174, loss = 0.10158287\n",
      "Iteration 175, loss = 0.10139013\n",
      "Iteration 176, loss = 0.10118210\n",
      "Iteration 177, loss = 0.10099216\n",
      "Iteration 178, loss = 0.10080486\n",
      "Iteration 179, loss = 0.10061847\n",
      "Iteration 180, loss = 0.10042979\n",
      "Iteration 181, loss = 0.10022973\n",
      "Iteration 182, loss = 0.10011382\n",
      "Iteration 183, loss = 0.09996128\n",
      "Iteration 184, loss = 0.09969886\n",
      "Iteration 185, loss = 0.09954018\n",
      "Iteration 186, loss = 0.09937262\n",
      "Iteration 187, loss = 0.09920292\n",
      "Iteration 188, loss = 0.09904040\n",
      "Iteration 189, loss = 0.09886186\n",
      "Iteration 190, loss = 0.09870736\n",
      "Iteration 191, loss = 0.09854310\n",
      "Iteration 192, loss = 0.09840753\n",
      "Iteration 193, loss = 0.09825909\n",
      "Iteration 194, loss = 0.09804568\n",
      "Iteration 195, loss = 0.09791905\n",
      "Iteration 196, loss = 0.09776377\n",
      "Iteration 197, loss = 0.09762340\n",
      "Iteration 198, loss = 0.09745284\n",
      "Iteration 199, loss = 0.09729600\n",
      "Iteration 200, loss = 0.09715439\n",
      "Iteration 201, loss = 0.09700044\n",
      "Iteration 202, loss = 0.09688352\n",
      "Iteration 203, loss = 0.09674165\n",
      "Iteration 204, loss = 0.09658063\n",
      "Iteration 205, loss = 0.09644814\n",
      "Iteration 206, loss = 0.09631718\n",
      "Iteration 207, loss = 0.09620431\n",
      "Iteration 208, loss = 0.09605944\n",
      "Iteration 209, loss = 0.09589999\n",
      "Iteration 210, loss = 0.09578670\n",
      "Iteration 211, loss = 0.09563722\n",
      "Iteration 212, loss = 0.09553497\n",
      "Iteration 213, loss = 0.09538320\n",
      "Iteration 214, loss = 0.09526733\n",
      "Iteration 215, loss = 0.09515909\n",
      "Iteration 216, loss = 0.09499696\n",
      "Iteration 217, loss = 0.09489733\n",
      "Iteration 218, loss = 0.09475195\n",
      "Iteration 219, loss = 0.09464924\n",
      "Iteration 220, loss = 0.09451462\n",
      "Iteration 221, loss = 0.09439386\n",
      "Iteration 222, loss = 0.09428485\n",
      "Iteration 223, loss = 0.09414188\n",
      "Iteration 224, loss = 0.09405322\n",
      "Iteration 225, loss = 0.09391995\n",
      "Iteration 226, loss = 0.09380211\n",
      "Iteration 227, loss = 0.09368380\n",
      "Iteration 228, loss = 0.09358808\n",
      "Iteration 229, loss = 0.09347876\n",
      "Iteration 230, loss = 0.09336828\n",
      "Iteration 231, loss = 0.09326009\n",
      "Iteration 232, loss = 0.09313338\n",
      "Iteration 233, loss = 0.09307839\n",
      "Iteration 234, loss = 0.09295321\n",
      "Iteration 235, loss = 0.09288619\n",
      "Iteration 236, loss = 0.09274430\n",
      "Iteration 237, loss = 0.09262332\n",
      "Iteration 238, loss = 0.09255967\n",
      "Iteration 239, loss = 0.09243405\n",
      "Iteration 240, loss = 0.09234984\n",
      "Iteration 241, loss = 0.09223845\n",
      "Iteration 242, loss = 0.09214606\n",
      "Iteration 243, loss = 0.09207970\n",
      "Iteration 244, loss = 0.09197965\n",
      "Iteration 245, loss = 0.09188358\n",
      "Iteration 246, loss = 0.09179966\n",
      "Iteration 247, loss = 0.09168432\n",
      "Iteration 248, loss = 0.09159363\n",
      "Iteration 249, loss = 0.09149797\n",
      "Iteration 250, loss = 0.09143230\n",
      "Iteration 251, loss = 0.09131519\n",
      "Iteration 252, loss = 0.09124080\n",
      "Iteration 253, loss = 0.09118461\n",
      "Iteration 254, loss = 0.09107897\n",
      "Iteration 255, loss = 0.09097835\n",
      "Iteration 256, loss = 0.09088540\n",
      "Iteration 257, loss = 0.09081858\n",
      "Iteration 258, loss = 0.09069671\n",
      "Iteration 259, loss = 0.09061088\n",
      "Iteration 260, loss = 0.09053622\n",
      "Iteration 261, loss = 0.09046035\n",
      "Iteration 262, loss = 0.09035177\n",
      "Iteration 263, loss = 0.09024836\n",
      "Iteration 264, loss = 0.09022546\n",
      "Iteration 265, loss = 0.09006626\n",
      "Iteration 266, loss = 0.08996907\n",
      "Iteration 267, loss = 0.08987590\n",
      "Iteration 268, loss = 0.08980031\n",
      "Iteration 269, loss = 0.08974821\n",
      "Iteration 270, loss = 0.08962215\n",
      "Iteration 271, loss = 0.08955946\n",
      "Iteration 272, loss = 0.08946112\n",
      "Iteration 273, loss = 0.08936253\n",
      "Iteration 274, loss = 0.08931276\n",
      "Iteration 275, loss = 0.08924744\n",
      "Iteration 276, loss = 0.08913715\n",
      "Iteration 277, loss = 0.08907004\n",
      "Iteration 278, loss = 0.08900007\n",
      "Iteration 279, loss = 0.08891357\n",
      "Iteration 280, loss = 0.08880188\n",
      "Iteration 281, loss = 0.08871412\n",
      "Iteration 282, loss = 0.08865096\n",
      "Iteration 283, loss = 0.08857922\n",
      "Iteration 284, loss = 0.08851295\n",
      "Iteration 285, loss = 0.08848218\n",
      "Iteration 286, loss = 0.08834527\n",
      "Iteration 287, loss = 0.08830134\n",
      "Iteration 288, loss = 0.08825113\n",
      "Iteration 289, loss = 0.08815688\n",
      "Iteration 290, loss = 0.08809102\n",
      "Iteration 291, loss = 0.08803270\n",
      "Iteration 292, loss = 0.08798506\n",
      "Iteration 293, loss = 0.08791851\n",
      "Iteration 294, loss = 0.08780733\n",
      "Iteration 295, loss = 0.08777301\n",
      "Iteration 296, loss = 0.08770238\n",
      "Iteration 297, loss = 0.08761193\n",
      "Iteration 298, loss = 0.08755892\n",
      "Iteration 299, loss = 0.08747935\n",
      "Iteration 300, loss = 0.08740179\n",
      "Iteration 301, loss = 0.08739155\n",
      "Iteration 302, loss = 0.08728526\n",
      "Iteration 303, loss = 0.08728547\n",
      "Iteration 304, loss = 0.08716434\n",
      "Iteration 305, loss = 0.08709338\n",
      "Iteration 306, loss = 0.08705808\n",
      "Iteration 307, loss = 0.08696926\n",
      "Iteration 308, loss = 0.08691330\n",
      "Iteration 309, loss = 0.08687392\n",
      "Iteration 310, loss = 0.08679780\n",
      "Iteration 311, loss = 0.08672354\n",
      "Iteration 312, loss = 0.08667023\n",
      "Iteration 313, loss = 0.08663200\n",
      "Iteration 314, loss = 0.08658494\n",
      "Iteration 315, loss = 0.08651888\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56377849\n",
      "Iteration 2, loss = 0.52732318\n",
      "Iteration 3, loss = 0.48590106\n",
      "Iteration 4, loss = 0.44714239\n",
      "Iteration 5, loss = 0.41356654\n",
      "Iteration 6, loss = 0.38546801\n",
      "Iteration 7, loss = 0.36177659\n",
      "Iteration 8, loss = 0.34121605\n",
      "Iteration 9, loss = 0.32380578\n",
      "Iteration 10, loss = 0.30870081\n",
      "Iteration 11, loss = 0.29525107\n",
      "Iteration 12, loss = 0.28352057\n",
      "Iteration 13, loss = 0.27345299\n",
      "Iteration 14, loss = 0.26389690\n",
      "Iteration 15, loss = 0.25579410\n",
      "Iteration 16, loss = 0.24830114\n",
      "Iteration 17, loss = 0.24161621\n",
      "Iteration 18, loss = 0.23562407\n",
      "Iteration 19, loss = 0.23006178\n",
      "Iteration 20, loss = 0.22503164\n",
      "Iteration 21, loss = 0.22052411\n",
      "Iteration 22, loss = 0.21614128\n",
      "Iteration 23, loss = 0.21230792\n",
      "Iteration 24, loss = 0.20856509\n",
      "Iteration 25, loss = 0.20517971\n",
      "Iteration 26, loss = 0.20208080\n",
      "Iteration 27, loss = 0.19896649\n",
      "Iteration 28, loss = 0.19612784\n",
      "Iteration 29, loss = 0.19356883\n",
      "Iteration 30, loss = 0.19112405\n",
      "Iteration 31, loss = 0.18872499\n",
      "Iteration 32, loss = 0.18639755\n",
      "Iteration 33, loss = 0.18436850\n",
      "Iteration 34, loss = 0.18229246\n",
      "Iteration 35, loss = 0.18038588\n",
      "Iteration 36, loss = 0.17846670\n",
      "Iteration 37, loss = 0.17670012\n",
      "Iteration 38, loss = 0.17504812\n",
      "Iteration 39, loss = 0.17332303\n",
      "Iteration 40, loss = 0.17171483\n",
      "Iteration 41, loss = 0.17027426\n",
      "Iteration 42, loss = 0.16879629\n",
      "Iteration 43, loss = 0.16740874\n",
      "Iteration 44, loss = 0.16599047\n",
      "Iteration 45, loss = 0.16471027\n",
      "Iteration 46, loss = 0.16346310\n",
      "Iteration 47, loss = 0.16217173\n",
      "Iteration 48, loss = 0.16094782\n",
      "Iteration 49, loss = 0.15983520\n",
      "Iteration 50, loss = 0.15870299\n",
      "Iteration 51, loss = 0.15763928\n",
      "Iteration 52, loss = 0.15652413\n",
      "Iteration 53, loss = 0.15550647\n",
      "Iteration 54, loss = 0.15451829\n",
      "Iteration 55, loss = 0.15350361\n",
      "Iteration 56, loss = 0.15257489\n",
      "Iteration 57, loss = 0.15160976\n",
      "Iteration 58, loss = 0.15073885\n",
      "Iteration 59, loss = 0.14982751\n",
      "Iteration 60, loss = 0.14892166\n",
      "Iteration 61, loss = 0.14806313\n",
      "Iteration 62, loss = 0.14729329\n",
      "Iteration 63, loss = 0.14648354\n",
      "Iteration 64, loss = 0.14566104\n",
      "Iteration 65, loss = 0.14486644\n",
      "Iteration 66, loss = 0.14411789\n",
      "Iteration 67, loss = 0.14338313\n",
      "Iteration 68, loss = 0.14263638\n",
      "Iteration 69, loss = 0.14192562\n",
      "Iteration 70, loss = 0.14119732\n",
      "Iteration 71, loss = 0.14046830\n",
      "Iteration 72, loss = 0.13974986\n",
      "Iteration 73, loss = 0.13903819\n",
      "Iteration 74, loss = 0.13841080\n",
      "Iteration 75, loss = 0.13768355\n",
      "Iteration 76, loss = 0.13699263\n",
      "Iteration 77, loss = 0.13636743\n",
      "Iteration 78, loss = 0.13571107\n",
      "Iteration 79, loss = 0.13503667\n",
      "Iteration 80, loss = 0.13441713\n",
      "Iteration 81, loss = 0.13374250\n",
      "Iteration 82, loss = 0.13313609\n",
      "Iteration 83, loss = 0.13252350\n",
      "Iteration 84, loss = 0.13182763\n",
      "Iteration 85, loss = 0.13121867\n",
      "Iteration 86, loss = 0.13058798\n",
      "Iteration 87, loss = 0.13002825\n",
      "Iteration 88, loss = 0.12942753\n",
      "Iteration 89, loss = 0.12883104\n",
      "Iteration 90, loss = 0.12819693\n",
      "Iteration 91, loss = 0.12772580\n",
      "Iteration 92, loss = 0.12700322\n",
      "Iteration 93, loss = 0.12641899\n",
      "Iteration 94, loss = 0.12582497\n",
      "Iteration 95, loss = 0.12525002\n",
      "Iteration 96, loss = 0.12468879\n",
      "Iteration 97, loss = 0.12415249\n",
      "Iteration 98, loss = 0.12358747\n",
      "Iteration 99, loss = 0.12305209\n",
      "Iteration 100, loss = 0.12259086\n",
      "Iteration 101, loss = 0.12198831\n",
      "Iteration 102, loss = 0.12148217\n",
      "Iteration 103, loss = 0.12094609\n",
      "Iteration 104, loss = 0.12045036\n",
      "Iteration 105, loss = 0.11999305\n",
      "Iteration 106, loss = 0.11946149\n",
      "Iteration 107, loss = 0.11902893\n",
      "Iteration 108, loss = 0.11856055\n",
      "Iteration 109, loss = 0.11811070\n",
      "Iteration 110, loss = 0.11763370\n",
      "Iteration 111, loss = 0.11720235\n",
      "Iteration 112, loss = 0.11675315\n",
      "Iteration 113, loss = 0.11633021\n",
      "Iteration 114, loss = 0.11591772\n",
      "Iteration 115, loss = 0.11551442\n",
      "Iteration 116, loss = 0.11504229\n",
      "Iteration 117, loss = 0.11463965\n",
      "Iteration 118, loss = 0.11425578\n",
      "Iteration 119, loss = 0.11389333\n",
      "Iteration 120, loss = 0.11340748\n",
      "Iteration 121, loss = 0.11304417\n",
      "Iteration 122, loss = 0.11264354\n",
      "Iteration 123, loss = 0.11229200\n",
      "Iteration 124, loss = 0.11192929\n",
      "Iteration 125, loss = 0.11153702\n",
      "Iteration 126, loss = 0.11117776\n",
      "Iteration 127, loss = 0.11087762\n",
      "Iteration 128, loss = 0.11045139\n",
      "Iteration 129, loss = 0.11018125\n",
      "Iteration 130, loss = 0.10978091\n",
      "Iteration 131, loss = 0.10944605\n",
      "Iteration 132, loss = 0.10914810\n",
      "Iteration 133, loss = 0.10880065\n",
      "Iteration 134, loss = 0.10845133\n",
      "Iteration 135, loss = 0.10815287\n",
      "Iteration 136, loss = 0.10784914\n",
      "Iteration 137, loss = 0.10757274\n",
      "Iteration 138, loss = 0.10723804\n",
      "Iteration 139, loss = 0.10697193\n",
      "Iteration 140, loss = 0.10665130\n",
      "Iteration 141, loss = 0.10638943\n",
      "Iteration 142, loss = 0.10607977\n",
      "Iteration 143, loss = 0.10582672\n",
      "Iteration 144, loss = 0.10553195\n",
      "Iteration 145, loss = 0.10529914\n",
      "Iteration 146, loss = 0.10504431\n",
      "Iteration 147, loss = 0.10477438\n",
      "Iteration 148, loss = 0.10454134\n",
      "Iteration 149, loss = 0.10429385\n",
      "Iteration 150, loss = 0.10405950\n",
      "Iteration 151, loss = 0.10379882\n",
      "Iteration 152, loss = 0.10360297\n",
      "Iteration 153, loss = 0.10336890\n",
      "Iteration 154, loss = 0.10312807\n",
      "Iteration 155, loss = 0.10292265\n",
      "Iteration 156, loss = 0.10270712\n",
      "Iteration 157, loss = 0.10247197\n",
      "Iteration 158, loss = 0.10225354\n",
      "Iteration 159, loss = 0.10203355\n",
      "Iteration 160, loss = 0.10183273\n",
      "Iteration 161, loss = 0.10162271\n",
      "Iteration 162, loss = 0.10145850\n",
      "Iteration 163, loss = 0.10122669\n",
      "Iteration 164, loss = 0.10106799\n",
      "Iteration 165, loss = 0.10083670\n",
      "Iteration 166, loss = 0.10066493\n",
      "Iteration 167, loss = 0.10047951\n",
      "Iteration 168, loss = 0.10032098\n",
      "Iteration 169, loss = 0.10009596\n",
      "Iteration 170, loss = 0.09992351\n",
      "Iteration 171, loss = 0.09975307\n",
      "Iteration 172, loss = 0.09959611\n",
      "Iteration 173, loss = 0.09944913\n",
      "Iteration 174, loss = 0.09922435\n",
      "Iteration 175, loss = 0.09909556\n",
      "Iteration 176, loss = 0.09892313\n",
      "Iteration 177, loss = 0.09876731\n",
      "Iteration 178, loss = 0.09861627\n",
      "Iteration 179, loss = 0.09845011\n",
      "Iteration 180, loss = 0.09827873\n",
      "Iteration 181, loss = 0.09813495\n",
      "Iteration 182, loss = 0.09798846\n",
      "Iteration 183, loss = 0.09786870\n",
      "Iteration 184, loss = 0.09769655\n",
      "Iteration 185, loss = 0.09758182\n",
      "Iteration 186, loss = 0.09741550\n",
      "Iteration 187, loss = 0.09726378\n",
      "Iteration 188, loss = 0.09712760\n",
      "Iteration 189, loss = 0.09700067\n",
      "Iteration 190, loss = 0.09685937\n",
      "Iteration 191, loss = 0.09674049\n",
      "Iteration 192, loss = 0.09661516\n",
      "Iteration 193, loss = 0.09647025\n",
      "Iteration 194, loss = 0.09634877\n",
      "Iteration 195, loss = 0.09622889\n",
      "Iteration 196, loss = 0.09610890\n",
      "Iteration 197, loss = 0.09596828\n",
      "Iteration 198, loss = 0.09584062\n",
      "Iteration 199, loss = 0.09571534\n",
      "Iteration 200, loss = 0.09563496\n",
      "Iteration 201, loss = 0.09547887\n",
      "Iteration 202, loss = 0.09537651\n",
      "Iteration 203, loss = 0.09525843\n",
      "Iteration 204, loss = 0.09511465\n",
      "Iteration 205, loss = 0.09503308\n",
      "Iteration 206, loss = 0.09490111\n",
      "Iteration 207, loss = 0.09479382\n",
      "Iteration 208, loss = 0.09471551\n",
      "Iteration 209, loss = 0.09458948\n",
      "Iteration 210, loss = 0.09447202\n",
      "Iteration 211, loss = 0.09435247\n",
      "Iteration 212, loss = 0.09425835\n",
      "Iteration 213, loss = 0.09417875\n",
      "Iteration 214, loss = 0.09403297\n",
      "Iteration 215, loss = 0.09399309\n",
      "Iteration 216, loss = 0.09381658\n",
      "Iteration 217, loss = 0.09375038\n",
      "Iteration 218, loss = 0.09362963\n",
      "Iteration 219, loss = 0.09353018\n",
      "Iteration 220, loss = 0.09345371\n",
      "Iteration 221, loss = 0.09334928\n",
      "Iteration 222, loss = 0.09324404\n",
      "Iteration 223, loss = 0.09315355\n",
      "Iteration 224, loss = 0.09302301\n",
      "Iteration 225, loss = 0.09291568\n",
      "Iteration 226, loss = 0.09281744\n",
      "Iteration 227, loss = 0.09271574\n",
      "Iteration 228, loss = 0.09263989\n",
      "Iteration 229, loss = 0.09252723\n",
      "Iteration 230, loss = 0.09241428\n",
      "Iteration 231, loss = 0.09234347\n",
      "Iteration 232, loss = 0.09221753\n",
      "Iteration 233, loss = 0.09212058\n",
      "Iteration 234, loss = 0.09203759\n",
      "Iteration 235, loss = 0.09197016\n",
      "Iteration 236, loss = 0.09185076\n",
      "Iteration 237, loss = 0.09174888\n",
      "Iteration 238, loss = 0.09173425\n",
      "Iteration 239, loss = 0.09155730\n",
      "Iteration 240, loss = 0.09148776\n",
      "Iteration 241, loss = 0.09141236\n",
      "Iteration 242, loss = 0.09131605\n",
      "Iteration 243, loss = 0.09126971\n",
      "Iteration 244, loss = 0.09116952\n",
      "Iteration 245, loss = 0.09108102\n",
      "Iteration 246, loss = 0.09099909\n",
      "Iteration 247, loss = 0.09090415\n",
      "Iteration 248, loss = 0.09081207\n",
      "Iteration 249, loss = 0.09072722\n",
      "Iteration 250, loss = 0.09065275\n",
      "Iteration 251, loss = 0.09057038\n",
      "Iteration 252, loss = 0.09049369\n",
      "Iteration 253, loss = 0.09043221\n",
      "Iteration 254, loss = 0.09035195\n",
      "Iteration 255, loss = 0.09029666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56279363\n",
      "Iteration 2, loss = 0.52625039\n",
      "Iteration 3, loss = 0.48474136\n",
      "Iteration 4, loss = 0.44579684\n",
      "Iteration 5, loss = 0.41258754\n",
      "Iteration 6, loss = 0.38472034\n",
      "Iteration 7, loss = 0.36115988\n",
      "Iteration 8, loss = 0.34082340\n",
      "Iteration 9, loss = 0.32379011\n",
      "Iteration 10, loss = 0.30886600\n",
      "Iteration 11, loss = 0.29568529\n",
      "Iteration 12, loss = 0.28412196\n",
      "Iteration 13, loss = 0.27427846\n",
      "Iteration 14, loss = 0.26480915\n",
      "Iteration 15, loss = 0.25680549\n",
      "Iteration 16, loss = 0.24956057\n",
      "Iteration 17, loss = 0.24296442\n",
      "Iteration 18, loss = 0.23697751\n",
      "Iteration 19, loss = 0.23151325\n",
      "Iteration 20, loss = 0.22664863\n",
      "Iteration 21, loss = 0.22226613\n",
      "Iteration 22, loss = 0.21792844\n",
      "Iteration 23, loss = 0.21411427\n",
      "Iteration 24, loss = 0.21042230\n",
      "Iteration 25, loss = 0.20720221\n",
      "Iteration 26, loss = 0.20410677\n",
      "Iteration 27, loss = 0.20103113\n",
      "Iteration 28, loss = 0.19829912\n",
      "Iteration 29, loss = 0.19580695\n",
      "Iteration 30, loss = 0.19333273\n",
      "Iteration 31, loss = 0.19104494\n",
      "Iteration 32, loss = 0.18874467\n",
      "Iteration 33, loss = 0.18674571\n",
      "Iteration 34, loss = 0.18473955\n",
      "Iteration 35, loss = 0.18285365\n",
      "Iteration 36, loss = 0.18102352\n",
      "Iteration 37, loss = 0.17929893\n",
      "Iteration 38, loss = 0.17772987\n",
      "Iteration 39, loss = 0.17602896\n",
      "Iteration 40, loss = 0.17447455\n",
      "Iteration 41, loss = 0.17305023\n",
      "Iteration 42, loss = 0.17159392\n",
      "Iteration 43, loss = 0.17024642\n",
      "Iteration 44, loss = 0.16888641\n",
      "Iteration 45, loss = 0.16764958\n",
      "Iteration 46, loss = 0.16641862\n",
      "Iteration 47, loss = 0.16516140\n",
      "Iteration 48, loss = 0.16395898\n",
      "Iteration 49, loss = 0.16286365\n",
      "Iteration 50, loss = 0.16179983\n",
      "Iteration 51, loss = 0.16074245\n",
      "Iteration 52, loss = 0.15965377\n",
      "Iteration 53, loss = 0.15864496\n",
      "Iteration 54, loss = 0.15765743\n",
      "Iteration 55, loss = 0.15669181\n",
      "Iteration 56, loss = 0.15577976\n",
      "Iteration 57, loss = 0.15490093\n",
      "Iteration 58, loss = 0.15400774\n",
      "Iteration 59, loss = 0.15313616\n",
      "Iteration 60, loss = 0.15219994\n",
      "Iteration 61, loss = 0.15136481\n",
      "Iteration 62, loss = 0.15061081\n",
      "Iteration 63, loss = 0.14982549\n",
      "Iteration 64, loss = 0.14893900\n",
      "Iteration 65, loss = 0.14811234\n",
      "Iteration 66, loss = 0.14731370\n",
      "Iteration 67, loss = 0.14656551\n",
      "Iteration 68, loss = 0.14578461\n",
      "Iteration 69, loss = 0.14503745\n",
      "Iteration 70, loss = 0.14429430\n",
      "Iteration 71, loss = 0.14355157\n",
      "Iteration 72, loss = 0.14280830\n",
      "Iteration 73, loss = 0.14208748\n",
      "Iteration 74, loss = 0.14147400\n",
      "Iteration 75, loss = 0.14067507\n",
      "Iteration 76, loss = 0.13999199\n",
      "Iteration 77, loss = 0.13935962\n",
      "Iteration 78, loss = 0.13867199\n",
      "Iteration 79, loss = 0.13800722\n",
      "Iteration 80, loss = 0.13741779\n",
      "Iteration 81, loss = 0.13676945\n",
      "Iteration 82, loss = 0.13617815\n",
      "Iteration 83, loss = 0.13559220\n",
      "Iteration 84, loss = 0.13488213\n",
      "Iteration 85, loss = 0.13428680\n",
      "Iteration 86, loss = 0.13364504\n",
      "Iteration 87, loss = 0.13306607\n",
      "Iteration 88, loss = 0.13248278\n",
      "Iteration 89, loss = 0.13187971\n",
      "Iteration 90, loss = 0.13130858\n",
      "Iteration 91, loss = 0.13082406\n",
      "Iteration 92, loss = 0.13017037\n",
      "Iteration 93, loss = 0.12965040\n",
      "Iteration 94, loss = 0.12912918\n",
      "Iteration 95, loss = 0.12861574\n",
      "Iteration 96, loss = 0.12811473\n",
      "Iteration 97, loss = 0.12759783\n",
      "Iteration 98, loss = 0.12710238\n",
      "Iteration 99, loss = 0.12660861\n",
      "Iteration 100, loss = 0.12612750\n",
      "Iteration 101, loss = 0.12559294\n",
      "Iteration 102, loss = 0.12512721\n",
      "Iteration 103, loss = 0.12461707\n",
      "Iteration 104, loss = 0.12415231\n",
      "Iteration 105, loss = 0.12372047\n",
      "Iteration 106, loss = 0.12324102\n",
      "Iteration 107, loss = 0.12284252\n",
      "Iteration 108, loss = 0.12237391\n",
      "Iteration 109, loss = 0.12193395\n",
      "Iteration 110, loss = 0.12152560\n",
      "Iteration 111, loss = 0.12112457\n",
      "Iteration 112, loss = 0.12068809\n",
      "Iteration 113, loss = 0.12028230\n",
      "Iteration 114, loss = 0.11987358\n",
      "Iteration 115, loss = 0.11950309\n",
      "Iteration 116, loss = 0.11908362\n",
      "Iteration 117, loss = 0.11873578\n",
      "Iteration 118, loss = 0.11839646\n",
      "Iteration 119, loss = 0.11799954\n",
      "Iteration 120, loss = 0.11758289\n",
      "Iteration 121, loss = 0.11724369\n",
      "Iteration 122, loss = 0.11688248\n",
      "Iteration 123, loss = 0.11652949\n",
      "Iteration 124, loss = 0.11620177\n",
      "Iteration 125, loss = 0.11582414\n",
      "Iteration 126, loss = 0.11548770\n",
      "Iteration 127, loss = 0.11519326\n",
      "Iteration 128, loss = 0.11479398\n",
      "Iteration 129, loss = 0.11454163\n",
      "Iteration 130, loss = 0.11418079\n",
      "Iteration 131, loss = 0.11383721\n",
      "Iteration 132, loss = 0.11356116\n",
      "Iteration 133, loss = 0.11323090\n",
      "Iteration 134, loss = 0.11291691\n",
      "Iteration 135, loss = 0.11267098\n",
      "Iteration 136, loss = 0.11234692\n",
      "Iteration 137, loss = 0.11209566\n",
      "Iteration 138, loss = 0.11180057\n",
      "Iteration 139, loss = 0.11155476\n",
      "Iteration 140, loss = 0.11124146\n",
      "Iteration 141, loss = 0.11100355\n",
      "Iteration 142, loss = 0.11072599\n",
      "Iteration 143, loss = 0.11046357\n",
      "Iteration 144, loss = 0.11021267\n",
      "Iteration 145, loss = 0.10998734\n",
      "Iteration 146, loss = 0.10973894\n",
      "Iteration 147, loss = 0.10949128\n",
      "Iteration 148, loss = 0.10925723\n",
      "Iteration 149, loss = 0.10899415\n",
      "Iteration 150, loss = 0.10877598\n",
      "Iteration 151, loss = 0.10854490\n",
      "Iteration 152, loss = 0.10833431\n",
      "Iteration 153, loss = 0.10808511\n",
      "Iteration 154, loss = 0.10785899\n",
      "Iteration 155, loss = 0.10765089\n",
      "Iteration 156, loss = 0.10745233\n",
      "Iteration 157, loss = 0.10721047\n",
      "Iteration 158, loss = 0.10700062\n",
      "Iteration 159, loss = 0.10678358\n",
      "Iteration 160, loss = 0.10658330\n",
      "Iteration 161, loss = 0.10639776\n",
      "Iteration 162, loss = 0.10622284\n",
      "Iteration 163, loss = 0.10597194\n",
      "Iteration 164, loss = 0.10577999\n",
      "Iteration 165, loss = 0.10558355\n",
      "Iteration 166, loss = 0.10538736\n",
      "Iteration 167, loss = 0.10517641\n",
      "Iteration 168, loss = 0.10503595\n",
      "Iteration 169, loss = 0.10482184\n",
      "Iteration 170, loss = 0.10463090\n",
      "Iteration 171, loss = 0.10445275\n",
      "Iteration 172, loss = 0.10430719\n",
      "Iteration 173, loss = 0.10411219\n",
      "Iteration 174, loss = 0.10390059\n",
      "Iteration 175, loss = 0.10374867\n",
      "Iteration 176, loss = 0.10355910\n",
      "Iteration 177, loss = 0.10340313\n",
      "Iteration 178, loss = 0.10324138\n",
      "Iteration 179, loss = 0.10305637\n",
      "Iteration 180, loss = 0.10287146\n",
      "Iteration 181, loss = 0.10272879\n",
      "Iteration 182, loss = 0.10255853\n",
      "Iteration 183, loss = 0.10241995\n",
      "Iteration 184, loss = 0.10227251\n",
      "Iteration 185, loss = 0.10212735\n",
      "Iteration 186, loss = 0.10193447\n",
      "Iteration 187, loss = 0.10178007\n",
      "Iteration 188, loss = 0.10164231\n",
      "Iteration 189, loss = 0.10145643\n",
      "Iteration 190, loss = 0.10127202\n",
      "Iteration 191, loss = 0.10113365\n",
      "Iteration 192, loss = 0.10091401\n",
      "Iteration 193, loss = 0.10077065\n",
      "Iteration 194, loss = 0.10058578\n",
      "Iteration 195, loss = 0.10042070\n",
      "Iteration 196, loss = 0.10023246\n",
      "Iteration 197, loss = 0.10006092\n",
      "Iteration 198, loss = 0.09988680\n",
      "Iteration 199, loss = 0.09975265\n",
      "Iteration 200, loss = 0.09961737\n",
      "Iteration 201, loss = 0.09942180\n",
      "Iteration 202, loss = 0.09926739\n",
      "Iteration 203, loss = 0.09910157\n",
      "Iteration 204, loss = 0.09893894\n",
      "Iteration 205, loss = 0.09880655\n",
      "Iteration 206, loss = 0.09863450\n",
      "Iteration 207, loss = 0.09849711\n",
      "Iteration 208, loss = 0.09837506\n",
      "Iteration 209, loss = 0.09820756\n",
      "Iteration 210, loss = 0.09807347\n",
      "Iteration 211, loss = 0.09790885\n",
      "Iteration 212, loss = 0.09779067\n",
      "Iteration 213, loss = 0.09763784\n",
      "Iteration 214, loss = 0.09749591\n",
      "Iteration 215, loss = 0.09742234\n",
      "Iteration 216, loss = 0.09721932\n",
      "Iteration 217, loss = 0.09710521\n",
      "Iteration 218, loss = 0.09698190\n",
      "Iteration 219, loss = 0.09685419\n",
      "Iteration 220, loss = 0.09674059\n",
      "Iteration 221, loss = 0.09661048\n",
      "Iteration 222, loss = 0.09649694\n",
      "Iteration 223, loss = 0.09637918\n",
      "Iteration 224, loss = 0.09623979\n",
      "Iteration 225, loss = 0.09612865\n",
      "Iteration 226, loss = 0.09599120\n",
      "Iteration 227, loss = 0.09588786\n",
      "Iteration 228, loss = 0.09580015\n",
      "Iteration 229, loss = 0.09567222\n",
      "Iteration 230, loss = 0.09554657\n",
      "Iteration 231, loss = 0.09544859\n",
      "Iteration 232, loss = 0.09531621\n",
      "Iteration 233, loss = 0.09520663\n",
      "Iteration 234, loss = 0.09510676\n",
      "Iteration 235, loss = 0.09502722\n",
      "Iteration 236, loss = 0.09491806\n",
      "Iteration 237, loss = 0.09479850\n",
      "Iteration 238, loss = 0.09471245\n",
      "Iteration 239, loss = 0.09457372\n",
      "Iteration 240, loss = 0.09447934\n",
      "Iteration 241, loss = 0.09439551\n",
      "Iteration 242, loss = 0.09425550\n",
      "Iteration 243, loss = 0.09419044\n",
      "Iteration 244, loss = 0.09406981\n",
      "Iteration 245, loss = 0.09397106\n",
      "Iteration 246, loss = 0.09387952\n",
      "Iteration 247, loss = 0.09377989\n",
      "Iteration 248, loss = 0.09368102\n",
      "Iteration 249, loss = 0.09356552\n",
      "Iteration 250, loss = 0.09349761\n",
      "Iteration 251, loss = 0.09338390\n",
      "Iteration 252, loss = 0.09330324\n",
      "Iteration 253, loss = 0.09322187\n",
      "Iteration 254, loss = 0.09311596\n",
      "Iteration 255, loss = 0.09305811\n",
      "Iteration 256, loss = 0.09293525\n",
      "Iteration 257, loss = 0.09285620\n",
      "Iteration 258, loss = 0.09278599\n",
      "Iteration 259, loss = 0.09269417\n",
      "Iteration 260, loss = 0.09259749\n",
      "Iteration 261, loss = 0.09256291\n",
      "Iteration 262, loss = 0.09242701\n",
      "Iteration 263, loss = 0.09233722\n",
      "Iteration 264, loss = 0.09224695\n",
      "Iteration 265, loss = 0.09221477\n",
      "Iteration 266, loss = 0.09208962\n",
      "Iteration 267, loss = 0.09200809\n",
      "Iteration 268, loss = 0.09198934\n",
      "Iteration 269, loss = 0.09184907\n",
      "Iteration 270, loss = 0.09180241\n",
      "Iteration 271, loss = 0.09171152\n",
      "Iteration 272, loss = 0.09160494\n",
      "Iteration 273, loss = 0.09152231\n",
      "Iteration 274, loss = 0.09145909\n",
      "Iteration 275, loss = 0.09138530\n",
      "Iteration 276, loss = 0.09132570\n",
      "Iteration 277, loss = 0.09122778\n",
      "Iteration 278, loss = 0.09119242\n",
      "Iteration 279, loss = 0.09107888\n",
      "Iteration 280, loss = 0.09099643\n",
      "Iteration 281, loss = 0.09094172\n",
      "Iteration 282, loss = 0.09090325\n",
      "Iteration 283, loss = 0.09077496\n",
      "Iteration 284, loss = 0.09073034\n",
      "Iteration 285, loss = 0.09066966\n",
      "Iteration 286, loss = 0.09056655\n",
      "Iteration 287, loss = 0.09050020\n",
      "Iteration 288, loss = 0.09043758\n",
      "Iteration 289, loss = 0.09038864\n",
      "Iteration 290, loss = 0.09032867\n",
      "Iteration 291, loss = 0.09022383\n",
      "Iteration 292, loss = 0.09018768\n",
      "Iteration 293, loss = 0.09013420\n",
      "Iteration 294, loss = 0.09006720\n",
      "Iteration 295, loss = 0.08997312\n",
      "Iteration 296, loss = 0.08989894\n",
      "Iteration 297, loss = 0.08982079\n",
      "Iteration 298, loss = 0.08977062\n",
      "Iteration 299, loss = 0.08972473\n",
      "Iteration 300, loss = 0.08965885\n",
      "Iteration 301, loss = 0.08958912\n",
      "Iteration 302, loss = 0.08953586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56307060\n",
      "Iteration 2, loss = 0.52684056\n",
      "Iteration 3, loss = 0.48554033\n",
      "Iteration 4, loss = 0.44706473\n",
      "Iteration 5, loss = 0.41409725\n",
      "Iteration 6, loss = 0.38640722\n",
      "Iteration 7, loss = 0.36270492\n",
      "Iteration 8, loss = 0.34293704\n",
      "Iteration 9, loss = 0.32553172\n",
      "Iteration 10, loss = 0.31063665\n",
      "Iteration 11, loss = 0.29766351\n",
      "Iteration 12, loss = 0.28598561\n",
      "Iteration 13, loss = 0.27629049\n",
      "Iteration 14, loss = 0.26721984\n",
      "Iteration 15, loss = 0.25908562\n",
      "Iteration 16, loss = 0.25198583\n",
      "Iteration 17, loss = 0.24566539\n",
      "Iteration 18, loss = 0.23955015\n",
      "Iteration 19, loss = 0.23421994\n",
      "Iteration 20, loss = 0.22933582\n",
      "Iteration 21, loss = 0.22485836\n",
      "Iteration 22, loss = 0.22068599\n",
      "Iteration 23, loss = 0.21685068\n",
      "Iteration 24, loss = 0.21327749\n",
      "Iteration 25, loss = 0.20994563\n",
      "Iteration 26, loss = 0.20685337\n",
      "Iteration 27, loss = 0.20407478\n",
      "Iteration 28, loss = 0.20135305\n",
      "Iteration 29, loss = 0.19878017\n",
      "Iteration 30, loss = 0.19630406\n",
      "Iteration 31, loss = 0.19407904\n",
      "Iteration 32, loss = 0.19195379\n",
      "Iteration 33, loss = 0.18988382\n",
      "Iteration 34, loss = 0.18792390\n",
      "Iteration 35, loss = 0.18602253\n",
      "Iteration 36, loss = 0.18421872\n",
      "Iteration 37, loss = 0.18250800\n",
      "Iteration 38, loss = 0.18094624\n",
      "Iteration 39, loss = 0.17924882\n",
      "Iteration 40, loss = 0.17779238\n",
      "Iteration 41, loss = 0.17624888\n",
      "Iteration 42, loss = 0.17489132\n",
      "Iteration 43, loss = 0.17353590\n",
      "Iteration 44, loss = 0.17223236\n",
      "Iteration 45, loss = 0.17091820\n",
      "Iteration 46, loss = 0.16975803\n",
      "Iteration 47, loss = 0.16855189\n",
      "Iteration 48, loss = 0.16737578\n",
      "Iteration 49, loss = 0.16627369\n",
      "Iteration 50, loss = 0.16511699\n",
      "Iteration 51, loss = 0.16406887\n",
      "Iteration 52, loss = 0.16311920\n",
      "Iteration 53, loss = 0.16203911\n",
      "Iteration 54, loss = 0.16110000\n",
      "Iteration 55, loss = 0.16015656\n",
      "Iteration 56, loss = 0.15921911\n",
      "Iteration 57, loss = 0.15828444\n",
      "Iteration 58, loss = 0.15745052\n",
      "Iteration 59, loss = 0.15664163\n",
      "Iteration 60, loss = 0.15572513\n",
      "Iteration 61, loss = 0.15493306\n",
      "Iteration 62, loss = 0.15421246\n",
      "Iteration 63, loss = 0.15335368\n",
      "Iteration 64, loss = 0.15255542\n",
      "Iteration 65, loss = 0.15178502\n",
      "Iteration 66, loss = 0.15097475\n",
      "Iteration 67, loss = 0.15019906\n",
      "Iteration 68, loss = 0.14942944\n",
      "Iteration 69, loss = 0.14864352\n",
      "Iteration 70, loss = 0.14794252\n",
      "Iteration 71, loss = 0.14720045\n",
      "Iteration 72, loss = 0.14647360\n",
      "Iteration 73, loss = 0.14580466\n",
      "Iteration 74, loss = 0.14513362\n",
      "Iteration 75, loss = 0.14444746\n",
      "Iteration 76, loss = 0.14372023\n",
      "Iteration 77, loss = 0.14306291\n",
      "Iteration 78, loss = 0.14233063\n",
      "Iteration 79, loss = 0.14165815\n",
      "Iteration 80, loss = 0.14105124\n",
      "Iteration 81, loss = 0.14041713\n",
      "Iteration 82, loss = 0.13979843\n",
      "Iteration 83, loss = 0.13919873\n",
      "Iteration 84, loss = 0.13857500\n",
      "Iteration 85, loss = 0.13797809\n",
      "Iteration 86, loss = 0.13735219\n",
      "Iteration 87, loss = 0.13684401\n",
      "Iteration 88, loss = 0.13619843\n",
      "Iteration 89, loss = 0.13564117\n",
      "Iteration 90, loss = 0.13506607\n",
      "Iteration 91, loss = 0.13456307\n",
      "Iteration 92, loss = 0.13398406\n",
      "Iteration 93, loss = 0.13347340\n",
      "Iteration 94, loss = 0.13295547\n",
      "Iteration 95, loss = 0.13249429\n",
      "Iteration 96, loss = 0.13194851\n",
      "Iteration 97, loss = 0.13147327\n",
      "Iteration 98, loss = 0.13093721\n",
      "Iteration 99, loss = 0.13047811\n",
      "Iteration 100, loss = 0.13003082\n",
      "Iteration 101, loss = 0.12952250\n",
      "Iteration 102, loss = 0.12907370\n",
      "Iteration 103, loss = 0.12857226\n",
      "Iteration 104, loss = 0.12815061\n",
      "Iteration 105, loss = 0.12767135\n",
      "Iteration 106, loss = 0.12720492\n",
      "Iteration 107, loss = 0.12675814\n",
      "Iteration 108, loss = 0.12632573\n",
      "Iteration 109, loss = 0.12586343\n",
      "Iteration 110, loss = 0.12551798\n",
      "Iteration 111, loss = 0.12508909\n",
      "Iteration 112, loss = 0.12460639\n",
      "Iteration 113, loss = 0.12417879\n",
      "Iteration 114, loss = 0.12376196\n",
      "Iteration 115, loss = 0.12340197\n",
      "Iteration 116, loss = 0.12298339\n",
      "Iteration 117, loss = 0.12268375\n",
      "Iteration 118, loss = 0.12223479\n",
      "Iteration 119, loss = 0.12188340\n",
      "Iteration 120, loss = 0.12150922\n",
      "Iteration 121, loss = 0.12117268\n",
      "Iteration 122, loss = 0.12076943\n",
      "Iteration 123, loss = 0.12046413\n",
      "Iteration 124, loss = 0.12009408\n",
      "Iteration 125, loss = 0.11971296\n",
      "Iteration 126, loss = 0.11939437\n",
      "Iteration 127, loss = 0.11909834\n",
      "Iteration 128, loss = 0.11878429\n",
      "Iteration 129, loss = 0.11845885\n",
      "Iteration 130, loss = 0.11815118\n",
      "Iteration 131, loss = 0.11783469\n",
      "Iteration 132, loss = 0.11753873\n",
      "Iteration 133, loss = 0.11724542\n",
      "Iteration 134, loss = 0.11694596\n",
      "Iteration 135, loss = 0.11669474\n",
      "Iteration 136, loss = 0.11640982\n",
      "Iteration 137, loss = 0.11614926\n",
      "Iteration 138, loss = 0.11588634\n",
      "Iteration 139, loss = 0.11558160\n",
      "Iteration 140, loss = 0.11532303\n",
      "Iteration 141, loss = 0.11507045\n",
      "Iteration 142, loss = 0.11487146\n",
      "Iteration 143, loss = 0.11458035\n",
      "Iteration 144, loss = 0.11431958\n",
      "Iteration 145, loss = 0.11407880\n",
      "Iteration 146, loss = 0.11384294\n",
      "Iteration 147, loss = 0.11363290\n",
      "Iteration 148, loss = 0.11338114\n",
      "Iteration 149, loss = 0.11313524\n",
      "Iteration 150, loss = 0.11291840\n",
      "Iteration 151, loss = 0.11271131\n",
      "Iteration 152, loss = 0.11248266\n",
      "Iteration 153, loss = 0.11223788\n",
      "Iteration 154, loss = 0.11204887\n",
      "Iteration 155, loss = 0.11185703\n",
      "Iteration 156, loss = 0.11160376\n",
      "Iteration 157, loss = 0.11143322\n",
      "Iteration 158, loss = 0.11122661\n",
      "Iteration 159, loss = 0.11101935\n",
      "Iteration 160, loss = 0.11080946\n",
      "Iteration 161, loss = 0.11063375\n",
      "Iteration 162, loss = 0.11044846\n",
      "Iteration 163, loss = 0.11023322\n",
      "Iteration 164, loss = 0.11009204\n",
      "Iteration 165, loss = 0.10990437\n",
      "Iteration 166, loss = 0.10968575\n",
      "Iteration 167, loss = 0.10951259\n",
      "Iteration 168, loss = 0.10933289\n",
      "Iteration 169, loss = 0.10916782\n",
      "Iteration 170, loss = 0.10895739\n",
      "Iteration 171, loss = 0.10877840\n",
      "Iteration 172, loss = 0.10856658\n",
      "Iteration 173, loss = 0.10843161\n",
      "Iteration 174, loss = 0.10822192\n",
      "Iteration 175, loss = 0.10801917\n",
      "Iteration 176, loss = 0.10784681\n",
      "Iteration 177, loss = 0.10773257\n",
      "Iteration 178, loss = 0.10767467\n",
      "Iteration 179, loss = 0.10734620\n",
      "Iteration 180, loss = 0.10716921\n",
      "Iteration 181, loss = 0.10703016\n",
      "Iteration 182, loss = 0.10682017\n",
      "Iteration 183, loss = 0.10669811\n",
      "Iteration 184, loss = 0.10650845\n",
      "Iteration 185, loss = 0.10636074\n",
      "Iteration 186, loss = 0.10618175\n",
      "Iteration 187, loss = 0.10608280\n",
      "Iteration 188, loss = 0.10589470\n",
      "Iteration 189, loss = 0.10578199\n",
      "Iteration 190, loss = 0.10559356\n",
      "Iteration 191, loss = 0.10546784\n",
      "Iteration 192, loss = 0.10532895\n",
      "Iteration 193, loss = 0.10515701\n",
      "Iteration 194, loss = 0.10503474\n",
      "Iteration 195, loss = 0.10488027\n",
      "Iteration 196, loss = 0.10473139\n",
      "Iteration 197, loss = 0.10456945\n",
      "Iteration 198, loss = 0.10443897\n",
      "Iteration 199, loss = 0.10426763\n",
      "Iteration 200, loss = 0.10411625\n",
      "Iteration 201, loss = 0.10396173\n",
      "Iteration 202, loss = 0.10382936\n",
      "Iteration 203, loss = 0.10369913\n",
      "Iteration 204, loss = 0.10359787\n",
      "Iteration 205, loss = 0.10341235\n",
      "Iteration 206, loss = 0.10326455\n",
      "Iteration 207, loss = 0.10316605\n",
      "Iteration 208, loss = 0.10302569\n",
      "Iteration 209, loss = 0.10288978\n",
      "Iteration 210, loss = 0.10272572\n",
      "Iteration 211, loss = 0.10262569\n",
      "Iteration 212, loss = 0.10246338\n",
      "Iteration 213, loss = 0.10237786\n",
      "Iteration 214, loss = 0.10221606\n",
      "Iteration 215, loss = 0.10211677\n",
      "Iteration 216, loss = 0.10196533\n",
      "Iteration 217, loss = 0.10184608\n",
      "Iteration 218, loss = 0.10172722\n",
      "Iteration 219, loss = 0.10159235\n",
      "Iteration 220, loss = 0.10148676\n",
      "Iteration 221, loss = 0.10134848\n",
      "Iteration 222, loss = 0.10127923\n",
      "Iteration 223, loss = 0.10113555\n",
      "Iteration 224, loss = 0.10102762\n",
      "Iteration 225, loss = 0.10090356\n",
      "Iteration 226, loss = 0.10078490\n",
      "Iteration 227, loss = 0.10068025\n",
      "Iteration 228, loss = 0.10057831\n",
      "Iteration 229, loss = 0.10047232\n",
      "Iteration 230, loss = 0.10035777\n",
      "Iteration 231, loss = 0.10025903\n",
      "Iteration 232, loss = 0.10015338\n",
      "Iteration 233, loss = 0.10006074\n",
      "Iteration 234, loss = 0.09998481\n",
      "Iteration 235, loss = 0.09993026\n",
      "Iteration 236, loss = 0.09975876\n",
      "Iteration 237, loss = 0.09969992\n",
      "Iteration 238, loss = 0.09956892\n",
      "Iteration 239, loss = 0.09947037\n",
      "Iteration 240, loss = 0.09939082\n",
      "Iteration 241, loss = 0.09933561\n",
      "Iteration 242, loss = 0.09920944\n",
      "Iteration 243, loss = 0.09912080\n",
      "Iteration 244, loss = 0.09903735\n",
      "Iteration 245, loss = 0.09897073\n",
      "Iteration 246, loss = 0.09887242\n",
      "Iteration 247, loss = 0.09878596\n",
      "Iteration 248, loss = 0.09868898\n",
      "Iteration 249, loss = 0.09861265\n",
      "Iteration 250, loss = 0.09857805\n",
      "Iteration 251, loss = 0.09845669\n",
      "Iteration 252, loss = 0.09839001\n",
      "Iteration 253, loss = 0.09833876\n",
      "Iteration 254, loss = 0.09821864\n",
      "Iteration 255, loss = 0.09813804\n",
      "Iteration 256, loss = 0.09806423\n",
      "Iteration 257, loss = 0.09797534\n",
      "Iteration 258, loss = 0.09791607\n",
      "Iteration 259, loss = 0.09784699\n",
      "Iteration 260, loss = 0.09777434\n",
      "Iteration 261, loss = 0.09777675\n",
      "Iteration 262, loss = 0.09768678\n",
      "Iteration 263, loss = 0.09754620\n",
      "Iteration 264, loss = 0.09748111\n",
      "Iteration 265, loss = 0.09741073\n",
      "Iteration 266, loss = 0.09741783\n",
      "Iteration 267, loss = 0.09726529\n",
      "Iteration 268, loss = 0.09719010\n",
      "Iteration 269, loss = 0.09715012\n",
      "Iteration 270, loss = 0.09709909\n",
      "Iteration 271, loss = 0.09700133\n",
      "Iteration 272, loss = 0.09693612\n",
      "Iteration 273, loss = 0.09685892\n",
      "Iteration 274, loss = 0.09680239\n",
      "Iteration 275, loss = 0.09671957\n",
      "Iteration 276, loss = 0.09666961\n",
      "Iteration 277, loss = 0.09663097\n",
      "Iteration 278, loss = 0.09653477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 279, loss = 0.09646499\n",
      "Iteration 280, loss = 0.09641771\n",
      "Iteration 281, loss = 0.09640213\n",
      "Iteration 282, loss = 0.09638213\n",
      "Iteration 283, loss = 0.09636765\n",
      "Iteration 284, loss = 0.09635655\n",
      "Iteration 285, loss = 0.09634483\n",
      "Iteration 286, loss = 0.09632866\n",
      "Iteration 287, loss = 0.09631729\n",
      "Iteration 288, loss = 0.09630331\n",
      "Iteration 289, loss = 0.09630091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 290, loss = 0.09627389\n",
      "Iteration 291, loss = 0.09626853\n",
      "Iteration 292, loss = 0.09626702\n",
      "Iteration 293, loss = 0.09626156\n",
      "Iteration 294, loss = 0.09626020\n",
      "Iteration 295, loss = 0.09625781\n",
      "Iteration 296, loss = 0.09625426\n",
      "Iteration 297, loss = 0.09625208\n",
      "Iteration 298, loss = 0.09624947\n",
      "Iteration 299, loss = 0.09624742\n",
      "Iteration 300, loss = 0.09624386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 301, loss = 0.09624078\n",
      "Iteration 302, loss = 0.09623971\n",
      "Iteration 303, loss = 0.09623887\n",
      "Iteration 304, loss = 0.09623820\n",
      "Iteration 305, loss = 0.09623794\n",
      "Iteration 306, loss = 0.09623715\n",
      "Iteration 307, loss = 0.09623680\n",
      "Iteration 308, loss = 0.09623639\n",
      "Iteration 309, loss = 0.09623572\n",
      "Iteration 310, loss = 0.09623529\n",
      "Iteration 311, loss = 0.09623472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 312, loss = 0.09623396\n",
      "Iteration 313, loss = 0.09623383\n",
      "Iteration 314, loss = 0.09623359\n",
      "Iteration 315, loss = 0.09623356\n",
      "Iteration 316, loss = 0.09623336\n",
      "Iteration 317, loss = 0.09623339\n",
      "Iteration 318, loss = 0.09623316\n",
      "Iteration 319, loss = 0.09623308\n",
      "Iteration 320, loss = 0.09623299\n",
      "Iteration 321, loss = 0.09623287\n",
      "Iteration 322, loss = 0.09623283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 323, loss = 0.09623264\n",
      "Iteration 324, loss = 0.09623262\n",
      "Iteration 325, loss = 0.09623258\n",
      "Iteration 326, loss = 0.09623255\n",
      "Iteration 327, loss = 0.09623253\n",
      "Iteration 328, loss = 0.09623251\n",
      "Iteration 329, loss = 0.09623250\n",
      "Iteration 330, loss = 0.09623247\n",
      "Iteration 331, loss = 0.09623246\n",
      "Iteration 332, loss = 0.09623243\n",
      "Iteration 333, loss = 0.09623241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 334, loss = 0.09623238\n",
      "Iteration 335, loss = 0.09623237\n",
      "Iteration 336, loss = 0.09623237\n",
      "Iteration 337, loss = 0.09623236\n",
      "Iteration 338, loss = 0.09623236\n",
      "Iteration 339, loss = 0.09623236\n",
      "Iteration 340, loss = 0.09623235\n",
      "Iteration 341, loss = 0.09623234\n",
      "Iteration 342, loss = 0.09623234\n",
      "Iteration 343, loss = 0.09623234\n",
      "Iteration 344, loss = 0.09623233\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56516300\n",
      "Iteration 2, loss = 0.52957022\n",
      "Iteration 3, loss = 0.48902782\n",
      "Iteration 4, loss = 0.45098046\n",
      "Iteration 5, loss = 0.41823323\n",
      "Iteration 6, loss = 0.39044472\n",
      "Iteration 7, loss = 0.36678122\n",
      "Iteration 8, loss = 0.34699691\n",
      "Iteration 9, loss = 0.32972117\n",
      "Iteration 10, loss = 0.31478561\n",
      "Iteration 11, loss = 0.30161406\n",
      "Iteration 12, loss = 0.28992682\n",
      "Iteration 13, loss = 0.28017483\n",
      "Iteration 14, loss = 0.27095668\n",
      "Iteration 15, loss = 0.26273376\n",
      "Iteration 16, loss = 0.25561992\n",
      "Iteration 17, loss = 0.24913708\n",
      "Iteration 18, loss = 0.24293634\n",
      "Iteration 19, loss = 0.23754556\n",
      "Iteration 20, loss = 0.23257341\n",
      "Iteration 21, loss = 0.22802014\n",
      "Iteration 22, loss = 0.22375776\n",
      "Iteration 23, loss = 0.21986514\n",
      "Iteration 24, loss = 0.21618056\n",
      "Iteration 25, loss = 0.21273363\n",
      "Iteration 26, loss = 0.20957516\n",
      "Iteration 27, loss = 0.20670320\n",
      "Iteration 28, loss = 0.20388132\n",
      "Iteration 29, loss = 0.20122994\n",
      "Iteration 30, loss = 0.19874193\n",
      "Iteration 31, loss = 0.19650214\n",
      "Iteration 32, loss = 0.19433193\n",
      "Iteration 33, loss = 0.19224987\n",
      "Iteration 34, loss = 0.19027986\n",
      "Iteration 35, loss = 0.18836820\n",
      "Iteration 36, loss = 0.18651584\n",
      "Iteration 37, loss = 0.18475414\n",
      "Iteration 38, loss = 0.18312341\n",
      "Iteration 39, loss = 0.18141602\n",
      "Iteration 40, loss = 0.17990885\n",
      "Iteration 41, loss = 0.17833007\n",
      "Iteration 42, loss = 0.17695680\n",
      "Iteration 43, loss = 0.17555605\n",
      "Iteration 44, loss = 0.17423424\n",
      "Iteration 45, loss = 0.17288916\n",
      "Iteration 46, loss = 0.17168849\n",
      "Iteration 47, loss = 0.17048382\n",
      "Iteration 48, loss = 0.16928645\n",
      "Iteration 49, loss = 0.16817537\n",
      "Iteration 50, loss = 0.16698748\n",
      "Iteration 51, loss = 0.16593793\n",
      "Iteration 52, loss = 0.16495063\n",
      "Iteration 53, loss = 0.16387392\n",
      "Iteration 54, loss = 0.16294085\n",
      "Iteration 55, loss = 0.16194521\n",
      "Iteration 56, loss = 0.16100112\n",
      "Iteration 57, loss = 0.16004642\n",
      "Iteration 58, loss = 0.15920175\n",
      "Iteration 59, loss = 0.15833444\n",
      "Iteration 60, loss = 0.15734335\n",
      "Iteration 61, loss = 0.15649759\n",
      "Iteration 62, loss = 0.15571079\n",
      "Iteration 63, loss = 0.15479205\n",
      "Iteration 64, loss = 0.15394875\n",
      "Iteration 65, loss = 0.15314145\n",
      "Iteration 66, loss = 0.15233524\n",
      "Iteration 67, loss = 0.15152583\n",
      "Iteration 68, loss = 0.15075612\n",
      "Iteration 69, loss = 0.14989102\n",
      "Iteration 70, loss = 0.14912346\n",
      "Iteration 71, loss = 0.14828059\n",
      "Iteration 72, loss = 0.14745710\n",
      "Iteration 73, loss = 0.14670396\n",
      "Iteration 74, loss = 0.14596173\n",
      "Iteration 75, loss = 0.14519451\n",
      "Iteration 76, loss = 0.14444155\n",
      "Iteration 77, loss = 0.14369238\n",
      "Iteration 78, loss = 0.14297040\n",
      "Iteration 79, loss = 0.14223682\n",
      "Iteration 80, loss = 0.14154871\n",
      "Iteration 81, loss = 0.14084627\n",
      "Iteration 82, loss = 0.14019288\n",
      "Iteration 83, loss = 0.13950714\n",
      "Iteration 84, loss = 0.13883093\n",
      "Iteration 85, loss = 0.13810705\n",
      "Iteration 86, loss = 0.13736908\n",
      "Iteration 87, loss = 0.13679351\n",
      "Iteration 88, loss = 0.13603602\n",
      "Iteration 89, loss = 0.13536231\n",
      "Iteration 90, loss = 0.13475656\n",
      "Iteration 91, loss = 0.13415548\n",
      "Iteration 92, loss = 0.13349010\n",
      "Iteration 93, loss = 0.13291133\n",
      "Iteration 94, loss = 0.13232214\n",
      "Iteration 95, loss = 0.13180683\n",
      "Iteration 96, loss = 0.13119362\n",
      "Iteration 97, loss = 0.13065410\n",
      "Iteration 98, loss = 0.13006084\n",
      "Iteration 99, loss = 0.12957442\n",
      "Iteration 100, loss = 0.12904640\n",
      "Iteration 101, loss = 0.12847750\n",
      "Iteration 102, loss = 0.12796982\n",
      "Iteration 103, loss = 0.12739693\n",
      "Iteration 104, loss = 0.12695096\n",
      "Iteration 105, loss = 0.12643406\n",
      "Iteration 106, loss = 0.12596619\n",
      "Iteration 107, loss = 0.12549343\n",
      "Iteration 108, loss = 0.12503089\n",
      "Iteration 109, loss = 0.12455558\n",
      "Iteration 110, loss = 0.12419185\n",
      "Iteration 111, loss = 0.12371060\n",
      "Iteration 112, loss = 0.12323207\n",
      "Iteration 113, loss = 0.12279262\n",
      "Iteration 114, loss = 0.12235951\n",
      "Iteration 115, loss = 0.12199074\n",
      "Iteration 116, loss = 0.12157630\n",
      "Iteration 117, loss = 0.12126849\n",
      "Iteration 118, loss = 0.12079372\n",
      "Iteration 119, loss = 0.12045134\n",
      "Iteration 120, loss = 0.12007459\n",
      "Iteration 121, loss = 0.11973442\n",
      "Iteration 122, loss = 0.11934502\n",
      "Iteration 123, loss = 0.11902753\n",
      "Iteration 124, loss = 0.11865608\n",
      "Iteration 125, loss = 0.11829071\n",
      "Iteration 126, loss = 0.11797689\n",
      "Iteration 127, loss = 0.11767340\n",
      "Iteration 128, loss = 0.11737649\n",
      "Iteration 129, loss = 0.11706233\n",
      "Iteration 130, loss = 0.11675955\n",
      "Iteration 131, loss = 0.11646123\n",
      "Iteration 132, loss = 0.11618851\n",
      "Iteration 133, loss = 0.11586967\n",
      "Iteration 134, loss = 0.11558845\n",
      "Iteration 135, loss = 0.11533090\n",
      "Iteration 136, loss = 0.11505110\n",
      "Iteration 137, loss = 0.11481156\n",
      "Iteration 138, loss = 0.11455137\n",
      "Iteration 139, loss = 0.11422845\n",
      "Iteration 140, loss = 0.11398140\n",
      "Iteration 141, loss = 0.11373325\n",
      "Iteration 142, loss = 0.11353680\n",
      "Iteration 143, loss = 0.11325078\n",
      "Iteration 144, loss = 0.11299495\n",
      "Iteration 145, loss = 0.11276062\n",
      "Iteration 146, loss = 0.11253452\n",
      "Iteration 147, loss = 0.11232468\n",
      "Iteration 148, loss = 0.11208834\n",
      "Iteration 149, loss = 0.11184745\n",
      "Iteration 150, loss = 0.11164191\n",
      "Iteration 151, loss = 0.11144522\n",
      "Iteration 152, loss = 0.11122791\n",
      "Iteration 153, loss = 0.11099003\n",
      "Iteration 154, loss = 0.11080914\n",
      "Iteration 155, loss = 0.11062844\n",
      "Iteration 156, loss = 0.11036894\n",
      "Iteration 157, loss = 0.11019182\n",
      "Iteration 158, loss = 0.10997605\n",
      "Iteration 159, loss = 0.10973957\n",
      "Iteration 160, loss = 0.10949191\n",
      "Iteration 161, loss = 0.10928067\n",
      "Iteration 162, loss = 0.10907237\n",
      "Iteration 163, loss = 0.10883689\n",
      "Iteration 164, loss = 0.10865614\n",
      "Iteration 165, loss = 0.10845885\n",
      "Iteration 166, loss = 0.10823073\n",
      "Iteration 167, loss = 0.10803025\n",
      "Iteration 168, loss = 0.10785092\n",
      "Iteration 169, loss = 0.10767937\n",
      "Iteration 170, loss = 0.10747993\n",
      "Iteration 171, loss = 0.10729985\n",
      "Iteration 172, loss = 0.10707742\n",
      "Iteration 173, loss = 0.10693705\n",
      "Iteration 174, loss = 0.10672794\n",
      "Iteration 175, loss = 0.10653433\n",
      "Iteration 176, loss = 0.10636613\n",
      "Iteration 177, loss = 0.10627090\n",
      "Iteration 178, loss = 0.10622155\n",
      "Iteration 179, loss = 0.10588015\n",
      "Iteration 180, loss = 0.10571697\n",
      "Iteration 181, loss = 0.10559030\n",
      "Iteration 182, loss = 0.10540349\n",
      "Iteration 183, loss = 0.10529411\n",
      "Iteration 184, loss = 0.10510683\n",
      "Iteration 185, loss = 0.10495021\n",
      "Iteration 186, loss = 0.10480252\n",
      "Iteration 187, loss = 0.10469582\n",
      "Iteration 188, loss = 0.10451996\n",
      "Iteration 189, loss = 0.10443443\n",
      "Iteration 190, loss = 0.10423681\n",
      "Iteration 191, loss = 0.10409898\n",
      "Iteration 192, loss = 0.10398363\n",
      "Iteration 193, loss = 0.10379173\n",
      "Iteration 194, loss = 0.10367705\n",
      "Iteration 195, loss = 0.10354051\n",
      "Iteration 196, loss = 0.10341885\n",
      "Iteration 197, loss = 0.10327056\n",
      "Iteration 198, loss = 0.10315285\n",
      "Iteration 199, loss = 0.10301128\n",
      "Iteration 200, loss = 0.10287583\n",
      "Iteration 201, loss = 0.10273547\n",
      "Iteration 202, loss = 0.10260845\n",
      "Iteration 203, loss = 0.10250581\n",
      "Iteration 204, loss = 0.10240367\n",
      "Iteration 205, loss = 0.10225651\n",
      "Iteration 206, loss = 0.10212318\n",
      "Iteration 207, loss = 0.10202832\n",
      "Iteration 208, loss = 0.10192072\n",
      "Iteration 209, loss = 0.10178171\n",
      "Iteration 210, loss = 0.10164962\n",
      "Iteration 211, loss = 0.10156073\n",
      "Iteration 212, loss = 0.10142203\n",
      "Iteration 213, loss = 0.10134362\n",
      "Iteration 214, loss = 0.10120824\n",
      "Iteration 215, loss = 0.10112901\n",
      "Iteration 216, loss = 0.10098457\n",
      "Iteration 217, loss = 0.10088733\n",
      "Iteration 218, loss = 0.10077337\n",
      "Iteration 219, loss = 0.10065973\n",
      "Iteration 220, loss = 0.10056189\n",
      "Iteration 221, loss = 0.10043163\n",
      "Iteration 222, loss = 0.10038447\n",
      "Iteration 223, loss = 0.10023665\n",
      "Iteration 224, loss = 0.10014116\n",
      "Iteration 225, loss = 0.10003160\n",
      "Iteration 226, loss = 0.09992192\n",
      "Iteration 227, loss = 0.09981761\n",
      "Iteration 228, loss = 0.09972595\n",
      "Iteration 229, loss = 0.09963379\n",
      "Iteration 230, loss = 0.09953701\n",
      "Iteration 231, loss = 0.09943849\n",
      "Iteration 232, loss = 0.09934401\n",
      "Iteration 233, loss = 0.09926876\n",
      "Iteration 234, loss = 0.09920068\n",
      "Iteration 235, loss = 0.09913767\n",
      "Iteration 236, loss = 0.09900409\n",
      "Iteration 237, loss = 0.09895610\n",
      "Iteration 238, loss = 0.09883262\n",
      "Iteration 239, loss = 0.09874442\n",
      "Iteration 240, loss = 0.09867529\n",
      "Iteration 241, loss = 0.09864680\n",
      "Iteration 242, loss = 0.09850552\n",
      "Iteration 243, loss = 0.09842188\n",
      "Iteration 244, loss = 0.09835078\n",
      "Iteration 245, loss = 0.09828300\n",
      "Iteration 246, loss = 0.09818572\n",
      "Iteration 247, loss = 0.09809563\n",
      "Iteration 248, loss = 0.09801800\n",
      "Iteration 249, loss = 0.09794027\n",
      "Iteration 250, loss = 0.09790842\n",
      "Iteration 251, loss = 0.09779484\n",
      "Iteration 252, loss = 0.09772642\n",
      "Iteration 253, loss = 0.09767526\n",
      "Iteration 254, loss = 0.09756724\n",
      "Iteration 255, loss = 0.09748908\n",
      "Iteration 256, loss = 0.09741133\n",
      "Iteration 257, loss = 0.09733435\n",
      "Iteration 258, loss = 0.09727450\n",
      "Iteration 259, loss = 0.09720423\n",
      "Iteration 260, loss = 0.09713871\n",
      "Iteration 261, loss = 0.09714564\n",
      "Iteration 262, loss = 0.09703129\n",
      "Iteration 263, loss = 0.09692302\n",
      "Iteration 264, loss = 0.09685073\n",
      "Iteration 265, loss = 0.09678627\n",
      "Iteration 266, loss = 0.09677990\n",
      "Iteration 267, loss = 0.09663251\n",
      "Iteration 268, loss = 0.09656829\n",
      "Iteration 269, loss = 0.09651846\n",
      "Iteration 270, loss = 0.09646025\n",
      "Iteration 271, loss = 0.09636792\n",
      "Iteration 272, loss = 0.09630414\n",
      "Iteration 273, loss = 0.09622311\n",
      "Iteration 274, loss = 0.09616442\n",
      "Iteration 275, loss = 0.09606713\n",
      "Iteration 276, loss = 0.09600272\n",
      "Iteration 277, loss = 0.09595867\n",
      "Iteration 278, loss = 0.09585624\n",
      "Iteration 279, loss = 0.09583408\n",
      "Iteration 280, loss = 0.09570162\n",
      "Iteration 281, loss = 0.09564382\n",
      "Iteration 282, loss = 0.09555244\n",
      "Iteration 283, loss = 0.09547764\n",
      "Iteration 284, loss = 0.09542467\n",
      "Iteration 285, loss = 0.09534419\n",
      "Iteration 286, loss = 0.09526628\n",
      "Iteration 287, loss = 0.09519569\n",
      "Iteration 288, loss = 0.09512453\n",
      "Iteration 289, loss = 0.09510918\n",
      "Iteration 290, loss = 0.09499256\n",
      "Iteration 291, loss = 0.09491984\n",
      "Iteration 292, loss = 0.09491451\n",
      "Iteration 293, loss = 0.09477724\n",
      "Iteration 294, loss = 0.09476151\n",
      "Iteration 295, loss = 0.09469227\n",
      "Iteration 296, loss = 0.09460240\n",
      "Iteration 297, loss = 0.09454127\n",
      "Iteration 298, loss = 0.09448165\n",
      "Iteration 299, loss = 0.09441631\n",
      "Iteration 300, loss = 0.09433374\n",
      "Iteration 301, loss = 0.09426701\n",
      "Iteration 302, loss = 0.09421938\n",
      "Iteration 303, loss = 0.09415579\n",
      "Iteration 304, loss = 0.09407838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56146013\n",
      "Iteration 2, loss = 0.52420401\n",
      "Iteration 3, loss = 0.48196029\n",
      "Iteration 4, loss = 0.44237589\n",
      "Iteration 5, loss = 0.40840913\n",
      "Iteration 6, loss = 0.37986444\n",
      "Iteration 7, loss = 0.35562388\n",
      "Iteration 8, loss = 0.33512772\n",
      "Iteration 9, loss = 0.31727160\n",
      "Iteration 10, loss = 0.30171771\n",
      "Iteration 11, loss = 0.28809570\n",
      "Iteration 12, loss = 0.27601021\n",
      "Iteration 13, loss = 0.26568292\n",
      "Iteration 14, loss = 0.25627262\n",
      "Iteration 15, loss = 0.24772120\n",
      "Iteration 16, loss = 0.24027288\n",
      "Iteration 17, loss = 0.23357107\n",
      "Iteration 18, loss = 0.22716012\n",
      "Iteration 19, loss = 0.22150226\n",
      "Iteration 20, loss = 0.21639896\n",
      "Iteration 21, loss = 0.21165810\n",
      "Iteration 22, loss = 0.20723245\n",
      "Iteration 23, loss = 0.20320019\n",
      "Iteration 24, loss = 0.19948037\n",
      "Iteration 25, loss = 0.19591353\n",
      "Iteration 26, loss = 0.19268751\n",
      "Iteration 27, loss = 0.18977335\n",
      "Iteration 28, loss = 0.18687542\n",
      "Iteration 29, loss = 0.18417780\n",
      "Iteration 30, loss = 0.18163663\n",
      "Iteration 31, loss = 0.17929834\n",
      "Iteration 32, loss = 0.17706697\n",
      "Iteration 33, loss = 0.17491321\n",
      "Iteration 34, loss = 0.17292113\n",
      "Iteration 35, loss = 0.17095010\n",
      "Iteration 36, loss = 0.16909075\n",
      "Iteration 37, loss = 0.16730101\n",
      "Iteration 38, loss = 0.16564836\n",
      "Iteration 39, loss = 0.16392496\n",
      "Iteration 40, loss = 0.16239952\n",
      "Iteration 41, loss = 0.16081442\n",
      "Iteration 42, loss = 0.15945993\n",
      "Iteration 43, loss = 0.15804297\n",
      "Iteration 44, loss = 0.15670704\n",
      "Iteration 45, loss = 0.15537718\n",
      "Iteration 46, loss = 0.15415465\n",
      "Iteration 47, loss = 0.15293346\n",
      "Iteration 48, loss = 0.15173892\n",
      "Iteration 49, loss = 0.15059898\n",
      "Iteration 50, loss = 0.14945275\n",
      "Iteration 51, loss = 0.14839236\n",
      "Iteration 52, loss = 0.14740503\n",
      "Iteration 53, loss = 0.14633313\n",
      "Iteration 54, loss = 0.14537955\n",
      "Iteration 55, loss = 0.14439467\n",
      "Iteration 56, loss = 0.14346667\n",
      "Iteration 57, loss = 0.14252794\n",
      "Iteration 58, loss = 0.14164010\n",
      "Iteration 59, loss = 0.14079304\n",
      "Iteration 60, loss = 0.13986294\n",
      "Iteration 61, loss = 0.13901454\n",
      "Iteration 62, loss = 0.13825486\n",
      "Iteration 63, loss = 0.13735490\n",
      "Iteration 64, loss = 0.13649281\n",
      "Iteration 65, loss = 0.13572031\n",
      "Iteration 66, loss = 0.13490806\n",
      "Iteration 67, loss = 0.13413125\n",
      "Iteration 68, loss = 0.13334908\n",
      "Iteration 69, loss = 0.13255317\n",
      "Iteration 70, loss = 0.13181600\n",
      "Iteration 71, loss = 0.13104206\n",
      "Iteration 72, loss = 0.13028425\n",
      "Iteration 73, loss = 0.12956584\n",
      "Iteration 74, loss = 0.12887498\n",
      "Iteration 75, loss = 0.12812533\n",
      "Iteration 76, loss = 0.12744269\n",
      "Iteration 77, loss = 0.12672390\n",
      "Iteration 78, loss = 0.12604384\n",
      "Iteration 79, loss = 0.12538821\n",
      "Iteration 80, loss = 0.12474954\n",
      "Iteration 81, loss = 0.12416192\n",
      "Iteration 82, loss = 0.12351708\n",
      "Iteration 83, loss = 0.12286562\n",
      "Iteration 84, loss = 0.12219239\n",
      "Iteration 85, loss = 0.12155217\n",
      "Iteration 86, loss = 0.12091338\n",
      "Iteration 87, loss = 0.12036043\n",
      "Iteration 88, loss = 0.11963905\n",
      "Iteration 89, loss = 0.11896336\n",
      "Iteration 90, loss = 0.11839309\n",
      "Iteration 91, loss = 0.11777314\n",
      "Iteration 92, loss = 0.11710934\n",
      "Iteration 93, loss = 0.11652443\n",
      "Iteration 94, loss = 0.11593772\n",
      "Iteration 95, loss = 0.11541637\n",
      "Iteration 96, loss = 0.11482041\n",
      "Iteration 97, loss = 0.11426263\n",
      "Iteration 98, loss = 0.11368490\n",
      "Iteration 99, loss = 0.11319363\n",
      "Iteration 100, loss = 0.11266270\n",
      "Iteration 101, loss = 0.11212700\n",
      "Iteration 102, loss = 0.11162077\n",
      "Iteration 103, loss = 0.11105520\n",
      "Iteration 104, loss = 0.11057175\n",
      "Iteration 105, loss = 0.11008914\n",
      "Iteration 106, loss = 0.10960827\n",
      "Iteration 107, loss = 0.10911696\n",
      "Iteration 108, loss = 0.10864587\n",
      "Iteration 109, loss = 0.10820114\n",
      "Iteration 110, loss = 0.10777709\n",
      "Iteration 111, loss = 0.10732716\n",
      "Iteration 112, loss = 0.10680649\n",
      "Iteration 113, loss = 0.10638577\n",
      "Iteration 114, loss = 0.10590005\n",
      "Iteration 115, loss = 0.10549123\n",
      "Iteration 116, loss = 0.10507152\n",
      "Iteration 117, loss = 0.10471356\n",
      "Iteration 118, loss = 0.10422938\n",
      "Iteration 119, loss = 0.10385378\n",
      "Iteration 120, loss = 0.10347964\n",
      "Iteration 121, loss = 0.10311530\n",
      "Iteration 122, loss = 0.10270238\n",
      "Iteration 123, loss = 0.10236431\n",
      "Iteration 124, loss = 0.10198407\n",
      "Iteration 125, loss = 0.10160853\n",
      "Iteration 126, loss = 0.10128305\n",
      "Iteration 127, loss = 0.10093797\n",
      "Iteration 128, loss = 0.10064180\n",
      "Iteration 129, loss = 0.10027490\n",
      "Iteration 130, loss = 0.10001001\n",
      "Iteration 131, loss = 0.09966900\n",
      "Iteration 132, loss = 0.09936399\n",
      "Iteration 133, loss = 0.09904909\n",
      "Iteration 134, loss = 0.09878310\n",
      "Iteration 135, loss = 0.09848740\n",
      "Iteration 136, loss = 0.09822759\n",
      "Iteration 137, loss = 0.09792471\n",
      "Iteration 138, loss = 0.09764743\n",
      "Iteration 139, loss = 0.09729474\n",
      "Iteration 140, loss = 0.09702078\n",
      "Iteration 141, loss = 0.09675400\n",
      "Iteration 142, loss = 0.09650857\n",
      "Iteration 143, loss = 0.09622044\n",
      "Iteration 144, loss = 0.09596048\n",
      "Iteration 145, loss = 0.09567946\n",
      "Iteration 146, loss = 0.09543344\n",
      "Iteration 147, loss = 0.09519087\n",
      "Iteration 148, loss = 0.09494646\n",
      "Iteration 149, loss = 0.09467181\n",
      "Iteration 150, loss = 0.09444603\n",
      "Iteration 151, loss = 0.09420398\n",
      "Iteration 152, loss = 0.09397331\n",
      "Iteration 153, loss = 0.09371352\n",
      "Iteration 154, loss = 0.09349162\n",
      "Iteration 155, loss = 0.09326300\n",
      "Iteration 156, loss = 0.09300913\n",
      "Iteration 157, loss = 0.09282283\n",
      "Iteration 158, loss = 0.09256280\n",
      "Iteration 159, loss = 0.09232391\n",
      "Iteration 160, loss = 0.09209628\n",
      "Iteration 161, loss = 0.09188369\n",
      "Iteration 162, loss = 0.09164774\n",
      "Iteration 163, loss = 0.09141712\n",
      "Iteration 164, loss = 0.09120647\n",
      "Iteration 165, loss = 0.09099189\n",
      "Iteration 166, loss = 0.09076386\n",
      "Iteration 167, loss = 0.09056708\n",
      "Iteration 168, loss = 0.09035821\n",
      "Iteration 169, loss = 0.09016625\n",
      "Iteration 170, loss = 0.08997629\n",
      "Iteration 171, loss = 0.08975738\n",
      "Iteration 172, loss = 0.08952425\n",
      "Iteration 173, loss = 0.08935644\n",
      "Iteration 174, loss = 0.08913487\n",
      "Iteration 175, loss = 0.08894273\n",
      "Iteration 176, loss = 0.08875601\n",
      "Iteration 177, loss = 0.08859809\n",
      "Iteration 178, loss = 0.08847381\n",
      "Iteration 179, loss = 0.08820879\n",
      "Iteration 180, loss = 0.08801573\n",
      "Iteration 181, loss = 0.08787713\n",
      "Iteration 182, loss = 0.08765354\n",
      "Iteration 183, loss = 0.08751938\n",
      "Iteration 184, loss = 0.08732219\n",
      "Iteration 185, loss = 0.08714288\n",
      "Iteration 186, loss = 0.08697040\n",
      "Iteration 187, loss = 0.08687345\n",
      "Iteration 188, loss = 0.08666397\n",
      "Iteration 189, loss = 0.08654768\n",
      "Iteration 190, loss = 0.08632356\n",
      "Iteration 191, loss = 0.08615988\n",
      "Iteration 192, loss = 0.08604249\n",
      "Iteration 193, loss = 0.08583601\n",
      "Iteration 194, loss = 0.08571153\n",
      "Iteration 195, loss = 0.08554481\n",
      "Iteration 196, loss = 0.08539434\n",
      "Iteration 197, loss = 0.08524335\n",
      "Iteration 198, loss = 0.08509634\n",
      "Iteration 199, loss = 0.08494814\n",
      "Iteration 200, loss = 0.08480788\n",
      "Iteration 201, loss = 0.08465017\n",
      "Iteration 202, loss = 0.08449595\n",
      "Iteration 203, loss = 0.08436791\n",
      "Iteration 204, loss = 0.08424023\n",
      "Iteration 205, loss = 0.08406693\n",
      "Iteration 206, loss = 0.08393369\n",
      "Iteration 207, loss = 0.08381315\n",
      "Iteration 208, loss = 0.08368113\n",
      "Iteration 209, loss = 0.08354670\n",
      "Iteration 210, loss = 0.08339393\n",
      "Iteration 211, loss = 0.08327268\n",
      "Iteration 212, loss = 0.08312749\n",
      "Iteration 213, loss = 0.08304165\n",
      "Iteration 214, loss = 0.08289205\n",
      "Iteration 215, loss = 0.08277829\n",
      "Iteration 216, loss = 0.08263361\n",
      "Iteration 217, loss = 0.08252465\n",
      "Iteration 218, loss = 0.08239428\n",
      "Iteration 219, loss = 0.08226768\n",
      "Iteration 220, loss = 0.08215769\n",
      "Iteration 221, loss = 0.08200966\n",
      "Iteration 222, loss = 0.08196500\n",
      "Iteration 223, loss = 0.08179148\n",
      "Iteration 224, loss = 0.08168716\n",
      "Iteration 225, loss = 0.08156075\n",
      "Iteration 226, loss = 0.08145524\n",
      "Iteration 227, loss = 0.08134907\n",
      "Iteration 228, loss = 0.08125216\n",
      "Iteration 229, loss = 0.08116017\n",
      "Iteration 230, loss = 0.08103983\n",
      "Iteration 231, loss = 0.08095228\n",
      "Iteration 232, loss = 0.08083404\n",
      "Iteration 233, loss = 0.08074420\n",
      "Iteration 234, loss = 0.08068003\n",
      "Iteration 235, loss = 0.08058146\n",
      "Iteration 236, loss = 0.08046006\n",
      "Iteration 237, loss = 0.08039122\n",
      "Iteration 238, loss = 0.08027715\n",
      "Iteration 239, loss = 0.08019548\n",
      "Iteration 240, loss = 0.08009881\n",
      "Iteration 241, loss = 0.08007416\n",
      "Iteration 242, loss = 0.07991928\n",
      "Iteration 243, loss = 0.07983459\n",
      "Iteration 244, loss = 0.07975819\n",
      "Iteration 245, loss = 0.07966913\n",
      "Iteration 246, loss = 0.07959236\n",
      "Iteration 247, loss = 0.07947883\n",
      "Iteration 248, loss = 0.07939540\n",
      "Iteration 249, loss = 0.07930825\n",
      "Iteration 250, loss = 0.07927804\n",
      "Iteration 251, loss = 0.07915842\n",
      "Iteration 252, loss = 0.07906400\n",
      "Iteration 253, loss = 0.07899354\n",
      "Iteration 254, loss = 0.07889424\n",
      "Iteration 255, loss = 0.07880681\n",
      "Iteration 256, loss = 0.07872681\n",
      "Iteration 257, loss = 0.07864742\n",
      "Iteration 258, loss = 0.07856384\n",
      "Iteration 259, loss = 0.07849754\n",
      "Iteration 260, loss = 0.07842076\n",
      "Iteration 261, loss = 0.07837476\n",
      "Iteration 262, loss = 0.07829194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.55838032\n",
      "Iteration 2, loss = 0.52291013\n",
      "Iteration 3, loss = 0.48216564\n",
      "Iteration 4, loss = 0.44510363\n",
      "Iteration 5, loss = 0.41245094\n",
      "Iteration 6, loss = 0.38551660\n",
      "Iteration 7, loss = 0.36229577\n",
      "Iteration 8, loss = 0.34284675\n",
      "Iteration 9, loss = 0.32605415\n",
      "Iteration 10, loss = 0.31099999\n",
      "Iteration 11, loss = 0.29819888\n",
      "Iteration 12, loss = 0.28673143\n",
      "Iteration 13, loss = 0.27708285\n",
      "Iteration 14, loss = 0.26813173\n",
      "Iteration 15, loss = 0.25998040\n",
      "Iteration 16, loss = 0.25292653\n",
      "Iteration 17, loss = 0.24656314\n",
      "Iteration 18, loss = 0.24038419\n",
      "Iteration 19, loss = 0.23497907\n",
      "Iteration 20, loss = 0.23014253\n",
      "Iteration 21, loss = 0.22549738\n",
      "Iteration 22, loss = 0.22126219\n",
      "Iteration 23, loss = 0.21735940\n",
      "Iteration 24, loss = 0.21377705\n",
      "Iteration 25, loss = 0.21032499\n",
      "Iteration 26, loss = 0.20722274\n",
      "Iteration 27, loss = 0.20437006\n",
      "Iteration 28, loss = 0.20152459\n",
      "Iteration 29, loss = 0.19891148\n",
      "Iteration 30, loss = 0.19648812\n",
      "Iteration 31, loss = 0.19412467\n",
      "Iteration 32, loss = 0.19194281\n",
      "Iteration 33, loss = 0.18986106\n",
      "Iteration 34, loss = 0.18783562\n",
      "Iteration 35, loss = 0.18595336\n",
      "Iteration 36, loss = 0.18406918\n",
      "Iteration 37, loss = 0.18230998\n",
      "Iteration 38, loss = 0.18068428\n",
      "Iteration 39, loss = 0.17901118\n",
      "Iteration 40, loss = 0.17748421\n",
      "Iteration 41, loss = 0.17590950\n",
      "Iteration 42, loss = 0.17450925\n",
      "Iteration 43, loss = 0.17307509\n",
      "Iteration 44, loss = 0.17173523\n",
      "Iteration 45, loss = 0.17041246\n",
      "Iteration 46, loss = 0.16919518\n",
      "Iteration 47, loss = 0.16792008\n",
      "Iteration 48, loss = 0.16673214\n",
      "Iteration 49, loss = 0.16557483\n",
      "Iteration 50, loss = 0.16439631\n",
      "Iteration 51, loss = 0.16330827\n",
      "Iteration 52, loss = 0.16228910\n",
      "Iteration 53, loss = 0.16122414\n",
      "Iteration 54, loss = 0.16022931\n",
      "Iteration 55, loss = 0.15921017\n",
      "Iteration 56, loss = 0.15826913\n",
      "Iteration 57, loss = 0.15734099\n",
      "Iteration 58, loss = 0.15634741\n",
      "Iteration 59, loss = 0.15550596\n",
      "Iteration 60, loss = 0.15455175\n",
      "Iteration 61, loss = 0.15362948\n",
      "Iteration 62, loss = 0.15277927\n",
      "Iteration 63, loss = 0.15182417\n",
      "Iteration 64, loss = 0.15090617\n",
      "Iteration 65, loss = 0.15003337\n",
      "Iteration 66, loss = 0.14921361\n",
      "Iteration 67, loss = 0.14837157\n",
      "Iteration 68, loss = 0.14744741\n",
      "Iteration 69, loss = 0.14658178\n",
      "Iteration 70, loss = 0.14576323\n",
      "Iteration 71, loss = 0.14490728\n",
      "Iteration 72, loss = 0.14411652\n",
      "Iteration 73, loss = 0.14329830\n",
      "Iteration 74, loss = 0.14257665\n",
      "Iteration 75, loss = 0.14168255\n",
      "Iteration 76, loss = 0.14091452\n",
      "Iteration 77, loss = 0.14010784\n",
      "Iteration 78, loss = 0.13933654\n",
      "Iteration 79, loss = 0.13857323\n",
      "Iteration 80, loss = 0.13781899\n",
      "Iteration 81, loss = 0.13710345\n",
      "Iteration 82, loss = 0.13638223\n",
      "Iteration 83, loss = 0.13567509\n",
      "Iteration 84, loss = 0.13498940\n",
      "Iteration 85, loss = 0.13431080\n",
      "Iteration 86, loss = 0.13361173\n",
      "Iteration 87, loss = 0.13307164\n",
      "Iteration 88, loss = 0.13237709\n",
      "Iteration 89, loss = 0.13167615\n",
      "Iteration 90, loss = 0.13105463\n",
      "Iteration 91, loss = 0.13045216\n",
      "Iteration 92, loss = 0.12975309\n",
      "Iteration 93, loss = 0.12915439\n",
      "Iteration 94, loss = 0.12850464\n",
      "Iteration 95, loss = 0.12796919\n",
      "Iteration 96, loss = 0.12734609\n",
      "Iteration 97, loss = 0.12672913\n",
      "Iteration 98, loss = 0.12617127\n",
      "Iteration 99, loss = 0.12558626\n",
      "Iteration 100, loss = 0.12504876\n",
      "Iteration 101, loss = 0.12447103\n",
      "Iteration 102, loss = 0.12393735\n",
      "Iteration 103, loss = 0.12336482\n",
      "Iteration 104, loss = 0.12288743\n",
      "Iteration 105, loss = 0.12232057\n",
      "Iteration 106, loss = 0.12184638\n",
      "Iteration 107, loss = 0.12134294\n",
      "Iteration 108, loss = 0.12087888\n",
      "Iteration 109, loss = 0.12041019\n",
      "Iteration 110, loss = 0.11996690\n",
      "Iteration 111, loss = 0.11954027\n",
      "Iteration 112, loss = 0.11903655\n",
      "Iteration 113, loss = 0.11856279\n",
      "Iteration 114, loss = 0.11810424\n",
      "Iteration 115, loss = 0.11767692\n",
      "Iteration 116, loss = 0.11724457\n",
      "Iteration 117, loss = 0.11686861\n",
      "Iteration 118, loss = 0.11641175\n",
      "Iteration 119, loss = 0.11604393\n",
      "Iteration 120, loss = 0.11565341\n",
      "Iteration 121, loss = 0.11530113\n",
      "Iteration 122, loss = 0.11488554\n",
      "Iteration 123, loss = 0.11452784\n",
      "Iteration 124, loss = 0.11416165\n",
      "Iteration 125, loss = 0.11378338\n",
      "Iteration 126, loss = 0.11343604\n",
      "Iteration 127, loss = 0.11308975\n",
      "Iteration 128, loss = 0.11277355\n",
      "Iteration 129, loss = 0.11241526\n",
      "Iteration 130, loss = 0.11209048\n",
      "Iteration 131, loss = 0.11177917\n",
      "Iteration 132, loss = 0.11146423\n",
      "Iteration 133, loss = 0.11115965\n",
      "Iteration 134, loss = 0.11083559\n",
      "Iteration 135, loss = 0.11053701\n",
      "Iteration 136, loss = 0.11024445\n",
      "Iteration 137, loss = 0.10994089\n",
      "Iteration 138, loss = 0.10962775\n",
      "Iteration 139, loss = 0.10925819\n",
      "Iteration 140, loss = 0.10894970\n",
      "Iteration 141, loss = 0.10865251\n",
      "Iteration 142, loss = 0.10835026\n",
      "Iteration 143, loss = 0.10806637\n",
      "Iteration 144, loss = 0.10778978\n",
      "Iteration 145, loss = 0.10749355\n",
      "Iteration 146, loss = 0.10720577\n",
      "Iteration 147, loss = 0.10695589\n",
      "Iteration 148, loss = 0.10671224\n",
      "Iteration 149, loss = 0.10640679\n",
      "Iteration 150, loss = 0.10613499\n",
      "Iteration 151, loss = 0.10584372\n",
      "Iteration 152, loss = 0.10560524\n",
      "Iteration 153, loss = 0.10533223\n",
      "Iteration 154, loss = 0.10507582\n",
      "Iteration 155, loss = 0.10481680\n",
      "Iteration 156, loss = 0.10456237\n",
      "Iteration 157, loss = 0.10435983\n",
      "Iteration 158, loss = 0.10406348\n",
      "Iteration 159, loss = 0.10384184\n",
      "Iteration 160, loss = 0.10359074\n",
      "Iteration 161, loss = 0.10338390\n",
      "Iteration 162, loss = 0.10313966\n",
      "Iteration 163, loss = 0.10289254\n",
      "Iteration 164, loss = 0.10266181\n",
      "Iteration 165, loss = 0.10245521\n",
      "Iteration 166, loss = 0.10222465\n",
      "Iteration 167, loss = 0.10201260\n",
      "Iteration 168, loss = 0.10182584\n",
      "Iteration 169, loss = 0.10159333\n",
      "Iteration 170, loss = 0.10139464\n",
      "Iteration 171, loss = 0.10122968\n",
      "Iteration 172, loss = 0.10097365\n",
      "Iteration 173, loss = 0.10080221\n",
      "Iteration 174, loss = 0.10059166\n",
      "Iteration 175, loss = 0.10037919\n",
      "Iteration 176, loss = 0.10018990\n",
      "Iteration 177, loss = 0.10002946\n",
      "Iteration 178, loss = 0.09991288\n",
      "Iteration 179, loss = 0.09965701\n",
      "Iteration 180, loss = 0.09946226\n",
      "Iteration 181, loss = 0.09932188\n",
      "Iteration 182, loss = 0.09913504\n",
      "Iteration 183, loss = 0.09896092\n",
      "Iteration 184, loss = 0.09879193\n",
      "Iteration 185, loss = 0.09859894\n",
      "Iteration 186, loss = 0.09842985\n",
      "Iteration 187, loss = 0.09830875\n",
      "Iteration 188, loss = 0.09813908\n",
      "Iteration 189, loss = 0.09803850\n",
      "Iteration 190, loss = 0.09779173\n",
      "Iteration 191, loss = 0.09763214\n",
      "Iteration 192, loss = 0.09749820\n",
      "Iteration 193, loss = 0.09733344\n",
      "Iteration 194, loss = 0.09718662\n",
      "Iteration 195, loss = 0.09703426\n",
      "Iteration 196, loss = 0.09688523\n",
      "Iteration 197, loss = 0.09673606\n",
      "Iteration 198, loss = 0.09661568\n",
      "Iteration 199, loss = 0.09644903\n",
      "Iteration 200, loss = 0.09633287\n",
      "Iteration 201, loss = 0.09619288\n",
      "Iteration 202, loss = 0.09604749\n",
      "Iteration 203, loss = 0.09591796\n",
      "Iteration 204, loss = 0.09578414\n",
      "Iteration 205, loss = 0.09561165\n",
      "Iteration 206, loss = 0.09550389\n",
      "Iteration 207, loss = 0.09537595\n",
      "Iteration 208, loss = 0.09524272\n",
      "Iteration 209, loss = 0.09514297\n",
      "Iteration 210, loss = 0.09501722\n",
      "Iteration 211, loss = 0.09488621\n",
      "Iteration 212, loss = 0.09477948\n",
      "Iteration 213, loss = 0.09471477\n",
      "Iteration 214, loss = 0.09452848\n",
      "Iteration 215, loss = 0.09446853\n",
      "Iteration 216, loss = 0.09431520\n",
      "Iteration 217, loss = 0.09420373\n",
      "Iteration 218, loss = 0.09407133\n",
      "Iteration 219, loss = 0.09396757\n",
      "Iteration 220, loss = 0.09384956\n",
      "Iteration 221, loss = 0.09373550\n",
      "Iteration 222, loss = 0.09367006\n",
      "Iteration 223, loss = 0.09353880\n",
      "Iteration 224, loss = 0.09343581\n",
      "Iteration 225, loss = 0.09332146\n",
      "Iteration 226, loss = 0.09319556\n",
      "Iteration 227, loss = 0.09309286\n",
      "Iteration 228, loss = 0.09298894\n",
      "Iteration 229, loss = 0.09289315\n",
      "Iteration 230, loss = 0.09278852\n",
      "Iteration 231, loss = 0.09268626\n",
      "Iteration 232, loss = 0.09258111\n",
      "Iteration 233, loss = 0.09248835\n",
      "Iteration 234, loss = 0.09242432\n",
      "Iteration 235, loss = 0.09230255\n",
      "Iteration 236, loss = 0.09219563\n",
      "Iteration 237, loss = 0.09211199\n",
      "Iteration 238, loss = 0.09201601\n",
      "Iteration 239, loss = 0.09194841\n",
      "Iteration 240, loss = 0.09182608\n",
      "Iteration 241, loss = 0.09175499\n",
      "Iteration 242, loss = 0.09164535\n",
      "Iteration 243, loss = 0.09156118\n",
      "Iteration 244, loss = 0.09149187\n",
      "Iteration 245, loss = 0.09139063\n",
      "Iteration 246, loss = 0.09129600\n",
      "Iteration 247, loss = 0.09120130\n",
      "Iteration 248, loss = 0.09112451\n",
      "Iteration 249, loss = 0.09102550\n",
      "Iteration 250, loss = 0.09096706\n",
      "Iteration 251, loss = 0.09090133\n",
      "Iteration 252, loss = 0.09079591\n",
      "Iteration 253, loss = 0.09075392\n",
      "Iteration 254, loss = 0.09065452\n",
      "Iteration 255, loss = 0.09056554\n",
      "Iteration 256, loss = 0.09046374\n",
      "Iteration 257, loss = 0.09040004\n",
      "Iteration 258, loss = 0.09030853\n",
      "Iteration 259, loss = 0.09022236\n",
      "Iteration 260, loss = 0.09017847\n",
      "Iteration 261, loss = 0.09011462\n",
      "Iteration 262, loss = 0.09003850\n",
      "Iteration 263, loss = 0.08992426\n",
      "Iteration 264, loss = 0.08983088\n",
      "Iteration 265, loss = 0.08977468\n",
      "Iteration 266, loss = 0.08971363\n",
      "Iteration 267, loss = 0.08960173\n",
      "Iteration 268, loss = 0.08954231\n",
      "Iteration 269, loss = 0.08945637\n",
      "Iteration 270, loss = 0.08936152\n",
      "Iteration 271, loss = 0.08929778\n",
      "Iteration 272, loss = 0.08923170\n",
      "Iteration 273, loss = 0.08913057\n",
      "Iteration 274, loss = 0.08904985\n",
      "Iteration 275, loss = 0.08896969\n",
      "Iteration 276, loss = 0.08891098\n",
      "Iteration 277, loss = 0.08884363\n",
      "Iteration 278, loss = 0.08878237\n",
      "Iteration 279, loss = 0.08871115\n",
      "Iteration 280, loss = 0.08859098\n",
      "Iteration 281, loss = 0.08853026\n",
      "Iteration 282, loss = 0.08845421\n",
      "Iteration 283, loss = 0.08838609\n",
      "Iteration 284, loss = 0.08831831\n",
      "Iteration 285, loss = 0.08824819\n",
      "Iteration 286, loss = 0.08816734\n",
      "Iteration 287, loss = 0.08814070\n",
      "Iteration 288, loss = 0.08803282\n",
      "Iteration 289, loss = 0.08798444\n",
      "Iteration 290, loss = 0.08790484\n",
      "Iteration 291, loss = 0.08782199\n",
      "Iteration 292, loss = 0.08778460\n",
      "Iteration 293, loss = 0.08770441\n",
      "Iteration 294, loss = 0.08765017\n",
      "Iteration 295, loss = 0.08761364\n",
      "Iteration 296, loss = 0.08750398\n",
      "Iteration 297, loss = 0.08743455\n",
      "Iteration 298, loss = 0.08736220\n",
      "Iteration 299, loss = 0.08731307\n",
      "Iteration 300, loss = 0.08723008\n",
      "Iteration 301, loss = 0.08718271\n",
      "Iteration 302, loss = 0.08710869\n",
      "Iteration 303, loss = 0.08703458\n",
      "Iteration 304, loss = 0.08697724\n",
      "Iteration 305, loss = 0.08694158\n",
      "Iteration 306, loss = 0.08687049\n",
      "Iteration 307, loss = 0.08680584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56803808\n",
      "Iteration 2, loss = 0.53157171\n",
      "Iteration 3, loss = 0.48919712\n",
      "Iteration 4, loss = 0.45055417\n",
      "Iteration 5, loss = 0.41646321\n",
      "Iteration 6, loss = 0.38844104\n",
      "Iteration 7, loss = 0.36412623\n",
      "Iteration 8, loss = 0.34387347\n",
      "Iteration 9, loss = 0.32633869\n",
      "Iteration 10, loss = 0.31081562\n",
      "Iteration 11, loss = 0.29741523\n",
      "Iteration 12, loss = 0.28565816\n",
      "Iteration 13, loss = 0.27545081\n",
      "Iteration 14, loss = 0.26625779\n",
      "Iteration 15, loss = 0.25787169\n",
      "Iteration 16, loss = 0.25045484\n",
      "Iteration 17, loss = 0.24374824\n",
      "Iteration 18, loss = 0.23751302\n",
      "Iteration 19, loss = 0.23197918\n",
      "Iteration 20, loss = 0.22698715\n",
      "Iteration 21, loss = 0.22218312\n",
      "Iteration 22, loss = 0.21792368\n",
      "Iteration 23, loss = 0.21393268\n",
      "Iteration 24, loss = 0.21023981\n",
      "Iteration 25, loss = 0.20677138\n",
      "Iteration 26, loss = 0.20358068\n",
      "Iteration 27, loss = 0.20055257\n",
      "Iteration 28, loss = 0.19770914\n",
      "Iteration 29, loss = 0.19509016\n",
      "Iteration 30, loss = 0.19260142\n",
      "Iteration 31, loss = 0.19015663\n",
      "Iteration 32, loss = 0.18796132\n",
      "Iteration 33, loss = 0.18584773\n",
      "Iteration 34, loss = 0.18377238\n",
      "Iteration 35, loss = 0.18187716\n",
      "Iteration 36, loss = 0.17996911\n",
      "Iteration 37, loss = 0.17823813\n",
      "Iteration 38, loss = 0.17656623\n",
      "Iteration 39, loss = 0.17492458\n",
      "Iteration 40, loss = 0.17341573\n",
      "Iteration 41, loss = 0.17186561\n",
      "Iteration 42, loss = 0.17051353\n",
      "Iteration 43, loss = 0.16911030\n",
      "Iteration 44, loss = 0.16781636\n",
      "Iteration 45, loss = 0.16655042\n",
      "Iteration 46, loss = 0.16533471\n",
      "Iteration 47, loss = 0.16418239\n",
      "Iteration 48, loss = 0.16303627\n",
      "Iteration 49, loss = 0.16192845\n",
      "Iteration 50, loss = 0.16087268\n",
      "Iteration 51, loss = 0.15986468\n",
      "Iteration 52, loss = 0.15890180\n",
      "Iteration 53, loss = 0.15793435\n",
      "Iteration 54, loss = 0.15701933\n",
      "Iteration 55, loss = 0.15608332\n",
      "Iteration 56, loss = 0.15527065\n",
      "Iteration 57, loss = 0.15440480\n",
      "Iteration 58, loss = 0.15351823\n",
      "Iteration 59, loss = 0.15274624\n",
      "Iteration 60, loss = 0.15196405\n",
      "Iteration 61, loss = 0.15117972\n",
      "Iteration 62, loss = 0.15045456\n",
      "Iteration 63, loss = 0.14975091\n",
      "Iteration 64, loss = 0.14898121\n",
      "Iteration 65, loss = 0.14827193\n",
      "Iteration 66, loss = 0.14763744\n",
      "Iteration 67, loss = 0.14697240\n",
      "Iteration 68, loss = 0.14630914\n",
      "Iteration 69, loss = 0.14568478\n",
      "Iteration 70, loss = 0.14508628\n",
      "Iteration 71, loss = 0.14446600\n",
      "Iteration 72, loss = 0.14387402\n",
      "Iteration 73, loss = 0.14328479\n",
      "Iteration 74, loss = 0.14276686\n",
      "Iteration 75, loss = 0.14213868\n",
      "Iteration 76, loss = 0.14161510\n",
      "Iteration 77, loss = 0.14103510\n",
      "Iteration 78, loss = 0.14049135\n",
      "Iteration 79, loss = 0.13995698\n",
      "Iteration 80, loss = 0.13945640\n",
      "Iteration 81, loss = 0.13895046\n",
      "Iteration 82, loss = 0.13844425\n",
      "Iteration 83, loss = 0.13793832\n",
      "Iteration 84, loss = 0.13746790\n",
      "Iteration 85, loss = 0.13698538\n",
      "Iteration 86, loss = 0.13647291\n",
      "Iteration 87, loss = 0.13603701\n",
      "Iteration 88, loss = 0.13555789\n",
      "Iteration 89, loss = 0.13506232\n",
      "Iteration 90, loss = 0.13459286\n",
      "Iteration 91, loss = 0.13411049\n",
      "Iteration 92, loss = 0.13362930\n",
      "Iteration 93, loss = 0.13321149\n",
      "Iteration 94, loss = 0.13268922\n",
      "Iteration 95, loss = 0.13229675\n",
      "Iteration 96, loss = 0.13181501\n",
      "Iteration 97, loss = 0.13136932\n",
      "Iteration 98, loss = 0.13092345\n",
      "Iteration 99, loss = 0.13046143\n",
      "Iteration 100, loss = 0.13002443\n",
      "Iteration 101, loss = 0.12961036\n",
      "Iteration 102, loss = 0.12922325\n",
      "Iteration 103, loss = 0.12879961\n",
      "Iteration 104, loss = 0.12840011\n",
      "Iteration 105, loss = 0.12795086\n",
      "Iteration 106, loss = 0.12756062\n",
      "Iteration 107, loss = 0.12720094\n",
      "Iteration 108, loss = 0.12679410\n",
      "Iteration 109, loss = 0.12641304\n",
      "Iteration 110, loss = 0.12601262\n",
      "Iteration 111, loss = 0.12564295\n",
      "Iteration 112, loss = 0.12525458\n",
      "Iteration 113, loss = 0.12487113\n",
      "Iteration 114, loss = 0.12449549\n",
      "Iteration 115, loss = 0.12414234\n",
      "Iteration 116, loss = 0.12378931\n",
      "Iteration 117, loss = 0.12343390\n",
      "Iteration 118, loss = 0.12309101\n",
      "Iteration 119, loss = 0.12278418\n",
      "Iteration 120, loss = 0.12244846\n",
      "Iteration 121, loss = 0.12213871\n",
      "Iteration 122, loss = 0.12180444\n",
      "Iteration 123, loss = 0.12148911\n",
      "Iteration 124, loss = 0.12117674\n",
      "Iteration 125, loss = 0.12086530\n",
      "Iteration 126, loss = 0.12056163\n",
      "Iteration 127, loss = 0.12028398\n",
      "Iteration 128, loss = 0.11999344\n",
      "Iteration 129, loss = 0.11969789\n",
      "Iteration 130, loss = 0.11939342\n",
      "Iteration 131, loss = 0.11909552\n",
      "Iteration 132, loss = 0.11878805\n",
      "Iteration 133, loss = 0.11848381\n",
      "Iteration 134, loss = 0.11817639\n",
      "Iteration 135, loss = 0.11788666\n",
      "Iteration 136, loss = 0.11761667\n",
      "Iteration 137, loss = 0.11733829\n",
      "Iteration 138, loss = 0.11705889\n",
      "Iteration 139, loss = 0.11675323\n",
      "Iteration 140, loss = 0.11647435\n",
      "Iteration 141, loss = 0.11621632\n",
      "Iteration 142, loss = 0.11594107\n",
      "Iteration 143, loss = 0.11569676\n",
      "Iteration 144, loss = 0.11543417\n",
      "Iteration 145, loss = 0.11515694\n",
      "Iteration 146, loss = 0.11491359\n",
      "Iteration 147, loss = 0.11471234\n",
      "Iteration 148, loss = 0.11448053\n",
      "Iteration 149, loss = 0.11419087\n",
      "Iteration 150, loss = 0.11395753\n",
      "Iteration 151, loss = 0.11367558\n",
      "Iteration 152, loss = 0.11345237\n",
      "Iteration 153, loss = 0.11320572\n",
      "Iteration 154, loss = 0.11297860\n",
      "Iteration 155, loss = 0.11275810\n",
      "Iteration 156, loss = 0.11252291\n",
      "Iteration 157, loss = 0.11235022\n",
      "Iteration 158, loss = 0.11207259\n",
      "Iteration 159, loss = 0.11187073\n",
      "Iteration 160, loss = 0.11164902\n",
      "Iteration 161, loss = 0.11146255\n",
      "Iteration 162, loss = 0.11123187\n",
      "Iteration 163, loss = 0.11102992\n",
      "Iteration 164, loss = 0.11081257\n",
      "Iteration 165, loss = 0.11064080\n",
      "Iteration 166, loss = 0.11042031\n",
      "Iteration 167, loss = 0.11023680\n",
      "Iteration 168, loss = 0.11004821\n",
      "Iteration 169, loss = 0.10983884\n",
      "Iteration 170, loss = 0.10966036\n",
      "Iteration 171, loss = 0.10949592\n",
      "Iteration 172, loss = 0.10927144\n",
      "Iteration 173, loss = 0.10911664\n",
      "Iteration 174, loss = 0.10890587\n",
      "Iteration 175, loss = 0.10871109\n",
      "Iteration 176, loss = 0.10854922\n",
      "Iteration 177, loss = 0.10839489\n",
      "Iteration 178, loss = 0.10827464\n",
      "Iteration 179, loss = 0.10805083\n",
      "Iteration 180, loss = 0.10785336\n",
      "Iteration 181, loss = 0.10770897\n",
      "Iteration 182, loss = 0.10753562\n",
      "Iteration 183, loss = 0.10736321\n",
      "Iteration 184, loss = 0.10718508\n",
      "Iteration 185, loss = 0.10697645\n",
      "Iteration 186, loss = 0.10681115\n",
      "Iteration 187, loss = 0.10668813\n",
      "Iteration 188, loss = 0.10648895\n",
      "Iteration 189, loss = 0.10639544\n",
      "Iteration 190, loss = 0.10614234\n",
      "Iteration 191, loss = 0.10597901\n",
      "Iteration 192, loss = 0.10580645\n",
      "Iteration 193, loss = 0.10564318\n",
      "Iteration 194, loss = 0.10550303\n",
      "Iteration 195, loss = 0.10532627\n",
      "Iteration 196, loss = 0.10516624\n",
      "Iteration 197, loss = 0.10500098\n",
      "Iteration 198, loss = 0.10488032\n",
      "Iteration 199, loss = 0.10469584\n",
      "Iteration 200, loss = 0.10458671\n",
      "Iteration 201, loss = 0.10441486\n",
      "Iteration 202, loss = 0.10426315\n",
      "Iteration 203, loss = 0.10414375\n",
      "Iteration 204, loss = 0.10397889\n",
      "Iteration 205, loss = 0.10378844\n",
      "Iteration 206, loss = 0.10366221\n",
      "Iteration 207, loss = 0.10350369\n",
      "Iteration 208, loss = 0.10337332\n",
      "Iteration 209, loss = 0.10324562\n",
      "Iteration 210, loss = 0.10309618\n",
      "Iteration 211, loss = 0.10294978\n",
      "Iteration 212, loss = 0.10281763\n",
      "Iteration 213, loss = 0.10274990\n",
      "Iteration 214, loss = 0.10254044\n",
      "Iteration 215, loss = 0.10246959\n",
      "Iteration 216, loss = 0.10229588\n",
      "Iteration 217, loss = 0.10216364\n",
      "Iteration 218, loss = 0.10203145\n",
      "Iteration 219, loss = 0.10190582\n",
      "Iteration 220, loss = 0.10179465\n",
      "Iteration 221, loss = 0.10163200\n",
      "Iteration 222, loss = 0.10154420\n",
      "Iteration 223, loss = 0.10140945\n",
      "Iteration 224, loss = 0.10128843\n",
      "Iteration 225, loss = 0.10115784\n",
      "Iteration 226, loss = 0.10103436\n",
      "Iteration 227, loss = 0.10091841\n",
      "Iteration 228, loss = 0.10079434\n",
      "Iteration 229, loss = 0.10069299\n",
      "Iteration 230, loss = 0.10057358\n",
      "Iteration 231, loss = 0.10046716\n",
      "Iteration 232, loss = 0.10034505\n",
      "Iteration 233, loss = 0.10023800\n",
      "Iteration 234, loss = 0.10015535\n",
      "Iteration 235, loss = 0.10004071\n",
      "Iteration 236, loss = 0.09991555\n",
      "Iteration 237, loss = 0.09980423\n",
      "Iteration 238, loss = 0.09969717\n",
      "Iteration 239, loss = 0.09961836\n",
      "Iteration 240, loss = 0.09947075\n",
      "Iteration 241, loss = 0.09939292\n",
      "Iteration 242, loss = 0.09923159\n",
      "Iteration 243, loss = 0.09911206\n",
      "Iteration 244, loss = 0.09900799\n",
      "Iteration 245, loss = 0.09887759\n",
      "Iteration 246, loss = 0.09875515\n",
      "Iteration 247, loss = 0.09864291\n",
      "Iteration 248, loss = 0.09852156\n",
      "Iteration 249, loss = 0.09841583\n",
      "Iteration 250, loss = 0.09830820\n",
      "Iteration 251, loss = 0.09822808\n",
      "Iteration 252, loss = 0.09807372\n",
      "Iteration 253, loss = 0.09801376\n",
      "Iteration 254, loss = 0.09789366\n",
      "Iteration 255, loss = 0.09776833\n",
      "Iteration 256, loss = 0.09764892\n",
      "Iteration 257, loss = 0.09754400\n",
      "Iteration 258, loss = 0.09743904\n",
      "Iteration 259, loss = 0.09733387\n",
      "Iteration 260, loss = 0.09727019\n",
      "Iteration 261, loss = 0.09718426\n",
      "Iteration 262, loss = 0.09707995\n",
      "Iteration 263, loss = 0.09696005\n",
      "Iteration 264, loss = 0.09685134\n",
      "Iteration 265, loss = 0.09680523\n",
      "Iteration 266, loss = 0.09673119\n",
      "Iteration 267, loss = 0.09663663\n",
      "Iteration 268, loss = 0.09654166\n",
      "Iteration 269, loss = 0.09646517\n",
      "Iteration 270, loss = 0.09638284\n",
      "Iteration 271, loss = 0.09630726\n",
      "Iteration 272, loss = 0.09624735\n",
      "Iteration 273, loss = 0.09614380\n",
      "Iteration 274, loss = 0.09606641\n",
      "Iteration 275, loss = 0.09598386\n",
      "Iteration 276, loss = 0.09592021\n",
      "Iteration 277, loss = 0.09585028\n",
      "Iteration 278, loss = 0.09580710\n",
      "Iteration 279, loss = 0.09571363\n",
      "Iteration 280, loss = 0.09560951\n",
      "Iteration 281, loss = 0.09552606\n",
      "Iteration 282, loss = 0.09546348\n",
      "Iteration 283, loss = 0.09538741\n",
      "Iteration 284, loss = 0.09533820\n",
      "Iteration 285, loss = 0.09522785\n",
      "Iteration 286, loss = 0.09516626\n",
      "Iteration 287, loss = 0.09513715\n",
      "Iteration 288, loss = 0.09500802\n",
      "Iteration 289, loss = 0.09495330\n",
      "Iteration 290, loss = 0.09488799\n",
      "Iteration 291, loss = 0.09479725\n",
      "Iteration 292, loss = 0.09474910\n",
      "Iteration 293, loss = 0.09468398\n",
      "Iteration 294, loss = 0.09464315\n",
      "Iteration 295, loss = 0.09458547\n",
      "Iteration 296, loss = 0.09450086\n",
      "Iteration 297, loss = 0.09442626\n",
      "Iteration 298, loss = 0.09435912\n",
      "Iteration 299, loss = 0.09430485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.56344225\n",
      "Iteration 2, loss = 0.52740961\n",
      "Iteration 3, loss = 0.48567934\n",
      "Iteration 4, loss = 0.44754221\n",
      "Iteration 5, loss = 0.41386048\n",
      "Iteration 6, loss = 0.38632438\n",
      "Iteration 7, loss = 0.36264248\n",
      "Iteration 8, loss = 0.34250568\n",
      "Iteration 9, loss = 0.32564312\n",
      "Iteration 10, loss = 0.31032000\n",
      "Iteration 11, loss = 0.29723886\n",
      "Iteration 12, loss = 0.28589536\n",
      "Iteration 13, loss = 0.27594456\n",
      "Iteration 14, loss = 0.26693304\n",
      "Iteration 15, loss = 0.25879908\n",
      "Iteration 16, loss = 0.25172699\n",
      "Iteration 17, loss = 0.24518722\n",
      "Iteration 18, loss = 0.23904746\n",
      "Iteration 19, loss = 0.23369815\n",
      "Iteration 20, loss = 0.22895272\n",
      "Iteration 21, loss = 0.22427265\n",
      "Iteration 22, loss = 0.22015460\n",
      "Iteration 23, loss = 0.21630768\n",
      "Iteration 24, loss = 0.21277809\n",
      "Iteration 25, loss = 0.20931242\n",
      "Iteration 26, loss = 0.20622758\n",
      "Iteration 27, loss = 0.20324880\n",
      "Iteration 28, loss = 0.20048240\n",
      "Iteration 29, loss = 0.19796253\n",
      "Iteration 30, loss = 0.19555321\n",
      "Iteration 31, loss = 0.19316566\n",
      "Iteration 32, loss = 0.19104748\n",
      "Iteration 33, loss = 0.18893332\n",
      "Iteration 34, loss = 0.18695796\n",
      "Iteration 35, loss = 0.18508164\n",
      "Iteration 36, loss = 0.18319370\n",
      "Iteration 37, loss = 0.18149221\n",
      "Iteration 38, loss = 0.17983042\n",
      "Iteration 39, loss = 0.17822834\n",
      "Iteration 40, loss = 0.17669631\n",
      "Iteration 41, loss = 0.17516059\n",
      "Iteration 42, loss = 0.17379043\n",
      "Iteration 43, loss = 0.17238372\n",
      "Iteration 44, loss = 0.17104962\n",
      "Iteration 45, loss = 0.16976673\n",
      "Iteration 46, loss = 0.16849885\n",
      "Iteration 47, loss = 0.16728118\n",
      "Iteration 48, loss = 0.16606344\n",
      "Iteration 49, loss = 0.16490784\n",
      "Iteration 50, loss = 0.16381241\n",
      "Iteration 51, loss = 0.16271929\n",
      "Iteration 52, loss = 0.16169056\n",
      "Iteration 53, loss = 0.16064135\n",
      "Iteration 54, loss = 0.15964881\n",
      "Iteration 55, loss = 0.15865295\n",
      "Iteration 56, loss = 0.15775817\n",
      "Iteration 57, loss = 0.15683384\n",
      "Iteration 58, loss = 0.15589343\n",
      "Iteration 59, loss = 0.15504077\n",
      "Iteration 60, loss = 0.15420164\n",
      "Iteration 61, loss = 0.15333560\n",
      "Iteration 62, loss = 0.15247320\n",
      "Iteration 63, loss = 0.15172096\n",
      "Iteration 64, loss = 0.15080426\n",
      "Iteration 65, loss = 0.14999412\n",
      "Iteration 66, loss = 0.14925712\n",
      "Iteration 67, loss = 0.14848267\n",
      "Iteration 68, loss = 0.14768704\n",
      "Iteration 69, loss = 0.14696982\n",
      "Iteration 70, loss = 0.14623636\n",
      "Iteration 71, loss = 0.14546916\n",
      "Iteration 72, loss = 0.14475263\n",
      "Iteration 73, loss = 0.14399007\n",
      "Iteration 74, loss = 0.14337135\n",
      "Iteration 75, loss = 0.14253616\n",
      "Iteration 76, loss = 0.14186102\n",
      "Iteration 77, loss = 0.14110598\n",
      "Iteration 78, loss = 0.14039499\n",
      "Iteration 79, loss = 0.13971822\n",
      "Iteration 80, loss = 0.13903303\n",
      "Iteration 81, loss = 0.13836538\n",
      "Iteration 82, loss = 0.13767967\n",
      "Iteration 83, loss = 0.13702863\n",
      "Iteration 84, loss = 0.13640132\n",
      "Iteration 85, loss = 0.13582215\n",
      "Iteration 86, loss = 0.13511423\n",
      "Iteration 87, loss = 0.13454971\n",
      "Iteration 88, loss = 0.13391351\n",
      "Iteration 89, loss = 0.13329636\n",
      "Iteration 90, loss = 0.13273314\n",
      "Iteration 91, loss = 0.13213662\n",
      "Iteration 92, loss = 0.13151538\n",
      "Iteration 93, loss = 0.13102299\n",
      "Iteration 94, loss = 0.13033964\n",
      "Iteration 95, loss = 0.12986002\n",
      "Iteration 96, loss = 0.12926649\n",
      "Iteration 97, loss = 0.12872646\n",
      "Iteration 98, loss = 0.12821454\n",
      "Iteration 99, loss = 0.12771229\n",
      "Iteration 100, loss = 0.12718672\n",
      "Iteration 101, loss = 0.12671046\n",
      "Iteration 102, loss = 0.12624832\n",
      "Iteration 103, loss = 0.12577138\n",
      "Iteration 104, loss = 0.12534481\n",
      "Iteration 105, loss = 0.12484771\n",
      "Iteration 106, loss = 0.12441314\n",
      "Iteration 107, loss = 0.12401414\n",
      "Iteration 108, loss = 0.12355713\n",
      "Iteration 109, loss = 0.12316218\n",
      "Iteration 110, loss = 0.12273199\n",
      "Iteration 111, loss = 0.12233438\n",
      "Iteration 112, loss = 0.12196873\n",
      "Iteration 113, loss = 0.12152885\n",
      "Iteration 114, loss = 0.12112847\n",
      "Iteration 115, loss = 0.12074762\n",
      "Iteration 116, loss = 0.12036064\n",
      "Iteration 117, loss = 0.11999721\n",
      "Iteration 118, loss = 0.11967267\n",
      "Iteration 119, loss = 0.11927471\n",
      "Iteration 120, loss = 0.11895578\n",
      "Iteration 121, loss = 0.11860789\n",
      "Iteration 122, loss = 0.11823437\n",
      "Iteration 123, loss = 0.11790021\n",
      "Iteration 124, loss = 0.11756625\n",
      "Iteration 125, loss = 0.11722854\n",
      "Iteration 126, loss = 0.11690818\n",
      "Iteration 127, loss = 0.11661376\n",
      "Iteration 128, loss = 0.11631497\n",
      "Iteration 129, loss = 0.11604270\n",
      "Iteration 130, loss = 0.11574186\n",
      "Iteration 131, loss = 0.11547258\n",
      "Iteration 132, loss = 0.11517141\n",
      "Iteration 133, loss = 0.11490916\n",
      "Iteration 134, loss = 0.11462542\n",
      "Iteration 135, loss = 0.11434460\n",
      "Iteration 136, loss = 0.11409888\n",
      "Iteration 137, loss = 0.11384388\n",
      "Iteration 138, loss = 0.11358744\n",
      "Iteration 139, loss = 0.11331055\n",
      "Iteration 140, loss = 0.11306397\n",
      "Iteration 141, loss = 0.11282755\n",
      "Iteration 142, loss = 0.11256815\n",
      "Iteration 143, loss = 0.11234695\n",
      "Iteration 144, loss = 0.11213460\n",
      "Iteration 145, loss = 0.11187538\n",
      "Iteration 146, loss = 0.11164828\n",
      "Iteration 147, loss = 0.11146099\n",
      "Iteration 148, loss = 0.11127471\n",
      "Iteration 149, loss = 0.11102870\n",
      "Iteration 150, loss = 0.11081174\n",
      "Iteration 151, loss = 0.11056070\n",
      "Iteration 152, loss = 0.11037509\n",
      "Iteration 153, loss = 0.11015644\n",
      "Iteration 154, loss = 0.10997641\n",
      "Iteration 155, loss = 0.10979446\n",
      "Iteration 156, loss = 0.10958292\n",
      "Iteration 157, loss = 0.10943169\n",
      "Iteration 158, loss = 0.10920828\n",
      "Iteration 159, loss = 0.10902875\n",
      "Iteration 160, loss = 0.10884561\n",
      "Iteration 161, loss = 0.10869405\n",
      "Iteration 162, loss = 0.10849662\n",
      "Iteration 163, loss = 0.10831776\n",
      "Iteration 164, loss = 0.10813882\n",
      "Iteration 165, loss = 0.10798613\n",
      "Iteration 166, loss = 0.10781117\n",
      "Iteration 167, loss = 0.10767465\n",
      "Iteration 168, loss = 0.10752423\n",
      "Iteration 169, loss = 0.10734384\n",
      "Iteration 170, loss = 0.10718378\n",
      "Iteration 171, loss = 0.10705025\n",
      "Iteration 172, loss = 0.10685094\n",
      "Iteration 173, loss = 0.10671329\n",
      "Iteration 174, loss = 0.10652047\n",
      "Iteration 175, loss = 0.10634207\n",
      "Iteration 176, loss = 0.10620153\n",
      "Iteration 177, loss = 0.10606577\n",
      "Iteration 178, loss = 0.10593966\n",
      "Iteration 179, loss = 0.10574463\n",
      "Iteration 180, loss = 0.10556706\n",
      "Iteration 181, loss = 0.10544773\n",
      "Iteration 182, loss = 0.10527833\n",
      "Iteration 183, loss = 0.10513723\n",
      "Iteration 184, loss = 0.10501046\n",
      "Iteration 185, loss = 0.10481010\n",
      "Iteration 186, loss = 0.10469334\n",
      "Iteration 187, loss = 0.10458281\n",
      "Iteration 188, loss = 0.10438928\n",
      "Iteration 189, loss = 0.10434681\n",
      "Iteration 190, loss = 0.10412327\n",
      "Iteration 191, loss = 0.10399711\n",
      "Iteration 192, loss = 0.10384902\n",
      "Iteration 193, loss = 0.10373503\n",
      "Iteration 194, loss = 0.10362678\n",
      "Iteration 195, loss = 0.10346438\n",
      "Iteration 196, loss = 0.10335919\n",
      "Iteration 197, loss = 0.10321488\n",
      "Iteration 198, loss = 0.10310350\n",
      "Iteration 199, loss = 0.10297296\n",
      "Iteration 200, loss = 0.10288848\n",
      "Iteration 201, loss = 0.10275634\n",
      "Iteration 202, loss = 0.10265049\n",
      "Iteration 203, loss = 0.10253585\n",
      "Iteration 204, loss = 0.10241818\n",
      "Iteration 205, loss = 0.10225756\n",
      "Iteration 206, loss = 0.10217324\n",
      "Iteration 207, loss = 0.10203565\n",
      "Iteration 208, loss = 0.10193993\n",
      "Iteration 209, loss = 0.10187472\n",
      "Iteration 210, loss = 0.10172384\n",
      "Iteration 211, loss = 0.10161410\n",
      "Iteration 212, loss = 0.10152639\n",
      "Iteration 213, loss = 0.10147197\n",
      "Iteration 214, loss = 0.10133088\n",
      "Iteration 215, loss = 0.10125423\n",
      "Iteration 216, loss = 0.10113337\n",
      "Iteration 217, loss = 0.10103422\n",
      "Iteration 218, loss = 0.10093974\n",
      "Iteration 219, loss = 0.10084650\n",
      "Iteration 220, loss = 0.10076826\n",
      "Iteration 221, loss = 0.10063327\n",
      "Iteration 222, loss = 0.10058490\n",
      "Iteration 223, loss = 0.10047459\n",
      "Iteration 224, loss = 0.10036828\n",
      "Iteration 225, loss = 0.10026866\n",
      "Iteration 226, loss = 0.10017719\n",
      "Iteration 227, loss = 0.10008277\n",
      "Iteration 228, loss = 0.09998672\n",
      "Iteration 229, loss = 0.09991181\n",
      "Iteration 230, loss = 0.09982522\n",
      "Iteration 231, loss = 0.09973068\n",
      "Iteration 232, loss = 0.09965100\n",
      "Iteration 233, loss = 0.09956822\n",
      "Iteration 234, loss = 0.09952622\n",
      "Iteration 235, loss = 0.09941398\n",
      "Iteration 236, loss = 0.09931753\n",
      "Iteration 237, loss = 0.09924107\n",
      "Iteration 238, loss = 0.09915009\n",
      "Iteration 239, loss = 0.09911517\n",
      "Iteration 240, loss = 0.09898436\n",
      "Iteration 241, loss = 0.09893968\n",
      "Iteration 242, loss = 0.09883242\n",
      "Iteration 243, loss = 0.09874476\n",
      "Iteration 244, loss = 0.09868954\n",
      "Iteration 245, loss = 0.09860167\n",
      "Iteration 246, loss = 0.09851552\n",
      "Iteration 247, loss = 0.09844102\n",
      "Iteration 248, loss = 0.09835558\n",
      "Iteration 249, loss = 0.09829632\n",
      "Iteration 250, loss = 0.09820522\n",
      "Iteration 251, loss = 0.09818248\n",
      "Iteration 252, loss = 0.09806638\n",
      "Iteration 253, loss = 0.09802402\n",
      "Iteration 254, loss = 0.09794747\n",
      "Iteration 255, loss = 0.09785114\n",
      "Iteration 256, loss = 0.09776621\n",
      "Iteration 257, loss = 0.09769984\n",
      "Iteration 258, loss = 0.09763607\n",
      "Iteration 259, loss = 0.09756156\n",
      "Iteration 260, loss = 0.09752140\n",
      "Iteration 261, loss = 0.09744360\n",
      "Iteration 262, loss = 0.09736803\n",
      "Iteration 263, loss = 0.09729106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.html\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(14,)], # [(14,),(100,),(14,4),(7,7,)] \n",
    "    'mlp__activation' : ['relu'], #['logistic','tanh','relu'] \n",
    "    'mlp__solver' : ['sgd'],\n",
    "    'mlp__alpha' : [0.1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1 ,*0.1\n",
    "    'mlp__batch_size' : [99], # ~297 samples for train at inner cv -> 3 equal size batches , #[16,32,64,128,'auto']\n",
    "    'mlp__learning_rate' : ['constant','adaptive'], # [*'constant','invscaling','adaptive']\n",
    "    'mlp__learning_rate_init' : [0.01],\n",
    "    'mlp__power_t' : [0.5],\n",
    "    'mlp__momentum' : [0.8], # np.arange(0.1,1,0.1), *0.8\n",
    "    'mlp__max_iter' : [1000],\n",
    "    'mlp__verbose' : [1]\n",
    "}\n",
    "\n",
    "search_sgd = RandomizedSearchCV(clf_mlp, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=100)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.968     0.991     0.979       212\n",
      "   Malignant      0.990     0.967     0.979       212\n",
      "\n",
      "    accuracy                          0.979       424\n",
      "   macro avg      0.979     0.979     0.979       424\n",
      "weighted avg      0.979     0.979     0.979       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.968     0.991     0.979       212\n",
      "   Malignant      0.990     0.967     0.979       212\n",
      "\n",
      "    accuracy                          0.979       424\n",
      "   macro avg      0.979     0.979     0.979       424\n",
      "weighted avg      0.979     0.979     0.979       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_mlp_sgd =  Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                ('mlp',MLPClassifier(solver='sgd',alpha=0.1, batch_size=99,\n",
    "                               hidden_layer_sizes=(14,),learning_rate='constant',\n",
    "                               learning_rate_init=0.01, max_iter=1000,momentum=0.8,\n",
    "                               random_state=13, verbose=0))])\n",
    "\n",
    "score = cross_val_score(clf_mlp_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Solver : LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.html\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(14,)],  # [(14,),(100,),(14,4),(7,7,)] \n",
    "    'mlp__activation' : ['relu'],\n",
    "    'mlp__solver' : ['lbfgs'],\n",
    "    'mlp__alpha' : [1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1\n",
    "    'mlp__max_iter' : np.arange(300,500,10) , # np.arange(100,300,10) , [100,200,500,1000]\n",
    "}\n",
    "\n",
    "search_lbfgs = RandomizedSearchCV(clf_mlp, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=100)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_lbfgs, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.972     0.976     0.974       212\n",
      "   Malignant      0.976     0.972     0.974       212\n",
      "\n",
      "    accuracy                          0.974       424\n",
      "   macro avg      0.974     0.974     0.974       424\n",
      "weighted avg      0.974     0.974     0.974       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16'></a>\n",
    "## 16) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below are the tables of the specific feature selection method.\n",
    "* The performance of the algorithms is in descending order.\n",
    "* All the results are the average values of a 10-fold cross validation.\n",
    "* The columns contain the accuracy and the average values of precision, recall and f1 score.\n",
    "* It is observed that the number of samples of Βenign and Μalignant cancer are equal (212 respectively), so the weighted average and the macro average are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"6\"> Selected Features : Default algorithms</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>precision </th>\n",
    "        <th>recall</th>\n",
    "        <th>f1 score</th>\n",
    "        <th>accuracy</th>\n",
    "        <th>Execution Time<br>\n",
    "        (seconds)</th>  \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MLP</th>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>3.570</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>AdaBoost</th>\n",
    "        <td>0.974</td>\n",
    "        <td>0.974</td>\n",
    "        <td>0.974</td>\n",
    "        <td>0.974</td>\n",
    "        <td>0.750</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SVC</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.075</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SGD</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.064</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ridge</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.073</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LDA</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.071</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Random Forest</th>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>1.138</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LGBM</th>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.471</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>XGBoost</th>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.565</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>QDA</th>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.063</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>KNN</th>\n",
    "        <td>0.944</td>\n",
    "        <td>0.943</td>\n",
    "        <td>0.943</td>\n",
    "        <td>0.943</td>\n",
    "        <td>0.081</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>GNB</th>\n",
    "        <td>0.935</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.052</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Decision Tree</th>\n",
    "        <td>0.915</td>\n",
    "        <td>0.915</td>\n",
    "        <td>0.915</td>\n",
    "        <td>0.915</td>\n",
    "        <td>0.058</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"6\"> Selected Features : Tuned algorithms</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>precision </th>\n",
    "        <th>recall</th>\n",
    "        <th>f1 score</th>\n",
    "        <th>accuracy</th>\n",
    "        <th>Execution Time<br>\n",
    "        (seconds)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MLP</th>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>trial and error</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SGD</th>\n",
    "        <td>0.970</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>3.455</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SVC</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>6.842</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LGBM</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>86.751</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>XGBoost</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>225.429</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LDA</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>6.350</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ridge</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>27.177</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Random Forest</th>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>153.843</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>AdaBoost</th>\n",
    "        <td>0.958</td>\n",
    "        <td>0.958</td>\n",
    "        <td>0.958</td>\n",
    "        <td>0.958</td>\n",
    "        <td>162.774</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>KNN</th>\n",
    "        <td>0.956</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>80.449</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>QDA</th>\n",
    "        <td>0.947</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>24.638</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Decision Tree</th>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>42.595</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>GNB</th>\n",
    "        <td>0.930</td>\n",
    "        <td>0.927</td>\n",
    "        <td>0.927</td>\n",
    "        <td>0.927</td>\n",
    "        <td>6.874</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As it is seen, some models perform better with default parameters. This can happen for various reasons such as:\n",
    "    - Unlucky selection of hyperparameters from random search\n",
    "    - Hyperparameters selected cause overfitting\n",
    "    - Smaller training sample in the inner loop due to nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Sources for nested cross validation :\n",
    "    1. [Cross-Validation and Hyperparameter Search in scikit-learn - A Complete Guide](<https://dev.to/balapriya/cross-validation-and-hyperparameter-search-in-scikit-learn-a-complete-guide-5ed8>)\n",
    "    2. [Nested Cross Validation for Algorithm Selection](<https://vitalflux.com/python-nested-cross-validation-algorithm-selection/>)\n",
    "    3. [Nested Cross-Validation for Machine Learning with Python](<https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/>)\n",
    "    4. [Nested cross validation for model selection](<https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection/65158#65158>)\n",
    "    5. [scikit-learn GridSearchCV with multiple repetitions](<https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764>)\n",
    "    6. [Model selection done right: A gentle introduction to nested cross-validation](<https://ploomber.io/blog/nested-cv/>)\n",
    "    7. [How to obtain optimal hyperparameters after nested cross validation?](<https://stats.stackexchange.com/questions/254612/how-to-obtain-optimal-hyperparameters-after-nested-cross-validation>)\n",
    "    8. [Cross-validation for parameter tuning, model selection, and feature selection](<https://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb>)\n",
    "- Sources for Hyper Parameter-Optimization :\n",
    "    1. [Random Search for Hyper-Parameter Optimization](<https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>)\n",
    "    2. [Hyperparameter tuning for machine learning models](<https://www.jeremyjordan.me/hyperparameter-tuning/>)\n",
    "- Sources for code :\n",
    "    - All sources are in comments at each code part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
