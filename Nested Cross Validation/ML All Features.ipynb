{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Διπλωματική Εργασία\n",
    "## Ταξινόμηση του καρκίνου του μαστού με μεθόδους μηχανικής μάθησης\n",
    "### Εξαγωγή χαρακτηριστικών με PCA\n",
    "\n",
    "> Λάζαρος Πανιτσίδης<br />\n",
    "> Τμήμα Μηχανικών Παραγωγής και Διοίκησης <br />\n",
    "> Διεθνές Πανεπιστήμιο της Ελλάδος <br />\n",
    "> lazarospanitsidis@outlook.com -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diploma thesis\n",
    "## Breast cancer classification using machine learning methods\n",
    "### All the features of the data set\n",
    "\n",
    "> Lazaros Panitsidis<br />\n",
    "> Department of Industrial Engineering and Management <br />\n",
    "> International Hellenic University <br />\n",
    "> lazarospanitsidis@outlook.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Useful Python Libraries](#1)\n",
    "1. [Data Processing](#2)\n",
    "1. [Gaussian Naive Bayes](#3)\n",
    "1. [Linear Discriminant Analysis](#4)\n",
    "1. [Quadratic Discriminant Analysis](#5)\n",
    "1. [Ridge Classifier](#6)\n",
    "1. [Decision Tree Classifier](#7)\n",
    "1. [Random Forest Classifier](#8)\n",
    "1. [ADA Boost Classifier (Adaptive Boosting)](#9)\n",
    "1. [C-Support Vector Classification](#10)\n",
    "1. [Stochastic Gradient Descent Classifier](#11)\n",
    "1. [eXtreme Gradient Boosting](#12)\n",
    "1. [Light Gradient Boosting Machine](#13)\n",
    "1. [K-Nearest Neighbors Classifier](#14)\n",
    "1. [Multi-layer Perceptron Classifier](#15)\n",
    "1. [Summary](#16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1) Useful Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#import warnings library\n",
    "import warnings\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# some of them are not used in this file\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE, RFECV , mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score , GridSearchCV , LeaveOneOut,KFold,RandomizedSearchCV,StratifiedKFold, HalvingGridSearchCV\n",
    "from skopt import BayesSearchCV # https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV , https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score , make_scorer , classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline , Pipeline # https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder , MinMaxScaler\n",
    "from xgboost import XGBClassifier , plot_importance\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier , RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import lightgbm as lgbm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pygad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 424 cases in this dataset\n",
      "There are 30 features in this dataset\n",
      "There are 212 cases diagnosed as malignant tumor\n",
      "There are 212 cases diagnosed as benign tumor\n",
      "The percentage of malignant cases is: 50.00%\n"
     ]
    }
   ],
   "source": [
    "dataWISC = pd.read_csv('dataWisc.csv')\n",
    "dataWISC.drop([\"id\", \"Unnamed: 32\"], axis = 1, inplace = True)\n",
    "\n",
    "# Undersampling function\n",
    "def make_undersample(_df, column):\n",
    "  dfs_r = {}\n",
    "  dfs_c = {}\n",
    "  smaller = 1e1000\n",
    "  ignore = \"\"\n",
    "  for c in _df[column].unique():\n",
    "    dfs_c[c] = _df[_df[column] == c]\n",
    "    if dfs_c[c].shape[0] < smaller:\n",
    "      smaller = dfs_c[c].shape[0]\n",
    "      ignore = c\n",
    "\n",
    "  for c in dfs_c:\n",
    "    if c == ignore:\n",
    "      continue\n",
    "    dfs_r[c] = resample(dfs_c[c], \n",
    "                        replace=False, # sample without replacement\n",
    "                        n_samples=smaller,\n",
    "                        random_state=0)\n",
    "  return pd.concat([dfs_r[c] for c in dfs_r] + [dfs_c[ignore]])\n",
    "\n",
    "dataWISC = make_undersample(dataWISC,'diagnosis')\n",
    "\n",
    "#Description of the dataset\n",
    "\n",
    "#how many cases are included in the dataset\n",
    "length = len(dataWISC)\n",
    "#how many features are in the dataset\n",
    "features = dataWISC.shape[1]-1 # - diagnosis\n",
    "\n",
    "# Number of malignant cases\n",
    "malignant = len(dataWISC[dataWISC['diagnosis']=='M'])\n",
    "\n",
    "#Number of benign cases\n",
    "benign = len(dataWISC[dataWISC['diagnosis']=='B'])\n",
    "\n",
    "#Rate of malignant tumors over all cases\n",
    "rate = (float(malignant)/(length))*100\n",
    "\n",
    "print (\"There are \"+ str(len(dataWISC))+\" cases in this dataset\")\n",
    "print (\"There are {}\".format(features)+\" features in this dataset\")\n",
    "print (\"There are {}\".format(malignant)+\" cases diagnosed as malignant tumor\")\n",
    "print (\"There are {}\".format(benign)+\" cases diagnosed as benign tumor\")\n",
    "print (\"The percentage of malignant cases is: {:.2f}%\".format(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataWISC.diagnosis                          # M or B \n",
    "x = dataWISC.drop('diagnosis',axis = 1 )\n",
    "target_names=['Benign','Malignant']\n",
    "le= LabelEncoder()\n",
    "le.fit(y)\n",
    "y_le = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13.49</td>\n",
       "      <td>22.30</td>\n",
       "      <td>86.91</td>\n",
       "      <td>561.0</td>\n",
       "      <td>0.08752</td>\n",
       "      <td>0.07698</td>\n",
       "      <td>0.04751</td>\n",
       "      <td>0.033840</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05718</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>1.353</td>\n",
       "      <td>1.735</td>\n",
       "      <td>20.20</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.02095</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.01641</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>15.15</td>\n",
       "      <td>31.82</td>\n",
       "      <td>99.00</td>\n",
       "      <td>698.8</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>0.17110</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.06917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>12.58</td>\n",
       "      <td>18.40</td>\n",
       "      <td>79.83</td>\n",
       "      <td>489.0</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05855</td>\n",
       "      <td>0.2719</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.721</td>\n",
       "      <td>22.45</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.02571</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>13.50</td>\n",
       "      <td>23.08</td>\n",
       "      <td>85.56</td>\n",
       "      <td>564.1</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.06624</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.06431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14.87</td>\n",
       "      <td>20.21</td>\n",
       "      <td>96.12</td>\n",
       "      <td>680.9</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.08345</td>\n",
       "      <td>0.06824</td>\n",
       "      <td>0.049510</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.05748</td>\n",
       "      <td>0.2323</td>\n",
       "      <td>1.636</td>\n",
       "      <td>1.596</td>\n",
       "      <td>21.84</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.02153</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.01959</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>16.01</td>\n",
       "      <td>28.48</td>\n",
       "      <td>103.90</td>\n",
       "      <td>783.6</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.13880</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "49         13.49         22.30           86.91      561.0          0.08752   \n",
       "285        12.58         18.40           79.83      489.0          0.08393   \n",
       "495        14.87         20.21           96.12      680.9          0.09587   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "49            0.07698         0.04751             0.033840         0.1809   \n",
       "285           0.04216         0.00186             0.002924         0.1697   \n",
       "495           0.08345         0.06824             0.049510         0.1487   \n",
       "\n",
       "     fractal_dimension_mean  radius_se  texture_se  perimeter_se  area_se  \\\n",
       "49                  0.05718     0.2338       1.353         1.735    20.20   \n",
       "285                 0.05855     0.2719       1.350         1.721    22.45   \n",
       "495                 0.05748     0.2323       1.636         1.596    21.84   \n",
       "\n",
       "     smoothness_se  compactness_se  concavity_se  concave points_se  \\\n",
       "49        0.004455        0.013820       0.02095           0.011840   \n",
       "285       0.006383        0.008008       0.00186           0.002924   \n",
       "495       0.005415        0.013710       0.02153           0.011830   \n",
       "\n",
       "     symmetry_se  fractal_dimension_se  radius_worst  texture_worst  \\\n",
       "49       0.01641              0.001956         15.15          31.82   \n",
       "285      0.02571              0.002015         13.50          23.08   \n",
       "495      0.01959              0.001812         16.01          28.48   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "49             99.00       698.8            0.1162            0.17110   \n",
       "285            85.56       564.1            0.1038            0.06624   \n",
       "495           103.90       783.6            0.1216            0.13880   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "49          0.228200              0.128200          0.2871   \n",
       "285         0.005579              0.008772          0.2505   \n",
       "495         0.170000              0.101700          0.2369   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "49                   0.06917  \n",
       "285                  0.06431  \n",
       "495                  0.06599  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALL features\n",
    "x_new = x\n",
    "x_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/#:~:text=Given%20the%20improved%20estimate%20of,biased%20estimates%20of%20model%20performance.\n",
    "# cv = LeaveOneOut()\n",
    "rng = np.random.RandomState(13) # random number generator , use it in every random state if shuffle=True for different results.Usefull to test a specific algorithm multiple times within a for loop.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "search_cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "  originalclass.extend(y_true)\n",
    "  predictedclass.extend(y_pred)\n",
    "  #print(classification_report(y_true, y_pred, target_names=target_names)) \n",
    "  return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def print_best_params(search):\n",
    "    print(\"\")\n",
    "    print(\"Best hyperparameters : \", search.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best estimator : \", search.best_estimator_)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method didn't work so it will not be used (nested cross validation which returns the best parameters and their scores)\n",
    "\n",
    "# Following kf is the outer loop\n",
    "outer_kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=13)\n",
    "inner_kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=13)\n",
    "# model = SVC()\n",
    "# params = {'kernel':['rbf','linear'],'C':[1,10]}\n",
    "def nested_cv_with_gscv(model,params,x,y):\n",
    "    outer_loop_accuracy_scores = []\n",
    "    inner_loop_won_params = []\n",
    "    inner_loop_accuracy_scores = []\n",
    "\n",
    "    # Looping through the outer loop, feeding each training set into a GSCV as the inner loop\n",
    "    for train_index,test_index in outer_kf.split(x,y):\n",
    "        \n",
    "        GSCV = GridSearchCV(estimator=model,param_grid=params,cv=inner_kf)\n",
    "        \n",
    "        # GSCV is looping through the training data to find the best parameters. This is the inner loop\n",
    "        GSCV.fit(x[train_index],y[train_index])\n",
    "        \n",
    "        # The best hyper parameters from GSCV is now being tested on the unseen outer loop test data.\n",
    "        pred = GSCV.predict(x[test_index])\n",
    "        \n",
    "        # Appending the \"winning\" hyper parameters and their associated accuracy score\n",
    "        inner_loop_won_params.append(GSCV.best_params_)\n",
    "        outer_loop_accuracy_scores.append(accuracy_score(y[test_index],pred))\n",
    "        inner_loop_accuracy_scores.append(GSCV.best_score_)\n",
    "\n",
    "    for i in zip(inner_loop_won_params,outer_loop_accuracy_scores,inner_loop_accuracy_scores):\n",
    "        print (i)\n",
    "\n",
    "    print('Mean of outer loop accuracy score:',np.mean(outer_loop_accuracy_scores))\n",
    "\n",
    "# https://github.com/rosscleung/Projects/blob/b9abc20db545d9f483e90a9b046ea50c74f25718/Tutorial%20notebooks/Nested%20Cross%20Validation%20Example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The process by which the best model will be selected is as follows:\n",
    "1. Evaluation of the default algorithm with 10-fold cross validation\n",
    "2. Evaluation of the tuned hyperparameter algorithm with nested cross-validation (5-fold Grid Search/Randomized Search inside a 10-fold cross validation)\n",
    "3. Choosing the best model (from steps 1 and 2) and finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3) [Gaussian Naive Bayes](<https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.911     0.962     0.936       212\n",
      "   Malignant      0.960     0.906     0.932       212\n",
      "\n",
      "    accuracy                          0.934       424\n",
      "   macro avg      0.935     0.934     0.934       424\n",
      "weighted avg      0.935     0.934     0.934       424\n",
      "\n",
      "--- Time of execution : 0.06382894515991211 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "# Cross validate\n",
    "clf_gnb = Pipeline([('scaler', StandardScaler()), ('gnb', GaussianNB())])\n",
    "score = cross_val_score(clf_gnb, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.903     0.962     0.932       212\n",
      "   Malignant      0.960     0.896     0.927       212\n",
      "\n",
      "    accuracy                          0.929       424\n",
      "   macro avg      0.931     0.929     0.929       424\n",
      "weighted avg      0.931     0.929     0.929       424\n",
      "\n",
      "--- Time of execution : 9.78564715385437 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = { 'gnb__var_smoothing': np.logspace(0,-10, num=100) }\n",
    "\n",
    "search = GridSearchCV(clf_gnb, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4) [Linear Discriminant Analysis](<https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.933     0.991     0.961       212\n",
      "   Malignant      0.990     0.929     0.959       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.962     0.960     0.960       424\n",
      "weighted avg      0.962     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 0.20174145698547363 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_lda = Pipeline([('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis())])\n",
    "\n",
    "score = cross_val_score(clf_lda, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.941     0.986     0.963       212\n",
      "   Malignant      0.985     0.939     0.961       212\n",
      "\n",
      "    accuracy                          0.962       424\n",
      "   macro avg      0.963     0.962     0.962       424\n",
      "weighted avg      0.963     0.962     0.962       424\n",
      "\n",
      "--- Time of execution : 7.975156545639038 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = [\n",
    "    {\n",
    "        'lda__solver' : ['lsqr','eigen'],\n",
    "        'lda__shrinkage':[None,'auto']\n",
    "    },\n",
    "    {\n",
    "        'lda__solver' : ['svd'],\n",
    "        'lda__tol': np.linspace(0, 0.01, num=100)\n",
    "    }\n",
    "]\n",
    "\n",
    "search = RandomizedSearchCV(clf_lda, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5) [Quadratic Discriminant Analysis](<https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.961     0.939     0.950       212\n",
      "   Malignant      0.940     0.962     0.951       212\n",
      "\n",
      "    accuracy                          0.950       424\n",
      "   macro avg      0.951     0.950     0.950       424\n",
      "weighted avg      0.951     0.950     0.950       424\n",
      "\n",
      "--- Time of execution : 0.13062047958374023 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_qda = Pipeline([('scaler', StandardScaler()), ('qda', QuadraticDiscriminantAnalysis())])\n",
    "\n",
    "score = cross_val_score(clf_qda, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.942     0.991     0.966       212\n",
      "   Malignant      0.990     0.939     0.964       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.966     0.965     0.965       424\n",
      "weighted avg      0.966     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 32.733346700668335 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'qda__reg_param': np.linspace(0, 1, num=100),\n",
    "    'qda__tol': np.linspace(0, 0.01, num=100)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_qda, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6) [Ridge Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.942     0.991     0.966       212\n",
      "   Malignant      0.990     0.939     0.964       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.966     0.965     0.965       424\n",
      "weighted avg      0.966     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 0.11970901489257812 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_rc = Pipeline([('scaler', StandardScaler()), ('rg', RidgeClassifier())])\n",
    "\n",
    "score = cross_val_score(clf_rc, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.938     0.991     0.963       212\n",
      "   Malignant      0.990     0.934     0.961       212\n",
      "\n",
      "    accuracy                          0.962       424\n",
      "   macro avg      0.964     0.962     0.962       424\n",
      "weighted avg      0.964     0.962     0.962       424\n",
      "\n",
      "--- Time of execution : 72.65345358848572 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'rg__alpha' : np.linspace(0, 1, num=10),\n",
    "    'rg__fit_intercept' : [True,False],\n",
    "    'rg__copy_X' : [True,False],\n",
    "    'rg__max_iter' : [None],\n",
    "    'rg__tol' : [0.001],\n",
    "    'rg__class_weight' : [None,'balanced'],\n",
    "    'rg__solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'rg__positive' : [False]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_rc, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7) [Decision Tree Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.916     0.925     0.920       212\n",
      "   Malignant      0.924     0.915     0.919       212\n",
      "\n",
      "    accuracy                          0.920       424\n",
      "   macro avg      0.920     0.920     0.920       424\n",
      "weighted avg      0.920     0.920     0.920       424\n",
      "\n",
      "--- Time of execution : 0.15259170532226562 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_tree = Pipeline([('scaler', StandardScaler()), ('tree', DecisionTreeClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_tree, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.925     0.929     0.927       212\n",
      "   Malignant      0.929     0.925     0.927       212\n",
      "\n",
      "    accuracy                          0.927       424\n",
      "   macro avg      0.927     0.927     0.927       424\n",
      "weighted avg      0.927     0.927     0.927       424\n",
      "\n",
      "--- Time of execution : 66.67729759216309 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'tree__criterion' :['gini','entropy'],\n",
    "    'tree__splitter' : ['best','random'],\n",
    "    'tree__max_depth': [list(range(2, 20)),None],\n",
    "    'tree__min_samples_split': list(range(2, 6)),\n",
    "    'tree__min_samples_leaf': list(range(1, 8)),\n",
    "    'tree__min_weight_fraction_leaf' : [0.0],\n",
    "    'tree__max_features': [None, 'sqrt', 'log2'],\n",
    "    'tree__max_leaf_nodes' : [None],\n",
    "    'tree__min_impurity_decrease' : [0.0],\n",
    "    'tree__class_weight' : [None,'balanced'],\n",
    "    'tree__ccp_alpha' : [0.0],\n",
    "    'tree__random_state' : [13]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_tree, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=1000)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters :  {'tree__splitter': 'random', 'tree__random_state': 13, 'tree__min_weight_fraction_leaf': 0.0, 'tree__min_samples_split': 5, 'tree__min_samples_leaf': 1, 'tree__min_impurity_decrease': 0.0, 'tree__max_leaf_nodes': None, 'tree__max_features': None, 'tree__max_depth': None, 'tree__criterion': 'gini', 'tree__class_weight': 'balanced', 'tree__ccp_alpha': 0.0}\n",
      "\n",
      "Best estimator :  Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('tree',\n",
      "                 DecisionTreeClassifier(class_weight='balanced',\n",
      "                                        min_samples_split=5, random_state=13,\n",
      "                                        splitter='random'))])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tree__splitter</th>\n",
       "      <th>param_tree__random_state</th>\n",
       "      <th>param_tree__min_weight_fraction_leaf</th>\n",
       "      <th>param_tree__min_samples_split</th>\n",
       "      <th>param_tree__min_samples_leaf</th>\n",
       "      <th>param_tree__min_impurity_decrease</th>\n",
       "      <th>param_tree__max_leaf_nodes</th>\n",
       "      <th>param_tree__max_features</th>\n",
       "      <th>param_tree__max_depth</th>\n",
       "      <th>param_tree__criterion</th>\n",
       "      <th>param_tree__class_weight</th>\n",
       "      <th>param_tree__ccp_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>1.681013e-03</td>\n",
       "      <td>random</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'random', 'tree__random_sta...</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>0.941046</td>\n",
       "      <td>0.952882</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.976177</td>\n",
       "      <td>0.950488</td>\n",
       "      <td>0.015527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>2.336015e-07</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>log2</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.941144</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>0.964281</td>\n",
       "      <td>0.948124</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>3.989949e-04</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>log2</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.941144</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>0.952935</td>\n",
       "      <td>0.964281</td>\n",
       "      <td>0.948124</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "394       0.007580      0.003253         0.003094    1.681013e-03   \n",
       "267       0.005288      0.000872         0.001995    2.336015e-07   \n",
       "158       0.004987      0.001263         0.002193    3.989949e-04   \n",
       "\n",
       "    param_tree__splitter param_tree__random_state  \\\n",
       "394               random                       13   \n",
       "267                 best                       13   \n",
       "158                 best                       13   \n",
       "\n",
       "    param_tree__min_weight_fraction_leaf param_tree__min_samples_split  \\\n",
       "394                                  0.0                             5   \n",
       "267                                  0.0                             5   \n",
       "158                                  0.0                             3   \n",
       "\n",
       "    param_tree__min_samples_leaf param_tree__min_impurity_decrease  \\\n",
       "394                            1                               0.0   \n",
       "267                            5                               0.0   \n",
       "158                            5                               0.0   \n",
       "\n",
       "    param_tree__max_leaf_nodes param_tree__max_features param_tree__max_depth  \\\n",
       "394                       None                     None                  None   \n",
       "267                       None                     log2                  None   \n",
       "158                       None                     log2                  None   \n",
       "\n",
       "    param_tree__criterion param_tree__class_weight param_tree__ccp_alpha  \\\n",
       "394                  gini                 balanced                   0.0   \n",
       "267               entropy                 balanced                   0.0   \n",
       "158               entropy                 balanced                   0.0   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "394  {'tree__splitter': 'random', 'tree__random_sta...           0.952935   \n",
       "267  {'tree__splitter': 'best', 'tree__random_state...           0.929324   \n",
       "158  {'tree__splitter': 'best', 'tree__random_state...           0.929324   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "394           0.941046           0.952882           0.929402   \n",
       "267           0.941144           0.952935           0.952935   \n",
       "158           0.941144           0.952935           0.952935   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "394           0.976177         0.950488        0.015527                1  \n",
       "267           0.964281         0.948124        0.011912                2  \n",
       "158           0.964281         0.948124        0.011912                2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(x_new, y) # we need this for adaboost\n",
    "\n",
    "print_best_params(search)\n",
    "search_results = pd.DataFrame(search.cv_results_)\n",
    "search_results.sort_values(by='mean_test_score',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8) [Random Forest Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.967     0.967     0.967       212\n",
      "   Malignant      0.967     0.967     0.967       212\n",
      "\n",
      "    accuracy                          0.967       424\n",
      "   macro avg      0.967     0.967     0.967       424\n",
      "weighted avg      0.967     0.967     0.967       424\n",
      "\n",
      "--- Time of execution : 2.058872938156128 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_rf = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=13))])\n",
    "                       \n",
    "score = cross_val_score(clf_rf, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'rf__bootstrap': [True,False],\n",
    "    'rf__max_depth': [list(range(5,15)), None],\n",
    "    'rf__n_estimators' :[100],\n",
    "    'rf__max_features': [None, 'sqrt', 'log2'],\n",
    "    'rf__max_leaf_nodes' : [None,list(range(5,15))],\n",
    "    'rf__min_samples_leaf': list(range(1,10)),\n",
    "    'rf__min_samples_split': list(range(2, 6)),\n",
    "    'rf__criterion' :['entropy','gini'],\n",
    "    'rf__random_state' : [13]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_rf, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.967     0.962     0.965       212\n",
      "   Malignant      0.962     0.967     0.965       212\n",
      "\n",
      "    accuracy                          0.965       424\n",
      "   macro avg      0.965     0.965     0.965       424\n",
      "weighted avg      0.965     0.965     0.965       424\n",
      "\n",
      "--- Time of execution : 234.31785249710083 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9) [ADA Boost Classifier (Adaptive Boosting)](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#:~:text=An%20AdaBoost%20%5B1%5D%20classifier%20is,focus%20more%20on%20difficult%20cases.>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.958     0.976     0.967       212\n",
      "   Malignant      0.976     0.958     0.967       212\n",
      "\n",
      "    accuracy                          0.967       424\n",
      "   macro avg      0.967     0.967     0.967       424\n",
      "weighted avg      0.967     0.967     0.967       424\n",
      "\n",
      "--- Time of execution : 1.2473196983337402 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_adaboost = Pipeline([('scaler', StandardScaler()), ('adab', AdaBoostClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_adaboost, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.958     0.967     0.962       212\n",
      "   Malignant      0.967     0.958     0.962       212\n",
      "\n",
      "    accuracy                          0.962       424\n",
      "   macro avg      0.962     0.962     0.962       424\n",
      "weighted avg      0.962     0.962     0.962       424\n",
      "\n",
      "--- Time of execution : 65.29903888702393 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'adab__base_estimator' : [DecisionTreeClassifier(max_features='sqrt',min_samples_split=5,random_state=13)],\n",
    "    'adab__n_estimators' : np.arange(100,210,10),\n",
    "    'adab__learning_rate' : np.power(10, np.arange(-3, 1, dtype=float)),\n",
    "    'adab__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "    'adab__random_state' : [13],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_adaboost, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10) [C-Support Vector Classification](<https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.972     0.981     0.977       212\n",
      "   Malignant      0.981     0.972     0.976       212\n",
      "\n",
      "    accuracy                          0.976       424\n",
      "   macro avg      0.976     0.976     0.976       424\n",
      "weighted avg      0.976     0.976     0.976       424\n",
      "\n",
      "--- Time of execution : 0.0937492847442627 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_svc = Pipeline([('scaler', StandardScaler()),('svc', SVC())])\n",
    "\n",
    "score = cross_val_score(clf_svc, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.968     0.986     0.977       212\n",
      "   Malignant      0.986     0.967     0.976       212\n",
      "\n",
      "    accuracy                          0.976       424\n",
      "   macro avg      0.977     0.976     0.976       424\n",
      "weighted avg      0.977     0.976     0.976       424\n",
      "\n",
      "--- Time of execution : 7.132982969284058 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = [\n",
    "    {\n",
    "        'svc__kernel': ['rbf'], \n",
    "        'svc__gamma': [1,1e-1,1e-2, 1e-3, 1e-4,'auto','scale'], \n",
    "        'svc__C': [1, 10, 100, 1000],\n",
    "        'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "        'svc__random_state' : [13]\n",
    "    },\n",
    "    {\n",
    "        'svc__kernel': ['linear'], \n",
    "        'svc__C': [1, 10, 100, 1000],\n",
    "        'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "        'svc__random_state' : [13]\n",
    "    },\n",
    "]\n",
    "\n",
    "search = GridSearchCV(clf_svc, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## 11) [Stochastic Gradient Descent Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.957     0.934     0.945       212\n",
      "   Malignant      0.935     0.958     0.946       212\n",
      "\n",
      "    accuracy                          0.946       424\n",
      "   macro avg      0.946     0.946     0.946       424\n",
      "weighted avg      0.946     0.946     0.946       424\n",
      "\n",
      "--- Time of execution : 0.08232498168945312 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_sgd = Pipeline([('scaler', StandardScaler()), ('sgd', SGDClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.968     0.991     0.979       212\n",
      "   Malignant      0.990     0.967     0.979       212\n",
      "\n",
      "    accuracy                          0.979       424\n",
      "   macro avg      0.979     0.979     0.979       424\n",
      "weighted avg      0.979     0.979     0.979       424\n",
      "\n",
      "--- Time of execution : 4.961320877075195 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'sgd__average': [True, False],\n",
    "    'sgd__l1_ratio': np.linspace(0, 1, num=10),\n",
    "    'sgd__alpha': np.power(10, np.arange(-2, 1, dtype=float)),\n",
    "    'sgd__random_state' : [13]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_sgd, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## 12) [eXtreme Gradient Boosting](<https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.962     0.962       212\n",
      "   Malignant      0.962     0.962     0.962       212\n",
      "\n",
      "    accuracy                          0.962       424\n",
      "   macro avg      0.962     0.962     0.962       424\n",
      "weighted avg      0.962     0.962     0.962       424\n",
      "\n",
      "--- Time of execution : 0.9670112133026123 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_xgb = Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_xgb, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018sp/lectures/lecturenote19.html\n",
    "# https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6\n",
    "\n",
    "start = time.time()\n",
    "param_grid = {\n",
    "        'xgb__booster' : ['gbtree'],\n",
    "        'xgb__validate_parameters' : [True],\n",
    "        'xgb__learning_rate' : np.arange(0.01,1.1,0.05),\n",
    "        'xgb__gamma' : np.arange(0,1.05,0.05),\n",
    "        'xgb__max_depth' : np.arange(2,11,1),\n",
    "        'xgb__min_child_weight' : np.arange(1,6,1),\n",
    "        'xgb__max_delta_step' : np.arange(0,5,1),\n",
    "        'xgb__subsample' : [0.5],\n",
    "        'xgb__colsample_bylevel' : [1],\n",
    "        'xgb__colsample_bynode' : [1],\n",
    "        'xgb__colsample_bytree' : [1],\n",
    "        'xgb__reg_lambda' : [0,1],\n",
    "        'xgb__reg_alpha' : [0],\n",
    "        'xgb__tree_method' : ['exact'],\n",
    "        'xgb__scale_pos_weight' : [1],\n",
    "        'xgb__objective' : ['binary:logistic'], # 'multi:softmax' -> same scores as 'binary:logistic' with grid search\n",
    "        #'num_class' : [2],\n",
    "        'xgb__n_estimators' : np.arange(100,210,10),\n",
    "        'xgb__random_state' : [13]\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf_xgb, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.958     0.967     0.962       212\n",
      "   Malignant      0.967     0.958     0.962       212\n",
      "\n",
      "    accuracy                          0.962       424\n",
      "   macro avg      0.962     0.962     0.962       424\n",
      "weighted avg      0.962     0.962     0.962       424\n",
      "\n",
      "--- Time of execution : 352.8771288394928 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## 13) [Light Gradient Boosting Machine](<https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.963     0.976     0.970       212\n",
      "   Malignant      0.976     0.962     0.969       212\n",
      "\n",
      "    accuracy                          0.969       424\n",
      "   macro avg      0.969     0.969     0.969       424\n",
      "weighted avg      0.969     0.969     0.969       424\n",
      "\n",
      "--- Time of execution : 0.6672425270080566 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_lgbm = Pipeline([('scaler', StandardScaler()), ('lgbm', lgbm.LGBMClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_lgbm, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "# https://www.youtube.com/watch?v=5CWwwtEM2TA&ab_channel=PyData & https://github.com/MSusik/newgradientboosting/blob/master/pydata.pdf\n",
    "\n",
    "start = time.time()\n",
    "param_grid = {\n",
    "        'lgbm__boosting_type' : ['gbdt','dart'],\n",
    "        'lgbm__num_leaves' : np.arange(5,55,1),\n",
    "        'lgbm__max_depth' : np.arange(2,11,1),\n",
    "        'lgbm__learning_rate' : np.arange(0.01,1.1,0.05),\n",
    "        'lgbm__n_estimators' : np.arange(100,210,10),\n",
    "        'lgbm__objective' : ['binary'],\n",
    "        'lgbm__min_child_samples' : np.arange(10,35,5),\n",
    "        'lgbm__subsample' : [0.5],\n",
    "        'lgbm__reg_lambda' : [0,1],\n",
    "        'lgbm__reg_alpha' : [0],\n",
    "        'lgbm__subsample' : [0.5],\n",
    "        'lgbm__colsample_bytree' : [1],\n",
    "        'lgbm__scale_pos_weight' : [1],\n",
    "        'lgbm__random_state' : [13]\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf_lgbm, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.959     0.981     0.970       212\n",
      "   Malignant      0.981     0.958     0.969       212\n",
      "\n",
      "    accuracy                          0.969       424\n",
      "   macro avg      0.970     0.969     0.969       424\n",
      "weighted avg      0.970     0.969     0.969       424\n",
      "\n",
      "--- Time of execution : 157.27623510360718 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## 14) [K-Nearest Neighbors Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.941     0.981     0.961       212\n",
      "   Malignant      0.980     0.939     0.959       212\n",
      "\n",
      "    accuracy                          0.960       424\n",
      "   macro avg      0.961     0.960     0.960       424\n",
      "weighted avg      0.961     0.960     0.960       424\n",
      "\n",
      "--- Time of execution : 0.09286952018737793 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "score = cross_val_score(clf_knn, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.954     0.986     0.970       212\n",
      "   Malignant      0.985     0.953     0.969       212\n",
      "\n",
      "    accuracy                          0.969       424\n",
      "   macro avg      0.970     0.969     0.969       424\n",
      "weighted avg      0.970     0.969     0.969       424\n",
      "\n",
      "--- Time of execution : 119.10199666023254 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': list(range(3,10)),\n",
    "    'knn__weights': ['uniform','distance'],\n",
    "    'knn__algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__leaf_size': [10,20,30,40,50],\n",
    "    'knn__p': [1,2],\n",
    "    'knn__metric': ['minkowski','manhattan','chebyshev']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_knn, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='15'></a>\n",
    "## 15) [Multi-layer Perceptron Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.972     0.972     0.972       212\n",
      "   Malignant      0.972     0.972     0.972       212\n",
      "\n",
      "    accuracy                          0.972       424\n",
      "   macro avg      0.972     0.972     0.972       424\n",
      "weighted avg      0.972     0.972     0.972       424\n",
      "\n",
      "--- Time of execution : 4.62785267829895 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_mlp =  Pipeline([('scaler', StandardScaler()),('mlp', MLPClassifier(shuffle=True,random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_mlp, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tried a wider range of hyperparameters in nested cross validation at first , but over testing, worst attempts were removed (those in comments). Finally, when few hyperparameters remained, they were tested separately with a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solver : ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.40940363\n",
      "Iteration 2, loss = 0.18075386\n",
      "Iteration 3, loss = 0.13222568\n",
      "Iteration 4, loss = 0.11133711\n",
      "Iteration 5, loss = 0.10023307\n",
      "Iteration 6, loss = 0.09270561\n",
      "Iteration 7, loss = 0.08874520\n",
      "Iteration 8, loss = 0.08327712\n",
      "Iteration 9, loss = 0.07921795\n",
      "Iteration 10, loss = 0.07707055\n",
      "Iteration 11, loss = 0.07351700\n",
      "Iteration 12, loss = 0.07056609\n",
      "Iteration 13, loss = 0.06805904\n",
      "Iteration 14, loss = 0.06629799\n",
      "Iteration 15, loss = 0.06383254\n",
      "Iteration 16, loss = 0.06174588\n",
      "Iteration 17, loss = 0.05983752\n",
      "Iteration 18, loss = 0.05802779\n",
      "Iteration 19, loss = 0.05655741\n",
      "Iteration 20, loss = 0.05495882\n",
      "Iteration 21, loss = 0.05326529\n",
      "Iteration 22, loss = 0.05155489\n",
      "Iteration 23, loss = 0.05039840\n",
      "Iteration 24, loss = 0.04936417\n",
      "Iteration 25, loss = 0.04820925\n",
      "Iteration 26, loss = 0.04716927\n",
      "Iteration 27, loss = 0.04611417\n",
      "Iteration 28, loss = 0.04527260\n",
      "Iteration 29, loss = 0.04507739\n",
      "Iteration 30, loss = 0.04337194\n",
      "Iteration 31, loss = 0.04371953\n",
      "Iteration 32, loss = 0.04236541\n",
      "Iteration 33, loss = 0.04149834\n",
      "Iteration 34, loss = 0.04071143\n",
      "Iteration 35, loss = 0.04028948\n",
      "Iteration 36, loss = 0.03972811\n",
      "Iteration 37, loss = 0.03897431\n",
      "Iteration 38, loss = 0.03838054\n",
      "Iteration 39, loss = 0.03830087\n",
      "Iteration 40, loss = 0.03776468\n",
      "Iteration 41, loss = 0.03761372\n",
      "Iteration 42, loss = 0.03680688\n",
      "Iteration 43, loss = 0.03634827\n",
      "Iteration 44, loss = 0.03607753\n",
      "Iteration 45, loss = 0.03585667\n",
      "Iteration 46, loss = 0.03520342\n",
      "Iteration 47, loss = 0.03482235\n",
      "Iteration 48, loss = 0.03470162\n",
      "Iteration 49, loss = 0.03450217\n",
      "Iteration 50, loss = 0.03419959\n",
      "Iteration 51, loss = 0.03370473\n",
      "Iteration 52, loss = 0.03349188\n",
      "Iteration 53, loss = 0.03332955\n",
      "Iteration 54, loss = 0.03316354\n",
      "Iteration 55, loss = 0.03268070\n",
      "Iteration 56, loss = 0.03296249\n",
      "Iteration 57, loss = 0.03230687\n",
      "Iteration 58, loss = 0.03191941\n",
      "Iteration 59, loss = 0.03186193\n",
      "Iteration 60, loss = 0.03173006\n",
      "Iteration 61, loss = 0.03190692\n",
      "Iteration 62, loss = 0.03199792\n",
      "Iteration 63, loss = 0.03098117\n",
      "Iteration 64, loss = 0.03155221\n",
      "Iteration 65, loss = 0.03108353\n",
      "Iteration 66, loss = 0.03121593\n",
      "Iteration 67, loss = 0.03095055\n",
      "Iteration 68, loss = 0.03107267\n",
      "Iteration 69, loss = 0.03116113\n",
      "Iteration 70, loss = 0.03058990\n",
      "Iteration 71, loss = 0.03052078\n",
      "Iteration 72, loss = 0.03044618\n",
      "Iteration 73, loss = 0.02977775\n",
      "Iteration 74, loss = 0.02961919\n",
      "Iteration 75, loss = 0.03005205\n",
      "Iteration 76, loss = 0.02984981\n",
      "Iteration 77, loss = 0.02961702\n",
      "Iteration 78, loss = 0.03037713\n",
      "Iteration 79, loss = 0.03075835\n",
      "Iteration 80, loss = 0.02940044\n",
      "Iteration 81, loss = 0.02947444\n",
      "Iteration 82, loss = 0.02900501\n",
      "Iteration 83, loss = 0.02893002\n",
      "Iteration 84, loss = 0.02900133\n",
      "Iteration 85, loss = 0.02910617\n",
      "Iteration 86, loss = 0.02843440\n",
      "Iteration 87, loss = 0.02878545\n",
      "Iteration 88, loss = 0.02864991\n",
      "Iteration 89, loss = 0.02849402\n",
      "Iteration 90, loss = 0.02842679\n",
      "Iteration 91, loss = 0.02840253\n",
      "Iteration 92, loss = 0.02873820\n",
      "Iteration 93, loss = 0.02794856\n",
      "Iteration 94, loss = 0.02805430\n",
      "Iteration 95, loss = 0.02918254\n",
      "Iteration 96, loss = 0.02792792\n",
      "Iteration 97, loss = 0.02935217\n",
      "Iteration 98, loss = 0.02866754\n",
      "Iteration 99, loss = 0.03126890\n",
      "Iteration 100, loss = 0.02749298\n",
      "Iteration 101, loss = 0.03059514\n",
      "Iteration 102, loss = 0.02878340\n",
      "Iteration 103, loss = 0.02907876\n",
      "Iteration 104, loss = 0.02913602\n",
      "Iteration 105, loss = 0.02848550\n",
      "Iteration 106, loss = 0.02763017\n",
      "Iteration 107, loss = 0.02787650\n",
      "Iteration 108, loss = 0.02846431\n",
      "Iteration 109, loss = 0.02819837\n",
      "Iteration 110, loss = 0.02876149\n",
      "Iteration 111, loss = 0.02805160\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.59716624\n",
      "Iteration 2, loss = 0.49183411\n",
      "Iteration 3, loss = 0.41306280\n",
      "Iteration 4, loss = 0.35660959\n",
      "Iteration 5, loss = 0.31671165\n",
      "Iteration 6, loss = 0.28605530\n",
      "Iteration 7, loss = 0.26206269\n",
      "Iteration 8, loss = 0.24241065\n",
      "Iteration 9, loss = 0.22681613\n",
      "Iteration 10, loss = 0.21338860\n",
      "Iteration 11, loss = 0.20209428\n",
      "Iteration 12, loss = 0.19146899\n",
      "Iteration 13, loss = 0.18328277\n",
      "Iteration 14, loss = 0.17577980\n",
      "Iteration 15, loss = 0.16918836\n",
      "Iteration 16, loss = 0.16318224\n",
      "Iteration 17, loss = 0.15780612\n",
      "Iteration 18, loss = 0.15289921\n",
      "Iteration 19, loss = 0.14870112\n",
      "Iteration 20, loss = 0.14434656\n",
      "Iteration 21, loss = 0.14054216\n",
      "Iteration 22, loss = 0.13714172\n",
      "Iteration 23, loss = 0.13397054\n",
      "Iteration 24, loss = 0.13095046\n",
      "Iteration 25, loss = 0.12811003\n",
      "Iteration 26, loss = 0.12553352\n",
      "Iteration 27, loss = 0.12310070\n",
      "Iteration 28, loss = 0.12085080\n",
      "Iteration 29, loss = 0.11868514\n",
      "Iteration 30, loss = 0.11649461\n",
      "Iteration 31, loss = 0.11454689\n",
      "Iteration 32, loss = 0.11274719\n",
      "Iteration 33, loss = 0.11099457\n",
      "Iteration 34, loss = 0.10940938\n",
      "Iteration 35, loss = 0.10779983\n",
      "Iteration 36, loss = 0.10627943\n",
      "Iteration 37, loss = 0.10486145\n",
      "Iteration 38, loss = 0.10351976\n",
      "Iteration 39, loss = 0.10224487\n",
      "Iteration 40, loss = 0.10097775\n",
      "Iteration 41, loss = 0.09978144\n",
      "Iteration 42, loss = 0.09860053\n",
      "Iteration 43, loss = 0.09757855\n",
      "Iteration 44, loss = 0.09647154\n",
      "Iteration 45, loss = 0.09546983\n",
      "Iteration 46, loss = 0.09442919\n",
      "Iteration 47, loss = 0.09350903\n",
      "Iteration 48, loss = 0.09257200\n",
      "Iteration 49, loss = 0.09171200\n",
      "Iteration 50, loss = 0.09088546\n",
      "Iteration 51, loss = 0.08996460\n",
      "Iteration 52, loss = 0.08912762\n",
      "Iteration 53, loss = 0.08834649\n",
      "Iteration 54, loss = 0.08753499\n",
      "Iteration 55, loss = 0.08678836\n",
      "Iteration 56, loss = 0.08606425\n",
      "Iteration 57, loss = 0.08533286\n",
      "Iteration 58, loss = 0.08461127\n",
      "Iteration 59, loss = 0.08399680\n",
      "Iteration 60, loss = 0.08329616\n",
      "Iteration 61, loss = 0.08271000\n",
      "Iteration 62, loss = 0.08204873\n",
      "Iteration 63, loss = 0.08144048\n",
      "Iteration 64, loss = 0.08083160\n",
      "Iteration 65, loss = 0.08026110\n",
      "Iteration 66, loss = 0.07967657\n",
      "Iteration 67, loss = 0.07914644\n",
      "Iteration 68, loss = 0.07857884\n",
      "Iteration 69, loss = 0.07811671\n",
      "Iteration 70, loss = 0.07753949\n",
      "Iteration 71, loss = 0.07702493\n",
      "Iteration 72, loss = 0.07655892\n",
      "Iteration 73, loss = 0.07604124\n",
      "Iteration 74, loss = 0.07553458\n",
      "Iteration 75, loss = 0.07502657\n",
      "Iteration 76, loss = 0.07453520\n",
      "Iteration 77, loss = 0.07412793\n",
      "Iteration 78, loss = 0.07366033\n",
      "Iteration 79, loss = 0.07335441\n",
      "Iteration 80, loss = 0.07275992\n",
      "Iteration 81, loss = 0.07231673\n",
      "Iteration 82, loss = 0.07191234\n",
      "Iteration 83, loss = 0.07151654\n",
      "Iteration 84, loss = 0.07104032\n",
      "Iteration 85, loss = 0.07064114\n",
      "Iteration 86, loss = 0.07025916\n",
      "Iteration 87, loss = 0.06989100\n",
      "Iteration 88, loss = 0.06944855\n",
      "Iteration 89, loss = 0.06903890\n",
      "Iteration 90, loss = 0.06867608\n",
      "Iteration 91, loss = 0.06833400\n",
      "Iteration 92, loss = 0.06792776\n",
      "Iteration 93, loss = 0.06753652\n",
      "Iteration 94, loss = 0.06719058\n",
      "Iteration 95, loss = 0.06682912\n",
      "Iteration 96, loss = 0.06645346\n",
      "Iteration 97, loss = 0.06614812\n",
      "Iteration 98, loss = 0.06578852\n",
      "Iteration 99, loss = 0.06563404\n",
      "Iteration 100, loss = 0.06504492\n",
      "Iteration 101, loss = 0.06470561\n",
      "Iteration 102, loss = 0.06439792\n",
      "Iteration 103, loss = 0.06411749\n",
      "Iteration 104, loss = 0.06376379\n",
      "Iteration 105, loss = 0.06342093\n",
      "Iteration 106, loss = 0.06316149\n",
      "Iteration 107, loss = 0.06285597\n",
      "Iteration 108, loss = 0.06254792\n",
      "Iteration 109, loss = 0.06216049\n",
      "Iteration 110, loss = 0.06183856\n",
      "Iteration 111, loss = 0.06154140\n",
      "Iteration 112, loss = 0.06129390\n",
      "Iteration 113, loss = 0.06094879\n",
      "Iteration 114, loss = 0.06068388\n",
      "Iteration 115, loss = 0.06034724\n",
      "Iteration 116, loss = 0.06012288\n",
      "Iteration 117, loss = 0.05977156\n",
      "Iteration 118, loss = 0.05954247\n",
      "Iteration 119, loss = 0.05920381\n",
      "Iteration 120, loss = 0.05902634\n",
      "Iteration 121, loss = 0.05867762\n",
      "Iteration 122, loss = 0.05846146\n",
      "Iteration 123, loss = 0.05814743\n",
      "Iteration 124, loss = 0.05790488\n",
      "Iteration 125, loss = 0.05763172\n",
      "Iteration 126, loss = 0.05734843\n",
      "Iteration 127, loss = 0.05715418\n",
      "Iteration 128, loss = 0.05686356\n",
      "Iteration 129, loss = 0.05658168\n",
      "Iteration 130, loss = 0.05637717\n",
      "Iteration 131, loss = 0.05609209\n",
      "Iteration 132, loss = 0.05583411\n",
      "Iteration 133, loss = 0.05562275\n",
      "Iteration 134, loss = 0.05537349\n",
      "Iteration 135, loss = 0.05512146\n",
      "Iteration 136, loss = 0.05491489\n",
      "Iteration 137, loss = 0.05465000\n",
      "Iteration 138, loss = 0.05446448\n",
      "Iteration 139, loss = 0.05430681\n",
      "Iteration 140, loss = 0.05396051\n",
      "Iteration 141, loss = 0.05372962\n",
      "Iteration 142, loss = 0.05351241\n",
      "Iteration 143, loss = 0.05331446\n",
      "Iteration 144, loss = 0.05310081\n",
      "Iteration 145, loss = 0.05284841\n",
      "Iteration 146, loss = 0.05263993\n",
      "Iteration 147, loss = 0.05245146\n",
      "Iteration 148, loss = 0.05225554\n",
      "Iteration 149, loss = 0.05200246\n",
      "Iteration 150, loss = 0.05178526\n",
      "Iteration 151, loss = 0.05159690\n",
      "Iteration 152, loss = 0.05147070\n",
      "Iteration 153, loss = 0.05117981\n",
      "Iteration 154, loss = 0.05099321\n",
      "Iteration 155, loss = 0.05082917\n",
      "Iteration 156, loss = 0.05056717\n",
      "Iteration 157, loss = 0.05040224\n",
      "Iteration 158, loss = 0.05018864\n",
      "Iteration 159, loss = 0.05001293\n",
      "Iteration 160, loss = 0.04978596\n",
      "Iteration 161, loss = 0.04963917\n",
      "Iteration 162, loss = 0.04945384\n",
      "Iteration 163, loss = 0.04928286\n",
      "Iteration 164, loss = 0.04907383\n",
      "Iteration 165, loss = 0.04889840\n",
      "Iteration 166, loss = 0.04873061\n",
      "Iteration 167, loss = 0.04857611\n",
      "Iteration 168, loss = 0.04842189\n",
      "Iteration 169, loss = 0.04819538\n",
      "Iteration 170, loss = 0.04806361\n",
      "Iteration 171, loss = 0.04784338\n",
      "Iteration 172, loss = 0.04768107\n",
      "Iteration 173, loss = 0.04751462\n",
      "Iteration 174, loss = 0.04736852\n",
      "Iteration 175, loss = 0.04717971\n",
      "Iteration 176, loss = 0.04708392\n",
      "Iteration 177, loss = 0.04684714\n",
      "Iteration 178, loss = 0.04670477\n",
      "Iteration 179, loss = 0.04655706\n",
      "Iteration 180, loss = 0.04641950\n",
      "Iteration 181, loss = 0.04624752\n",
      "Iteration 182, loss = 0.04612044\n",
      "Iteration 183, loss = 0.04593179\n",
      "Iteration 184, loss = 0.04580140\n",
      "Iteration 185, loss = 0.04565189\n",
      "Iteration 186, loss = 0.04559302\n",
      "Iteration 187, loss = 0.04536644\n",
      "Iteration 188, loss = 0.04521200\n",
      "Iteration 189, loss = 0.04517255\n",
      "Iteration 190, loss = 0.04496333\n",
      "Iteration 191, loss = 0.04488820\n",
      "Iteration 192, loss = 0.04464643\n",
      "Iteration 193, loss = 0.04451132\n",
      "Iteration 194, loss = 0.04439770\n",
      "Iteration 195, loss = 0.04429728\n",
      "Iteration 196, loss = 0.04419049\n",
      "Iteration 197, loss = 0.04398837\n",
      "Iteration 198, loss = 0.04384551\n",
      "Iteration 199, loss = 0.04371579\n",
      "Iteration 200, loss = 0.04361955\n",
      "Iteration 201, loss = 0.04344991\n",
      "Iteration 202, loss = 0.04332389\n",
      "Iteration 203, loss = 0.04322565\n",
      "Iteration 204, loss = 0.04310540\n",
      "Iteration 205, loss = 0.04301819\n",
      "Iteration 206, loss = 0.04285901\n",
      "Iteration 207, loss = 0.04273742\n",
      "Iteration 208, loss = 0.04265141\n",
      "Iteration 209, loss = 0.04249876\n",
      "Iteration 210, loss = 0.04235962\n",
      "Iteration 211, loss = 0.04229785\n",
      "Iteration 212, loss = 0.04219232\n",
      "Iteration 213, loss = 0.04201181\n",
      "Iteration 214, loss = 0.04188277\n",
      "Iteration 215, loss = 0.04181274\n",
      "Iteration 216, loss = 0.04169647\n",
      "Iteration 217, loss = 0.04157964\n",
      "Iteration 218, loss = 0.04146838\n",
      "Iteration 219, loss = 0.04136330\n",
      "Iteration 220, loss = 0.04121511\n",
      "Iteration 221, loss = 0.04117364\n",
      "Iteration 222, loss = 0.04101919\n",
      "Iteration 223, loss = 0.04090351\n",
      "Iteration 224, loss = 0.04080932\n",
      "Iteration 225, loss = 0.04069734\n",
      "Iteration 226, loss = 0.04060655\n",
      "Iteration 227, loss = 0.04049500\n",
      "Iteration 228, loss = 0.04041520\n",
      "Iteration 229, loss = 0.04029680\n",
      "Iteration 230, loss = 0.04022329\n",
      "Iteration 231, loss = 0.04008992\n",
      "Iteration 232, loss = 0.04003715\n",
      "Iteration 233, loss = 0.03991209\n",
      "Iteration 234, loss = 0.03982481\n",
      "Iteration 235, loss = 0.03973544\n",
      "Iteration 236, loss = 0.03966309\n",
      "Iteration 237, loss = 0.03950372\n",
      "Iteration 238, loss = 0.03941319\n",
      "Iteration 239, loss = 0.03930406\n",
      "Iteration 240, loss = 0.03927966\n",
      "Iteration 241, loss = 0.03917882\n",
      "Iteration 242, loss = 0.03905275\n",
      "Iteration 243, loss = 0.03897350\n",
      "Iteration 244, loss = 0.03888369\n",
      "Iteration 245, loss = 0.03878347\n",
      "Iteration 246, loss = 0.03868471\n",
      "Iteration 247, loss = 0.03861250\n",
      "Iteration 248, loss = 0.03851162\n",
      "Iteration 249, loss = 0.03842566\n",
      "Iteration 250, loss = 0.03834412\n",
      "Iteration 251, loss = 0.03828112\n",
      "Iteration 252, loss = 0.03818279\n",
      "Iteration 253, loss = 0.03810236\n",
      "Iteration 254, loss = 0.03807388\n",
      "Iteration 255, loss = 0.03809719\n",
      "Iteration 256, loss = 0.03787208\n",
      "Iteration 257, loss = 0.03786459\n",
      "Iteration 258, loss = 0.03774275\n",
      "Iteration 259, loss = 0.03764535\n",
      "Iteration 260, loss = 0.03755062\n",
      "Iteration 261, loss = 0.03761510\n",
      "Iteration 262, loss = 0.03738533\n",
      "Iteration 263, loss = 0.03736337\n",
      "Iteration 264, loss = 0.03725747\n",
      "Iteration 265, loss = 0.03717610\n",
      "Iteration 266, loss = 0.03709330\n",
      "Iteration 267, loss = 0.03711070\n",
      "Iteration 268, loss = 0.03694605\n",
      "Iteration 269, loss = 0.03684478\n",
      "Iteration 270, loss = 0.03681376\n",
      "Iteration 271, loss = 0.03672360\n",
      "Iteration 272, loss = 0.03663104\n",
      "Iteration 273, loss = 0.03655908\n",
      "Iteration 274, loss = 0.03654987\n",
      "Iteration 275, loss = 0.03646957\n",
      "Iteration 276, loss = 0.03640364\n",
      "Iteration 277, loss = 0.03632440\n",
      "Iteration 278, loss = 0.03634435\n",
      "Iteration 279, loss = 0.03614667\n",
      "Iteration 280, loss = 0.03610774\n",
      "Iteration 281, loss = 0.03603926\n",
      "Iteration 282, loss = 0.03596818\n",
      "Iteration 283, loss = 0.03590653\n",
      "Iteration 284, loss = 0.03584163\n",
      "Iteration 285, loss = 0.03578179\n",
      "Iteration 286, loss = 0.03569472\n",
      "Iteration 287, loss = 0.03570789\n",
      "Iteration 288, loss = 0.03556339\n",
      "Iteration 289, loss = 0.03550621\n",
      "Iteration 290, loss = 0.03546555\n",
      "Iteration 291, loss = 0.03542677\n",
      "Iteration 292, loss = 0.03535862\n",
      "Iteration 293, loss = 0.03529378\n",
      "Iteration 294, loss = 0.03519935\n",
      "Iteration 295, loss = 0.03511860\n",
      "Iteration 296, loss = 0.03508430\n",
      "Iteration 297, loss = 0.03499031\n",
      "Iteration 298, loss = 0.03494946\n",
      "Iteration 299, loss = 0.03486470\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.41591736\n",
      "Iteration 2, loss = 0.18385766\n",
      "Iteration 3, loss = 0.13098083\n",
      "Iteration 4, loss = 0.10954491\n",
      "Iteration 5, loss = 0.09889692\n",
      "Iteration 6, loss = 0.09356899\n",
      "Iteration 7, loss = 0.08811304\n",
      "Iteration 8, loss = 0.08209295\n",
      "Iteration 9, loss = 0.07834636\n",
      "Iteration 10, loss = 0.07522974\n",
      "Iteration 11, loss = 0.07237722\n",
      "Iteration 12, loss = 0.06965800\n",
      "Iteration 13, loss = 0.06732592\n",
      "Iteration 14, loss = 0.06491814\n",
      "Iteration 15, loss = 0.06304942\n",
      "Iteration 16, loss = 0.06081089\n",
      "Iteration 17, loss = 0.05876872\n",
      "Iteration 18, loss = 0.05710407\n",
      "Iteration 19, loss = 0.05608342\n",
      "Iteration 20, loss = 0.05467460\n",
      "Iteration 21, loss = 0.05261542\n",
      "Iteration 22, loss = 0.05123485\n",
      "Iteration 23, loss = 0.04987728\n",
      "Iteration 24, loss = 0.04929903\n",
      "Iteration 25, loss = 0.04787509\n",
      "Iteration 26, loss = 0.04673326\n",
      "Iteration 27, loss = 0.04544200\n",
      "Iteration 28, loss = 0.04462992\n",
      "Iteration 29, loss = 0.04535528\n",
      "Iteration 30, loss = 0.04274602\n",
      "Iteration 31, loss = 0.04270473\n",
      "Iteration 32, loss = 0.04324579\n",
      "Iteration 33, loss = 0.04110465\n",
      "Iteration 34, loss = 0.04048146\n",
      "Iteration 35, loss = 0.03911467\n",
      "Iteration 36, loss = 0.04036730\n",
      "Iteration 37, loss = 0.03851267\n",
      "Iteration 38, loss = 0.03867939\n",
      "Iteration 39, loss = 0.03750214\n",
      "Iteration 40, loss = 0.03807884\n",
      "Iteration 41, loss = 0.03788913\n",
      "Iteration 42, loss = 0.03678722\n",
      "Iteration 43, loss = 0.03593000\n",
      "Iteration 44, loss = 0.03565286\n",
      "Iteration 45, loss = 0.03460910\n",
      "Iteration 46, loss = 0.03484237\n",
      "Iteration 47, loss = 0.03395067\n",
      "Iteration 48, loss = 0.03422026\n",
      "Iteration 49, loss = 0.03351939\n",
      "Iteration 50, loss = 0.03508598\n",
      "Iteration 51, loss = 0.03258745\n",
      "Iteration 52, loss = 0.03351599\n",
      "Iteration 53, loss = 0.03294632\n",
      "Iteration 54, loss = 0.03214891\n",
      "Iteration 55, loss = 0.03239759\n",
      "Iteration 56, loss = 0.03192941\n",
      "Iteration 57, loss = 0.03232431\n",
      "Iteration 58, loss = 0.03155297\n",
      "Iteration 59, loss = 0.03163562\n",
      "Iteration 60, loss = 0.03110074\n",
      "Iteration 61, loss = 0.03127268\n",
      "Iteration 62, loss = 0.03145463\n",
      "Iteration 63, loss = 0.03024534\n",
      "Iteration 64, loss = 0.03079844\n",
      "Iteration 65, loss = 0.03025834\n",
      "Iteration 66, loss = 0.03042386\n",
      "Iteration 67, loss = 0.02998042\n",
      "Iteration 68, loss = 0.02982478\n",
      "Iteration 69, loss = 0.03003407\n",
      "Iteration 70, loss = 0.02956402\n",
      "Iteration 71, loss = 0.02943768\n",
      "Iteration 72, loss = 0.02935108\n",
      "Iteration 73, loss = 0.02903893\n",
      "Iteration 74, loss = 0.02894429\n",
      "Iteration 75, loss = 0.02942006\n",
      "Iteration 76, loss = 0.02852275\n",
      "Iteration 77, loss = 0.02891879\n",
      "Iteration 78, loss = 0.02896867\n",
      "Iteration 79, loss = 0.02911787\n",
      "Iteration 80, loss = 0.02862330\n",
      "Iteration 81, loss = 0.02857482\n",
      "Iteration 82, loss = 0.02811133\n",
      "Iteration 83, loss = 0.02830405\n",
      "Iteration 84, loss = 0.02865038\n",
      "Iteration 85, loss = 0.02846245\n",
      "Iteration 86, loss = 0.02802987\n",
      "Iteration 87, loss = 0.02844388\n",
      "Iteration 88, loss = 0.02753406\n",
      "Iteration 89, loss = 0.02825567\n",
      "Iteration 90, loss = 0.02772753\n",
      "Iteration 91, loss = 0.02809861\n",
      "Iteration 92, loss = 0.02852299\n",
      "Iteration 93, loss = 0.02768912\n",
      "Iteration 94, loss = 0.02850445\n",
      "Iteration 95, loss = 0.02809412\n",
      "Iteration 96, loss = 0.02768213\n",
      "Iteration 97, loss = 0.02794567\n",
      "Iteration 98, loss = 0.02800390\n",
      "Iteration 99, loss = 0.02924774\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.59697288\n",
      "Iteration 2, loss = 0.48878874\n",
      "Iteration 3, loss = 0.41154880\n",
      "Iteration 4, loss = 0.35318320\n",
      "Iteration 5, loss = 0.31340267\n",
      "Iteration 6, loss = 0.28192705\n",
      "Iteration 7, loss = 0.25828059\n",
      "Iteration 8, loss = 0.23903608\n",
      "Iteration 9, loss = 0.22357168\n",
      "Iteration 10, loss = 0.21027525\n",
      "Iteration 11, loss = 0.19917900\n",
      "Iteration 12, loss = 0.18884819\n",
      "Iteration 13, loss = 0.18102379\n",
      "Iteration 14, loss = 0.17307788\n",
      "Iteration 15, loss = 0.16640921\n",
      "Iteration 16, loss = 0.16052800\n",
      "Iteration 17, loss = 0.15501272\n",
      "Iteration 18, loss = 0.15000248\n",
      "Iteration 19, loss = 0.14541565\n",
      "Iteration 20, loss = 0.14132937\n",
      "Iteration 21, loss = 0.13745983\n",
      "Iteration 22, loss = 0.13395487\n",
      "Iteration 23, loss = 0.13066408\n",
      "Iteration 24, loss = 0.12752179\n",
      "Iteration 25, loss = 0.12452175\n",
      "Iteration 26, loss = 0.12181692\n",
      "Iteration 27, loss = 0.11936404\n",
      "Iteration 28, loss = 0.11691064\n",
      "Iteration 29, loss = 0.11457852\n",
      "Iteration 30, loss = 0.11246539\n",
      "Iteration 31, loss = 0.11042733\n",
      "Iteration 32, loss = 0.10861845\n",
      "Iteration 33, loss = 0.10679705\n",
      "Iteration 34, loss = 0.10513530\n",
      "Iteration 35, loss = 0.10343865\n",
      "Iteration 36, loss = 0.10190133\n",
      "Iteration 37, loss = 0.10043617\n",
      "Iteration 38, loss = 0.09908632\n",
      "Iteration 39, loss = 0.09775347\n",
      "Iteration 40, loss = 0.09646149\n",
      "Iteration 41, loss = 0.09524362\n",
      "Iteration 42, loss = 0.09401305\n",
      "Iteration 43, loss = 0.09297865\n",
      "Iteration 44, loss = 0.09184883\n",
      "Iteration 45, loss = 0.09080532\n",
      "Iteration 46, loss = 0.08973857\n",
      "Iteration 47, loss = 0.08882821\n",
      "Iteration 48, loss = 0.08790234\n",
      "Iteration 49, loss = 0.08702326\n",
      "Iteration 50, loss = 0.08616882\n",
      "Iteration 51, loss = 0.08529689\n",
      "Iteration 52, loss = 0.08448019\n",
      "Iteration 53, loss = 0.08371045\n",
      "Iteration 54, loss = 0.08300320\n",
      "Iteration 55, loss = 0.08227641\n",
      "Iteration 56, loss = 0.08151607\n",
      "Iteration 57, loss = 0.08077368\n",
      "Iteration 58, loss = 0.08008287\n",
      "Iteration 59, loss = 0.07959649\n",
      "Iteration 60, loss = 0.07878915\n",
      "Iteration 61, loss = 0.07831387\n",
      "Iteration 62, loss = 0.07760034\n",
      "Iteration 63, loss = 0.07697723\n",
      "Iteration 64, loss = 0.07641923\n",
      "Iteration 65, loss = 0.07581949\n",
      "Iteration 66, loss = 0.07533619\n",
      "Iteration 67, loss = 0.07481914\n",
      "Iteration 68, loss = 0.07423799\n",
      "Iteration 69, loss = 0.07382967\n",
      "Iteration 70, loss = 0.07325790\n",
      "Iteration 71, loss = 0.07274854\n",
      "Iteration 72, loss = 0.07227579\n",
      "Iteration 73, loss = 0.07177607\n",
      "Iteration 74, loss = 0.07132993\n",
      "Iteration 75, loss = 0.07087891\n",
      "Iteration 76, loss = 0.07038679\n",
      "Iteration 77, loss = 0.07001054\n",
      "Iteration 78, loss = 0.06954967\n",
      "Iteration 79, loss = 0.06917175\n",
      "Iteration 80, loss = 0.06871931\n",
      "Iteration 81, loss = 0.06834466\n",
      "Iteration 82, loss = 0.06789370\n",
      "Iteration 83, loss = 0.06752978\n",
      "Iteration 84, loss = 0.06713872\n",
      "Iteration 85, loss = 0.06675682\n",
      "Iteration 86, loss = 0.06637703\n",
      "Iteration 87, loss = 0.06607411\n",
      "Iteration 88, loss = 0.06561956\n",
      "Iteration 89, loss = 0.06524484\n",
      "Iteration 90, loss = 0.06491675\n",
      "Iteration 91, loss = 0.06457279\n",
      "Iteration 92, loss = 0.06422335\n",
      "Iteration 93, loss = 0.06387849\n",
      "Iteration 94, loss = 0.06353990\n",
      "Iteration 95, loss = 0.06319525\n",
      "Iteration 96, loss = 0.06289988\n",
      "Iteration 97, loss = 0.06270563\n",
      "Iteration 98, loss = 0.06224349\n",
      "Iteration 99, loss = 0.06195216\n",
      "Iteration 100, loss = 0.06158487\n",
      "Iteration 101, loss = 0.06130368\n",
      "Iteration 102, loss = 0.06099665\n",
      "Iteration 103, loss = 0.06069394\n",
      "Iteration 104, loss = 0.06047300\n",
      "Iteration 105, loss = 0.06015982\n",
      "Iteration 106, loss = 0.05979936\n",
      "Iteration 107, loss = 0.05956036\n",
      "Iteration 108, loss = 0.05938796\n",
      "Iteration 109, loss = 0.05892381\n",
      "Iteration 110, loss = 0.05866785\n",
      "Iteration 111, loss = 0.05837708\n",
      "Iteration 112, loss = 0.05811348\n",
      "Iteration 113, loss = 0.05782643\n",
      "Iteration 114, loss = 0.05753443\n",
      "Iteration 115, loss = 0.05726569\n",
      "Iteration 116, loss = 0.05704379\n",
      "Iteration 117, loss = 0.05673204\n",
      "Iteration 118, loss = 0.05652234\n",
      "Iteration 119, loss = 0.05622716\n",
      "Iteration 120, loss = 0.05599688\n",
      "Iteration 121, loss = 0.05569784\n",
      "Iteration 122, loss = 0.05544402\n",
      "Iteration 123, loss = 0.05518760\n",
      "Iteration 124, loss = 0.05497307\n",
      "Iteration 125, loss = 0.05473727\n",
      "Iteration 126, loss = 0.05449024\n",
      "Iteration 127, loss = 0.05426304\n",
      "Iteration 128, loss = 0.05398635\n",
      "Iteration 129, loss = 0.05380836\n",
      "Iteration 130, loss = 0.05355294\n",
      "Iteration 131, loss = 0.05331727\n",
      "Iteration 132, loss = 0.05308221\n",
      "Iteration 133, loss = 0.05288831\n",
      "Iteration 134, loss = 0.05262353\n",
      "Iteration 135, loss = 0.05240651\n",
      "Iteration 136, loss = 0.05215517\n",
      "Iteration 137, loss = 0.05198247\n",
      "Iteration 138, loss = 0.05179923\n",
      "Iteration 139, loss = 0.05158147\n",
      "Iteration 140, loss = 0.05133054\n",
      "Iteration 141, loss = 0.05111853\n",
      "Iteration 142, loss = 0.05093457\n",
      "Iteration 143, loss = 0.05075138\n",
      "Iteration 144, loss = 0.05053240\n",
      "Iteration 145, loss = 0.05035823\n",
      "Iteration 146, loss = 0.05012803\n",
      "Iteration 147, loss = 0.04993481\n",
      "Iteration 148, loss = 0.04975147\n",
      "Iteration 149, loss = 0.04953218\n",
      "Iteration 150, loss = 0.04936713\n",
      "Iteration 151, loss = 0.04916416\n",
      "Iteration 152, loss = 0.04911439\n",
      "Iteration 153, loss = 0.04881559\n",
      "Iteration 154, loss = 0.04863287\n",
      "Iteration 155, loss = 0.04843702\n",
      "Iteration 156, loss = 0.04825170\n",
      "Iteration 157, loss = 0.04809710\n",
      "Iteration 158, loss = 0.04795528\n",
      "Iteration 159, loss = 0.04777285\n",
      "Iteration 160, loss = 0.04754720\n",
      "Iteration 161, loss = 0.04744372\n",
      "Iteration 162, loss = 0.04722181\n",
      "Iteration 163, loss = 0.04705236\n",
      "Iteration 164, loss = 0.04687705\n",
      "Iteration 165, loss = 0.04670768\n",
      "Iteration 166, loss = 0.04657167\n",
      "Iteration 167, loss = 0.04637410\n",
      "Iteration 168, loss = 0.04623782\n",
      "Iteration 169, loss = 0.04605684\n",
      "Iteration 170, loss = 0.04601023\n",
      "Iteration 171, loss = 0.04573830\n",
      "Iteration 172, loss = 0.04560430\n",
      "Iteration 173, loss = 0.04547728\n",
      "Iteration 174, loss = 0.04532423\n",
      "Iteration 175, loss = 0.04517566\n",
      "Iteration 176, loss = 0.04498681\n",
      "Iteration 177, loss = 0.04484128\n",
      "Iteration 178, loss = 0.04468377\n",
      "Iteration 179, loss = 0.04458542\n",
      "Iteration 180, loss = 0.04440113\n",
      "Iteration 181, loss = 0.04428690\n",
      "Iteration 182, loss = 0.04422788\n",
      "Iteration 183, loss = 0.04401741\n",
      "Iteration 184, loss = 0.04387958\n",
      "Iteration 185, loss = 0.04374916\n",
      "Iteration 186, loss = 0.04361660\n",
      "Iteration 187, loss = 0.04344946\n",
      "Iteration 188, loss = 0.04330801\n",
      "Iteration 189, loss = 0.04322679\n",
      "Iteration 190, loss = 0.04305772\n",
      "Iteration 191, loss = 0.04295459\n",
      "Iteration 192, loss = 0.04283387\n",
      "Iteration 193, loss = 0.04269720\n",
      "Iteration 194, loss = 0.04255718\n",
      "Iteration 195, loss = 0.04247442\n",
      "Iteration 196, loss = 0.04230159\n",
      "Iteration 197, loss = 0.04218637\n",
      "Iteration 198, loss = 0.04205925\n",
      "Iteration 199, loss = 0.04193372\n",
      "Iteration 200, loss = 0.04182711\n",
      "Iteration 201, loss = 0.04175517\n",
      "Iteration 202, loss = 0.04158966\n",
      "Iteration 203, loss = 0.04146537\n",
      "Iteration 204, loss = 0.04136310\n",
      "Iteration 205, loss = 0.04125201\n",
      "Iteration 206, loss = 0.04114321\n",
      "Iteration 207, loss = 0.04104098\n",
      "Iteration 208, loss = 0.04089555\n",
      "Iteration 209, loss = 0.04082635\n",
      "Iteration 210, loss = 0.04067949\n",
      "Iteration 211, loss = 0.04057940\n",
      "Iteration 212, loss = 0.04048578\n",
      "Iteration 213, loss = 0.04037064\n",
      "Iteration 214, loss = 0.04031440\n",
      "Iteration 215, loss = 0.04017059\n",
      "Iteration 216, loss = 0.04008162\n",
      "Iteration 217, loss = 0.03996662\n",
      "Iteration 218, loss = 0.03984942\n",
      "Iteration 219, loss = 0.03977190\n",
      "Iteration 220, loss = 0.03967320\n",
      "Iteration 221, loss = 0.03958160\n",
      "Iteration 222, loss = 0.03946500\n",
      "Iteration 223, loss = 0.03935626\n",
      "Iteration 224, loss = 0.03927896\n",
      "Iteration 225, loss = 0.03915849\n",
      "Iteration 226, loss = 0.03915000\n",
      "Iteration 227, loss = 0.03899767\n",
      "Iteration 228, loss = 0.03887978\n",
      "Iteration 229, loss = 0.03880486\n",
      "Iteration 230, loss = 0.03874712\n",
      "Iteration 231, loss = 0.03867067\n",
      "Iteration 232, loss = 0.03854494\n",
      "Iteration 233, loss = 0.03845922\n",
      "Iteration 234, loss = 0.03836150\n",
      "Iteration 235, loss = 0.03825848\n",
      "Iteration 236, loss = 0.03820469\n",
      "Iteration 237, loss = 0.03808557\n",
      "Iteration 238, loss = 0.03808690\n",
      "Iteration 239, loss = 0.03790588\n",
      "Iteration 240, loss = 0.03785078\n",
      "Iteration 241, loss = 0.03783282\n",
      "Iteration 242, loss = 0.03772459\n",
      "Iteration 243, loss = 0.03761902\n",
      "Iteration 244, loss = 0.03750452\n",
      "Iteration 245, loss = 0.03743579\n",
      "Iteration 246, loss = 0.03734571\n",
      "Iteration 247, loss = 0.03726362\n",
      "Iteration 248, loss = 0.03720740\n",
      "Iteration 249, loss = 0.03708685\n",
      "Iteration 250, loss = 0.03702206\n",
      "Iteration 251, loss = 0.03693965\n",
      "Iteration 252, loss = 0.03690155\n",
      "Iteration 253, loss = 0.03683997\n",
      "Iteration 254, loss = 0.03673603\n",
      "Iteration 255, loss = 0.03666888\n",
      "Iteration 256, loss = 0.03657862\n",
      "Iteration 257, loss = 0.03650374\n",
      "Iteration 258, loss = 0.03647107\n",
      "Iteration 259, loss = 0.03640738\n",
      "Iteration 260, loss = 0.03629723\n",
      "Iteration 261, loss = 0.03621964\n",
      "Iteration 262, loss = 0.03614274\n",
      "Iteration 263, loss = 0.03615924\n",
      "Iteration 264, loss = 0.03599718\n",
      "Iteration 265, loss = 0.03592455\n",
      "Iteration 266, loss = 0.03593378\n",
      "Iteration 267, loss = 0.03582029\n",
      "Iteration 268, loss = 0.03573718\n",
      "Iteration 269, loss = 0.03564511\n",
      "Iteration 270, loss = 0.03559286\n",
      "Iteration 271, loss = 0.03556222\n",
      "Iteration 272, loss = 0.03550013\n",
      "Iteration 273, loss = 0.03541059\n",
      "Iteration 274, loss = 0.03531037\n",
      "Iteration 275, loss = 0.03525429\n",
      "Iteration 276, loss = 0.03519944\n",
      "Iteration 277, loss = 0.03512833\n",
      "Iteration 278, loss = 0.03509396\n",
      "Iteration 279, loss = 0.03501244\n",
      "Iteration 280, loss = 0.03492261\n",
      "Iteration 281, loss = 0.03486743\n",
      "Iteration 282, loss = 0.03480050\n",
      "Iteration 283, loss = 0.03477444\n",
      "Iteration 284, loss = 0.03472513\n",
      "Iteration 285, loss = 0.03467327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.60166771\n",
      "Iteration 2, loss = 0.49312663\n",
      "Iteration 3, loss = 0.41686670\n",
      "Iteration 4, loss = 0.35948114\n",
      "Iteration 5, loss = 0.31766698\n",
      "Iteration 6, loss = 0.28692946\n",
      "Iteration 7, loss = 0.26302948\n",
      "Iteration 8, loss = 0.24453418\n",
      "Iteration 9, loss = 0.22776426\n",
      "Iteration 10, loss = 0.21488434\n",
      "Iteration 11, loss = 0.20370534\n",
      "Iteration 12, loss = 0.19380019\n",
      "Iteration 13, loss = 0.18570589\n",
      "Iteration 14, loss = 0.17816992\n",
      "Iteration 15, loss = 0.17163072\n",
      "Iteration 16, loss = 0.16563665\n",
      "Iteration 17, loss = 0.16046037\n",
      "Iteration 18, loss = 0.15551663\n",
      "Iteration 19, loss = 0.15098793\n",
      "Iteration 20, loss = 0.14699827\n",
      "Iteration 21, loss = 0.14321026\n",
      "Iteration 22, loss = 0.13970861\n",
      "Iteration 23, loss = 0.13657590\n",
      "Iteration 24, loss = 0.13331330\n",
      "Iteration 25, loss = 0.13063198\n",
      "Iteration 26, loss = 0.12786264\n",
      "Iteration 27, loss = 0.12544665\n",
      "Iteration 28, loss = 0.12305673\n",
      "Iteration 29, loss = 0.12079537\n",
      "Iteration 30, loss = 0.11876054\n",
      "Iteration 31, loss = 0.11678047\n",
      "Iteration 32, loss = 0.11493798\n",
      "Iteration 33, loss = 0.11310033\n",
      "Iteration 34, loss = 0.11145190\n",
      "Iteration 35, loss = 0.10989265\n",
      "Iteration 36, loss = 0.10836058\n",
      "Iteration 37, loss = 0.10690930\n",
      "Iteration 38, loss = 0.10551921\n",
      "Iteration 39, loss = 0.10425148\n",
      "Iteration 40, loss = 0.10294569\n",
      "Iteration 41, loss = 0.10178343\n",
      "Iteration 42, loss = 0.10059970\n",
      "Iteration 43, loss = 0.09954283\n",
      "Iteration 44, loss = 0.09842288\n",
      "Iteration 45, loss = 0.09749049\n",
      "Iteration 46, loss = 0.09639672\n",
      "Iteration 47, loss = 0.09542949\n",
      "Iteration 48, loss = 0.09462031\n",
      "Iteration 49, loss = 0.09364240\n",
      "Iteration 50, loss = 0.09278470\n",
      "Iteration 51, loss = 0.09193016\n",
      "Iteration 52, loss = 0.09111799\n",
      "Iteration 53, loss = 0.09032945\n",
      "Iteration 54, loss = 0.08965327\n",
      "Iteration 55, loss = 0.08881964\n",
      "Iteration 56, loss = 0.08816587\n",
      "Iteration 57, loss = 0.08740533\n",
      "Iteration 58, loss = 0.08674852\n",
      "Iteration 59, loss = 0.08605665\n",
      "Iteration 60, loss = 0.08538143\n",
      "Iteration 61, loss = 0.08477347\n",
      "Iteration 62, loss = 0.08417576\n",
      "Iteration 63, loss = 0.08356652\n",
      "Iteration 64, loss = 0.08296154\n",
      "Iteration 65, loss = 0.08240711\n",
      "Iteration 66, loss = 0.08186188\n",
      "Iteration 67, loss = 0.08127621\n",
      "Iteration 68, loss = 0.08092065\n",
      "Iteration 69, loss = 0.08024028\n",
      "Iteration 70, loss = 0.07971247\n",
      "Iteration 71, loss = 0.07920198\n",
      "Iteration 72, loss = 0.07870966\n",
      "Iteration 73, loss = 0.07819385\n",
      "Iteration 74, loss = 0.07777552\n",
      "Iteration 75, loss = 0.07729502\n",
      "Iteration 76, loss = 0.07683436\n",
      "Iteration 77, loss = 0.07633882\n",
      "Iteration 78, loss = 0.07586977\n",
      "Iteration 79, loss = 0.07549085\n",
      "Iteration 80, loss = 0.07504383\n",
      "Iteration 81, loss = 0.07461160\n",
      "Iteration 82, loss = 0.07417531\n",
      "Iteration 83, loss = 0.07388961\n",
      "Iteration 84, loss = 0.07330362\n",
      "Iteration 85, loss = 0.07300571\n",
      "Iteration 86, loss = 0.07257482\n",
      "Iteration 87, loss = 0.07215935\n",
      "Iteration 88, loss = 0.07177689\n",
      "Iteration 89, loss = 0.07138894\n",
      "Iteration 90, loss = 0.07102631\n",
      "Iteration 91, loss = 0.07069766\n",
      "Iteration 92, loss = 0.07025532\n",
      "Iteration 93, loss = 0.06988840\n",
      "Iteration 94, loss = 0.06952336\n",
      "Iteration 95, loss = 0.06912940\n",
      "Iteration 96, loss = 0.06881006\n",
      "Iteration 97, loss = 0.06857568\n",
      "Iteration 98, loss = 0.06819742\n",
      "Iteration 99, loss = 0.06777556\n",
      "Iteration 100, loss = 0.06748619\n",
      "Iteration 101, loss = 0.06710211\n",
      "Iteration 102, loss = 0.06674501\n",
      "Iteration 103, loss = 0.06643996\n",
      "Iteration 104, loss = 0.06610658\n",
      "Iteration 105, loss = 0.06580694\n",
      "Iteration 106, loss = 0.06545889\n",
      "Iteration 107, loss = 0.06512862\n",
      "Iteration 108, loss = 0.06482619\n",
      "Iteration 109, loss = 0.06448206\n",
      "Iteration 110, loss = 0.06424500\n",
      "Iteration 111, loss = 0.06389918\n",
      "Iteration 112, loss = 0.06360192\n",
      "Iteration 113, loss = 0.06328472\n",
      "Iteration 114, loss = 0.06294732\n",
      "Iteration 115, loss = 0.06276283\n",
      "Iteration 116, loss = 0.06243827\n",
      "Iteration 117, loss = 0.06219352\n",
      "Iteration 118, loss = 0.06184432\n",
      "Iteration 119, loss = 0.06157426\n",
      "Iteration 120, loss = 0.06127004\n",
      "Iteration 121, loss = 0.06101584\n",
      "Iteration 122, loss = 0.06069984\n",
      "Iteration 123, loss = 0.06050684\n",
      "Iteration 124, loss = 0.06018385\n",
      "Iteration 125, loss = 0.05994418\n",
      "Iteration 126, loss = 0.05963772\n",
      "Iteration 127, loss = 0.05938791\n",
      "Iteration 128, loss = 0.05912095\n",
      "Iteration 129, loss = 0.05883537\n",
      "Iteration 130, loss = 0.05869377\n",
      "Iteration 131, loss = 0.05837802\n",
      "Iteration 132, loss = 0.05808288\n",
      "Iteration 133, loss = 0.05784804\n",
      "Iteration 134, loss = 0.05768177\n",
      "Iteration 135, loss = 0.05737623\n",
      "Iteration 136, loss = 0.05708649\n",
      "Iteration 137, loss = 0.05694213\n",
      "Iteration 138, loss = 0.05666262\n",
      "Iteration 139, loss = 0.05642243\n",
      "Iteration 140, loss = 0.05617324\n",
      "Iteration 141, loss = 0.05592271\n",
      "Iteration 142, loss = 0.05572831\n",
      "Iteration 143, loss = 0.05554085\n",
      "Iteration 144, loss = 0.05528349\n",
      "Iteration 145, loss = 0.05509042\n",
      "Iteration 146, loss = 0.05487375\n",
      "Iteration 147, loss = 0.05466405\n",
      "Iteration 148, loss = 0.05444927\n",
      "Iteration 149, loss = 0.05426328\n",
      "Iteration 150, loss = 0.05411691\n",
      "Iteration 151, loss = 0.05382961\n",
      "Iteration 152, loss = 0.05359076\n",
      "Iteration 153, loss = 0.05357559\n",
      "Iteration 154, loss = 0.05322682\n",
      "Iteration 155, loss = 0.05299374\n",
      "Iteration 156, loss = 0.05282633\n",
      "Iteration 157, loss = 0.05256865\n",
      "Iteration 158, loss = 0.05248226\n",
      "Iteration 159, loss = 0.05225510\n",
      "Iteration 160, loss = 0.05203643\n",
      "Iteration 161, loss = 0.05186475\n",
      "Iteration 162, loss = 0.05173185\n",
      "Iteration 163, loss = 0.05148314\n",
      "Iteration 164, loss = 0.05128600\n",
      "Iteration 165, loss = 0.05118815\n",
      "Iteration 166, loss = 0.05096824\n",
      "Iteration 167, loss = 0.05072939\n",
      "Iteration 168, loss = 0.05059352\n",
      "Iteration 169, loss = 0.05049181\n",
      "Iteration 170, loss = 0.05021180\n",
      "Iteration 171, loss = 0.05004888\n",
      "Iteration 172, loss = 0.04985932\n",
      "Iteration 173, loss = 0.04970522\n",
      "Iteration 174, loss = 0.04957519\n",
      "Iteration 175, loss = 0.04941908\n",
      "Iteration 176, loss = 0.04918704\n",
      "Iteration 177, loss = 0.04907347\n",
      "Iteration 178, loss = 0.04884434\n",
      "Iteration 179, loss = 0.04870212\n",
      "Iteration 180, loss = 0.04873472\n",
      "Iteration 181, loss = 0.04840661\n",
      "Iteration 182, loss = 0.04821041\n",
      "Iteration 183, loss = 0.04813877\n",
      "Iteration 184, loss = 0.04793032\n",
      "Iteration 185, loss = 0.04779518\n",
      "Iteration 186, loss = 0.04760817\n",
      "Iteration 187, loss = 0.04751837\n",
      "Iteration 188, loss = 0.04735492\n",
      "Iteration 189, loss = 0.04723886\n",
      "Iteration 190, loss = 0.04705356\n",
      "Iteration 191, loss = 0.04695346\n",
      "Iteration 192, loss = 0.04679208\n",
      "Iteration 193, loss = 0.04663403\n",
      "Iteration 194, loss = 0.04649407\n",
      "Iteration 195, loss = 0.04636112\n",
      "Iteration 196, loss = 0.04619547\n",
      "Iteration 197, loss = 0.04609190\n",
      "Iteration 198, loss = 0.04597943\n",
      "Iteration 199, loss = 0.04583375\n",
      "Iteration 200, loss = 0.04572693\n",
      "Iteration 201, loss = 0.04559399\n",
      "Iteration 202, loss = 0.04548872\n",
      "Iteration 203, loss = 0.04533447\n",
      "Iteration 204, loss = 0.04520220\n",
      "Iteration 205, loss = 0.04508221\n",
      "Iteration 206, loss = 0.04494056\n",
      "Iteration 207, loss = 0.04487624\n",
      "Iteration 208, loss = 0.04470835\n",
      "Iteration 209, loss = 0.04454790\n",
      "Iteration 210, loss = 0.04449385\n",
      "Iteration 211, loss = 0.04434719\n",
      "Iteration 212, loss = 0.04423749\n",
      "Iteration 213, loss = 0.04412372\n",
      "Iteration 214, loss = 0.04400564\n",
      "Iteration 215, loss = 0.04388484\n",
      "Iteration 216, loss = 0.04375973\n",
      "Iteration 217, loss = 0.04366270\n",
      "Iteration 218, loss = 0.04358817\n",
      "Iteration 219, loss = 0.04348905\n",
      "Iteration 220, loss = 0.04341683\n",
      "Iteration 221, loss = 0.04321209\n",
      "Iteration 222, loss = 0.04312685\n",
      "Iteration 223, loss = 0.04304082\n",
      "Iteration 224, loss = 0.04290551\n",
      "Iteration 225, loss = 0.04285799\n",
      "Iteration 226, loss = 0.04277777\n",
      "Iteration 227, loss = 0.04268954\n",
      "Iteration 228, loss = 0.04252817\n",
      "Iteration 229, loss = 0.04241296\n",
      "Iteration 230, loss = 0.04237656\n",
      "Iteration 231, loss = 0.04220534\n",
      "Iteration 232, loss = 0.04212472\n",
      "Iteration 233, loss = 0.04201034\n",
      "Iteration 234, loss = 0.04201919\n",
      "Iteration 235, loss = 0.04182055\n",
      "Iteration 236, loss = 0.04172490\n",
      "Iteration 237, loss = 0.04168005\n",
      "Iteration 238, loss = 0.04155772\n",
      "Iteration 239, loss = 0.04144081\n",
      "Iteration 240, loss = 0.04134105\n",
      "Iteration 241, loss = 0.04133048\n",
      "Iteration 242, loss = 0.04119442\n",
      "Iteration 243, loss = 0.04108217\n",
      "Iteration 244, loss = 0.04104645\n",
      "Iteration 245, loss = 0.04094284\n",
      "Iteration 246, loss = 0.04079878\n",
      "Iteration 247, loss = 0.04075205\n",
      "Iteration 248, loss = 0.04064735\n",
      "Iteration 249, loss = 0.04061692\n",
      "Iteration 250, loss = 0.04048061\n",
      "Iteration 251, loss = 0.04043066\n",
      "Iteration 252, loss = 0.04040602\n",
      "Iteration 253, loss = 0.04023911\n",
      "Iteration 254, loss = 0.04014033\n",
      "Iteration 255, loss = 0.04013804\n",
      "Iteration 256, loss = 0.03999030\n",
      "Iteration 257, loss = 0.03990427\n",
      "Iteration 258, loss = 0.03982447\n",
      "Iteration 259, loss = 0.03979370\n",
      "Iteration 260, loss = 0.03969555\n",
      "Iteration 261, loss = 0.03961265\n",
      "Iteration 262, loss = 0.03951212\n",
      "Iteration 263, loss = 0.03944039\n",
      "Iteration 264, loss = 0.03935927\n",
      "Iteration 265, loss = 0.03927933\n",
      "Iteration 266, loss = 0.03921714\n",
      "Iteration 267, loss = 0.03914543\n",
      "Iteration 268, loss = 0.03908634\n",
      "Iteration 269, loss = 0.03899850\n",
      "Iteration 270, loss = 0.03904786\n",
      "Iteration 271, loss = 0.03884545\n",
      "Iteration 272, loss = 0.03877826\n",
      "Iteration 273, loss = 0.03869738\n",
      "Iteration 274, loss = 0.03862401\n",
      "Iteration 275, loss = 0.03857544\n",
      "Iteration 276, loss = 0.03850756\n",
      "Iteration 277, loss = 0.03841520\n",
      "Iteration 278, loss = 0.03836605\n",
      "Iteration 279, loss = 0.03827167\n",
      "Iteration 280, loss = 0.03819315\n",
      "Iteration 281, loss = 0.03811229\n",
      "Iteration 282, loss = 0.03809961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.59785756\n",
      "Iteration 2, loss = 0.48989639\n",
      "Iteration 3, loss = 0.41346932\n",
      "Iteration 4, loss = 0.35701511\n",
      "Iteration 5, loss = 0.31622652\n",
      "Iteration 6, loss = 0.28610334\n",
      "Iteration 7, loss = 0.26274940\n",
      "Iteration 8, loss = 0.24459926\n",
      "Iteration 9, loss = 0.22814020\n",
      "Iteration 10, loss = 0.21542651\n",
      "Iteration 11, loss = 0.20428094\n",
      "Iteration 12, loss = 0.19436478\n",
      "Iteration 13, loss = 0.18632447\n",
      "Iteration 14, loss = 0.17886373\n",
      "Iteration 15, loss = 0.17223240\n",
      "Iteration 16, loss = 0.16630425\n",
      "Iteration 17, loss = 0.16115453\n",
      "Iteration 18, loss = 0.15621445\n",
      "Iteration 19, loss = 0.15177155\n",
      "Iteration 20, loss = 0.14776594\n",
      "Iteration 21, loss = 0.14397956\n",
      "Iteration 22, loss = 0.14044614\n",
      "Iteration 23, loss = 0.13741468\n",
      "Iteration 24, loss = 0.13417486\n",
      "Iteration 25, loss = 0.13143247\n",
      "Iteration 26, loss = 0.12872577\n",
      "Iteration 27, loss = 0.12630589\n",
      "Iteration 28, loss = 0.12393881\n",
      "Iteration 29, loss = 0.12169053\n",
      "Iteration 30, loss = 0.11963688\n",
      "Iteration 31, loss = 0.11764705\n",
      "Iteration 32, loss = 0.11577612\n",
      "Iteration 33, loss = 0.11398081\n",
      "Iteration 34, loss = 0.11233360\n",
      "Iteration 35, loss = 0.11069730\n",
      "Iteration 36, loss = 0.10919655\n",
      "Iteration 37, loss = 0.10773947\n",
      "Iteration 38, loss = 0.10634864\n",
      "Iteration 39, loss = 0.10502258\n",
      "Iteration 40, loss = 0.10374266\n",
      "Iteration 41, loss = 0.10254358\n",
      "Iteration 42, loss = 0.10132355\n",
      "Iteration 43, loss = 0.10022927\n",
      "Iteration 44, loss = 0.09912993\n",
      "Iteration 45, loss = 0.09814204\n",
      "Iteration 46, loss = 0.09703074\n",
      "Iteration 47, loss = 0.09606168\n",
      "Iteration 48, loss = 0.09519801\n",
      "Iteration 49, loss = 0.09418862\n",
      "Iteration 50, loss = 0.09332367\n",
      "Iteration 51, loss = 0.09247769\n",
      "Iteration 52, loss = 0.09164711\n",
      "Iteration 53, loss = 0.09082519\n",
      "Iteration 54, loss = 0.09014624\n",
      "Iteration 55, loss = 0.08928508\n",
      "Iteration 56, loss = 0.08864661\n",
      "Iteration 57, loss = 0.08786429\n",
      "Iteration 58, loss = 0.08715923\n",
      "Iteration 59, loss = 0.08649377\n",
      "Iteration 60, loss = 0.08577018\n",
      "Iteration 61, loss = 0.08517665\n",
      "Iteration 62, loss = 0.08454853\n",
      "Iteration 63, loss = 0.08390747\n",
      "Iteration 64, loss = 0.08330611\n",
      "Iteration 65, loss = 0.08273206\n",
      "Iteration 66, loss = 0.08217482\n",
      "Iteration 67, loss = 0.08157411\n",
      "Iteration 68, loss = 0.08124925\n",
      "Iteration 69, loss = 0.08051042\n",
      "Iteration 70, loss = 0.07996532\n",
      "Iteration 71, loss = 0.07943976\n",
      "Iteration 72, loss = 0.07894094\n",
      "Iteration 73, loss = 0.07842004\n",
      "Iteration 74, loss = 0.07796392\n",
      "Iteration 75, loss = 0.07748698\n",
      "Iteration 76, loss = 0.07704849\n",
      "Iteration 77, loss = 0.07652735\n",
      "Iteration 78, loss = 0.07607244\n",
      "Iteration 79, loss = 0.07568988\n",
      "Iteration 80, loss = 0.07523456\n",
      "Iteration 81, loss = 0.07478197\n",
      "Iteration 82, loss = 0.07431081\n",
      "Iteration 83, loss = 0.07403000\n",
      "Iteration 84, loss = 0.07345793\n",
      "Iteration 85, loss = 0.07309893\n",
      "Iteration 86, loss = 0.07266209\n",
      "Iteration 87, loss = 0.07224842\n",
      "Iteration 88, loss = 0.07184348\n",
      "Iteration 89, loss = 0.07146060\n",
      "Iteration 90, loss = 0.07109114\n",
      "Iteration 91, loss = 0.07074331\n",
      "Iteration 92, loss = 0.07029415\n",
      "Iteration 93, loss = 0.06991331\n",
      "Iteration 94, loss = 0.06953434\n",
      "Iteration 95, loss = 0.06914232\n",
      "Iteration 96, loss = 0.06880240\n",
      "Iteration 97, loss = 0.06858618\n",
      "Iteration 98, loss = 0.06817791\n",
      "Iteration 99, loss = 0.06775437\n",
      "Iteration 100, loss = 0.06746007\n",
      "Iteration 101, loss = 0.06705659\n",
      "Iteration 102, loss = 0.06669392\n",
      "Iteration 103, loss = 0.06637506\n",
      "Iteration 104, loss = 0.06605117\n",
      "Iteration 105, loss = 0.06572621\n",
      "Iteration 106, loss = 0.06538811\n",
      "Iteration 107, loss = 0.06503892\n",
      "Iteration 108, loss = 0.06475003\n",
      "Iteration 109, loss = 0.06439620\n",
      "Iteration 110, loss = 0.06414509\n",
      "Iteration 111, loss = 0.06379888\n",
      "Iteration 112, loss = 0.06351059\n",
      "Iteration 113, loss = 0.06316644\n",
      "Iteration 114, loss = 0.06283783\n",
      "Iteration 115, loss = 0.06262296\n",
      "Iteration 116, loss = 0.06229884\n",
      "Iteration 117, loss = 0.06203789\n",
      "Iteration 118, loss = 0.06167416\n",
      "Iteration 119, loss = 0.06143940\n",
      "Iteration 120, loss = 0.06111136\n",
      "Iteration 121, loss = 0.06086924\n",
      "Iteration 122, loss = 0.06052650\n",
      "Iteration 123, loss = 0.06030484\n",
      "Iteration 124, loss = 0.06000116\n",
      "Iteration 125, loss = 0.05977689\n",
      "Iteration 126, loss = 0.05944122\n",
      "Iteration 127, loss = 0.05919248\n",
      "Iteration 128, loss = 0.05893512\n",
      "Iteration 129, loss = 0.05862785\n",
      "Iteration 130, loss = 0.05848188\n",
      "Iteration 131, loss = 0.05818997\n",
      "Iteration 132, loss = 0.05788056\n",
      "Iteration 133, loss = 0.05765929\n",
      "Iteration 134, loss = 0.05746835\n",
      "Iteration 135, loss = 0.05715201\n",
      "Iteration 136, loss = 0.05689290\n",
      "Iteration 137, loss = 0.05673013\n",
      "Iteration 138, loss = 0.05644726\n",
      "Iteration 139, loss = 0.05621194\n",
      "Iteration 140, loss = 0.05596648\n",
      "Iteration 141, loss = 0.05571009\n",
      "Iteration 142, loss = 0.05551438\n",
      "Iteration 143, loss = 0.05531117\n",
      "Iteration 144, loss = 0.05509306\n",
      "Iteration 145, loss = 0.05488358\n",
      "Iteration 146, loss = 0.05466311\n",
      "Iteration 147, loss = 0.05445187\n",
      "Iteration 148, loss = 0.05424426\n",
      "Iteration 149, loss = 0.05404352\n",
      "Iteration 150, loss = 0.05390393\n",
      "Iteration 151, loss = 0.05363031\n",
      "Iteration 152, loss = 0.05340147\n",
      "Iteration 153, loss = 0.05336745\n",
      "Iteration 154, loss = 0.05304483\n",
      "Iteration 155, loss = 0.05283115\n",
      "Iteration 156, loss = 0.05266032\n",
      "Iteration 157, loss = 0.05240660\n",
      "Iteration 158, loss = 0.05232027\n",
      "Iteration 159, loss = 0.05209147\n",
      "Iteration 160, loss = 0.05189076\n",
      "Iteration 161, loss = 0.05169652\n",
      "Iteration 162, loss = 0.05158511\n",
      "Iteration 163, loss = 0.05134539\n",
      "Iteration 164, loss = 0.05113596\n",
      "Iteration 165, loss = 0.05108189\n",
      "Iteration 166, loss = 0.05084612\n",
      "Iteration 167, loss = 0.05062128\n",
      "Iteration 168, loss = 0.05047828\n",
      "Iteration 169, loss = 0.05037214\n",
      "Iteration 170, loss = 0.05010452\n",
      "Iteration 171, loss = 0.04993877\n",
      "Iteration 172, loss = 0.04976322\n",
      "Iteration 173, loss = 0.04960864\n",
      "Iteration 174, loss = 0.04948061\n",
      "Iteration 175, loss = 0.04932664\n",
      "Iteration 176, loss = 0.04910897\n",
      "Iteration 177, loss = 0.04899473\n",
      "Iteration 178, loss = 0.04877536\n",
      "Iteration 179, loss = 0.04861016\n",
      "Iteration 180, loss = 0.04867674\n",
      "Iteration 181, loss = 0.04834251\n",
      "Iteration 182, loss = 0.04814209\n",
      "Iteration 183, loss = 0.04809764\n",
      "Iteration 184, loss = 0.04787050\n",
      "Iteration 185, loss = 0.04772144\n",
      "Iteration 186, loss = 0.04753757\n",
      "Iteration 187, loss = 0.04744113\n",
      "Iteration 188, loss = 0.04728194\n",
      "Iteration 189, loss = 0.04716227\n",
      "Iteration 190, loss = 0.04698495\n",
      "Iteration 191, loss = 0.04686906\n",
      "Iteration 192, loss = 0.04671265\n",
      "Iteration 193, loss = 0.04657518\n",
      "Iteration 194, loss = 0.04642992\n",
      "Iteration 195, loss = 0.04630847\n",
      "Iteration 196, loss = 0.04612730\n",
      "Iteration 197, loss = 0.04601959\n",
      "Iteration 198, loss = 0.04589574\n",
      "Iteration 199, loss = 0.04575777\n",
      "Iteration 200, loss = 0.04562935\n",
      "Iteration 201, loss = 0.04550766\n",
      "Iteration 202, loss = 0.04539491\n",
      "Iteration 203, loss = 0.04524359\n",
      "Iteration 204, loss = 0.04511711\n",
      "Iteration 205, loss = 0.04501052\n",
      "Iteration 206, loss = 0.04486162\n",
      "Iteration 207, loss = 0.04481538\n",
      "Iteration 208, loss = 0.04462247\n",
      "Iteration 209, loss = 0.04446962\n",
      "Iteration 210, loss = 0.04438899\n",
      "Iteration 211, loss = 0.04428455\n",
      "Iteration 212, loss = 0.04416185\n",
      "Iteration 213, loss = 0.04404648\n",
      "Iteration 214, loss = 0.04391152\n",
      "Iteration 215, loss = 0.04379173\n",
      "Iteration 216, loss = 0.04365958\n",
      "Iteration 217, loss = 0.04356313\n",
      "Iteration 218, loss = 0.04346979\n",
      "Iteration 219, loss = 0.04338929\n",
      "Iteration 220, loss = 0.04332975\n",
      "Iteration 221, loss = 0.04311799\n",
      "Iteration 222, loss = 0.04301871\n",
      "Iteration 223, loss = 0.04290907\n",
      "Iteration 224, loss = 0.04278830\n",
      "Iteration 225, loss = 0.04276460\n",
      "Iteration 226, loss = 0.04267886\n",
      "Iteration 227, loss = 0.04255354\n",
      "Iteration 228, loss = 0.04239975\n",
      "Iteration 229, loss = 0.04227407\n",
      "Iteration 230, loss = 0.04222196\n",
      "Iteration 231, loss = 0.04206938\n",
      "Iteration 232, loss = 0.04198708\n",
      "Iteration 233, loss = 0.04186109\n",
      "Iteration 234, loss = 0.04188533\n",
      "Iteration 235, loss = 0.04166063\n",
      "Iteration 236, loss = 0.04156682\n",
      "Iteration 237, loss = 0.04152674\n",
      "Iteration 238, loss = 0.04140306\n",
      "Iteration 239, loss = 0.04128524\n",
      "Iteration 240, loss = 0.04119460\n",
      "Iteration 241, loss = 0.04120655\n",
      "Iteration 242, loss = 0.04103210\n",
      "Iteration 243, loss = 0.04092183\n",
      "Iteration 244, loss = 0.04089334\n",
      "Iteration 245, loss = 0.04080591\n",
      "Iteration 246, loss = 0.04065191\n",
      "Iteration 247, loss = 0.04059668\n",
      "Iteration 248, loss = 0.04048809\n",
      "Iteration 249, loss = 0.04046673\n",
      "Iteration 250, loss = 0.04032736\n",
      "Iteration 251, loss = 0.04025633\n",
      "Iteration 252, loss = 0.04024090\n",
      "Iteration 253, loss = 0.04007084\n",
      "Iteration 254, loss = 0.03996478\n",
      "Iteration 255, loss = 0.03997722\n",
      "Iteration 256, loss = 0.03982636\n",
      "Iteration 257, loss = 0.03974334\n",
      "Iteration 258, loss = 0.03966173\n",
      "Iteration 259, loss = 0.03962590\n",
      "Iteration 260, loss = 0.03953619\n",
      "Iteration 261, loss = 0.03945025\n",
      "Iteration 262, loss = 0.03934378\n",
      "Iteration 263, loss = 0.03927052\n",
      "Iteration 264, loss = 0.03920295\n",
      "Iteration 265, loss = 0.03911021\n",
      "Iteration 266, loss = 0.03904749\n",
      "Iteration 267, loss = 0.03898498\n",
      "Iteration 268, loss = 0.03892737\n",
      "Iteration 269, loss = 0.03883803\n",
      "Iteration 270, loss = 0.03889864\n",
      "Iteration 271, loss = 0.03868672\n",
      "Iteration 272, loss = 0.03863862\n",
      "Iteration 273, loss = 0.03854608\n",
      "Iteration 274, loss = 0.03846798\n",
      "Iteration 275, loss = 0.03842769\n",
      "Iteration 276, loss = 0.03836204\n",
      "Iteration 277, loss = 0.03827292\n",
      "Iteration 278, loss = 0.03823410\n",
      "Iteration 279, loss = 0.03811432\n",
      "Iteration 280, loss = 0.03804384\n",
      "Iteration 281, loss = 0.03797679\n",
      "Iteration 282, loss = 0.03794641\n",
      "Iteration 283, loss = 0.03785002\n",
      "Iteration 284, loss = 0.03782023\n",
      "Iteration 285, loss = 0.03773245\n",
      "Iteration 286, loss = 0.03767197\n",
      "Iteration 287, loss = 0.03761850\n",
      "Iteration 288, loss = 0.03752955\n",
      "Iteration 289, loss = 0.03747557\n",
      "Iteration 290, loss = 0.03739912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.59551392\n",
      "Iteration 2, loss = 0.48906251\n",
      "Iteration 3, loss = 0.41081023\n",
      "Iteration 4, loss = 0.35217122\n",
      "Iteration 5, loss = 0.31089228\n",
      "Iteration 6, loss = 0.27978694\n",
      "Iteration 7, loss = 0.25575414\n",
      "Iteration 8, loss = 0.23707034\n",
      "Iteration 9, loss = 0.22028195\n",
      "Iteration 10, loss = 0.20718822\n",
      "Iteration 11, loss = 0.19576550\n",
      "Iteration 12, loss = 0.18551444\n",
      "Iteration 13, loss = 0.17706477\n",
      "Iteration 14, loss = 0.16940876\n",
      "Iteration 15, loss = 0.16241752\n",
      "Iteration 16, loss = 0.15629029\n",
      "Iteration 17, loss = 0.15088508\n",
      "Iteration 18, loss = 0.14571147\n",
      "Iteration 19, loss = 0.14101342\n",
      "Iteration 20, loss = 0.13679318\n",
      "Iteration 21, loss = 0.13278538\n",
      "Iteration 22, loss = 0.12915533\n",
      "Iteration 23, loss = 0.12592679\n",
      "Iteration 24, loss = 0.12256865\n",
      "Iteration 25, loss = 0.11958941\n",
      "Iteration 26, loss = 0.11686006\n",
      "Iteration 27, loss = 0.11432495\n",
      "Iteration 28, loss = 0.11188270\n",
      "Iteration 29, loss = 0.10958894\n",
      "Iteration 30, loss = 0.10745311\n",
      "Iteration 31, loss = 0.10541340\n",
      "Iteration 32, loss = 0.10345230\n",
      "Iteration 33, loss = 0.10161954\n",
      "Iteration 34, loss = 0.09985829\n",
      "Iteration 35, loss = 0.09815454\n",
      "Iteration 36, loss = 0.09655437\n",
      "Iteration 37, loss = 0.09512446\n",
      "Iteration 38, loss = 0.09363869\n",
      "Iteration 39, loss = 0.09219269\n",
      "Iteration 40, loss = 0.09090949\n",
      "Iteration 41, loss = 0.08959504\n",
      "Iteration 42, loss = 0.08832115\n",
      "Iteration 43, loss = 0.08717644\n",
      "Iteration 44, loss = 0.08606786\n",
      "Iteration 45, loss = 0.08490938\n",
      "Iteration 46, loss = 0.08384108\n",
      "Iteration 47, loss = 0.08281457\n",
      "Iteration 48, loss = 0.08191625\n",
      "Iteration 49, loss = 0.08087171\n",
      "Iteration 50, loss = 0.07998671\n",
      "Iteration 51, loss = 0.07911536\n",
      "Iteration 52, loss = 0.07829777\n",
      "Iteration 53, loss = 0.07742275\n",
      "Iteration 54, loss = 0.07665534\n",
      "Iteration 55, loss = 0.07583932\n",
      "Iteration 56, loss = 0.07521310\n",
      "Iteration 57, loss = 0.07443318\n",
      "Iteration 58, loss = 0.07372534\n",
      "Iteration 59, loss = 0.07302466\n",
      "Iteration 60, loss = 0.07232320\n",
      "Iteration 61, loss = 0.07170377\n",
      "Iteration 62, loss = 0.07109272\n",
      "Iteration 63, loss = 0.07045286\n",
      "Iteration 64, loss = 0.06987520\n",
      "Iteration 65, loss = 0.06930190\n",
      "Iteration 66, loss = 0.06876495\n",
      "Iteration 67, loss = 0.06819479\n",
      "Iteration 68, loss = 0.06776969\n",
      "Iteration 69, loss = 0.06714587\n",
      "Iteration 70, loss = 0.06660389\n",
      "Iteration 71, loss = 0.06608530\n",
      "Iteration 72, loss = 0.06558161\n",
      "Iteration 73, loss = 0.06511455\n",
      "Iteration 74, loss = 0.06466487\n",
      "Iteration 75, loss = 0.06419622\n",
      "Iteration 76, loss = 0.06376306\n",
      "Iteration 77, loss = 0.06330469\n",
      "Iteration 78, loss = 0.06289349\n",
      "Iteration 79, loss = 0.06246550\n",
      "Iteration 80, loss = 0.06205382\n",
      "Iteration 81, loss = 0.06166499\n",
      "Iteration 82, loss = 0.06121089\n",
      "Iteration 83, loss = 0.06091130\n",
      "Iteration 84, loss = 0.06046181\n",
      "Iteration 85, loss = 0.06009212\n",
      "Iteration 86, loss = 0.05973208\n",
      "Iteration 87, loss = 0.05936587\n",
      "Iteration 88, loss = 0.05901029\n",
      "Iteration 89, loss = 0.05864138\n",
      "Iteration 90, loss = 0.05830669\n",
      "Iteration 91, loss = 0.05795361\n",
      "Iteration 92, loss = 0.05762606\n",
      "Iteration 93, loss = 0.05731674\n",
      "Iteration 94, loss = 0.05695317\n",
      "Iteration 95, loss = 0.05663284\n",
      "Iteration 96, loss = 0.05631710\n",
      "Iteration 97, loss = 0.05606248\n",
      "Iteration 98, loss = 0.05572551\n",
      "Iteration 99, loss = 0.05542738\n",
      "Iteration 100, loss = 0.05514119\n",
      "Iteration 101, loss = 0.05484204\n",
      "Iteration 102, loss = 0.05455980\n",
      "Iteration 103, loss = 0.05426461\n",
      "Iteration 104, loss = 0.05399733\n",
      "Iteration 105, loss = 0.05373027\n",
      "Iteration 106, loss = 0.05345802\n",
      "Iteration 107, loss = 0.05318934\n",
      "Iteration 108, loss = 0.05295482\n",
      "Iteration 109, loss = 0.05269303\n",
      "Iteration 110, loss = 0.05244054\n",
      "Iteration 111, loss = 0.05216271\n",
      "Iteration 112, loss = 0.05196732\n",
      "Iteration 113, loss = 0.05169404\n",
      "Iteration 114, loss = 0.05145218\n",
      "Iteration 115, loss = 0.05126958\n",
      "Iteration 116, loss = 0.05103555\n",
      "Iteration 117, loss = 0.05081431\n",
      "Iteration 118, loss = 0.05049229\n",
      "Iteration 119, loss = 0.05037629\n",
      "Iteration 120, loss = 0.05008967\n",
      "Iteration 121, loss = 0.04993620\n",
      "Iteration 122, loss = 0.04963128\n",
      "Iteration 123, loss = 0.04941203\n",
      "Iteration 124, loss = 0.04920386\n",
      "Iteration 125, loss = 0.04902183\n",
      "Iteration 126, loss = 0.04878932\n",
      "Iteration 127, loss = 0.04858369\n",
      "Iteration 128, loss = 0.04842264\n",
      "Iteration 129, loss = 0.04818078\n",
      "Iteration 130, loss = 0.04799648\n",
      "Iteration 131, loss = 0.04783099\n",
      "Iteration 132, loss = 0.04760995\n",
      "Iteration 133, loss = 0.04747468\n",
      "Iteration 134, loss = 0.04724562\n",
      "Iteration 135, loss = 0.04703151\n",
      "Iteration 136, loss = 0.04686795\n",
      "Iteration 137, loss = 0.04675394\n",
      "Iteration 138, loss = 0.04653012\n",
      "Iteration 139, loss = 0.04638331\n",
      "Iteration 140, loss = 0.04617814\n",
      "Iteration 141, loss = 0.04598283\n",
      "Iteration 142, loss = 0.04582517\n",
      "Iteration 143, loss = 0.04564965\n",
      "Iteration 144, loss = 0.04548731\n",
      "Iteration 145, loss = 0.04535028\n",
      "Iteration 146, loss = 0.04517680\n",
      "Iteration 147, loss = 0.04499828\n",
      "Iteration 148, loss = 0.04487573\n",
      "Iteration 149, loss = 0.04466430\n",
      "Iteration 150, loss = 0.04454281\n",
      "Iteration 151, loss = 0.04433644\n",
      "Iteration 152, loss = 0.04417979\n",
      "Iteration 153, loss = 0.04408457\n",
      "Iteration 154, loss = 0.04388677\n",
      "Iteration 155, loss = 0.04375505\n",
      "Iteration 156, loss = 0.04359623\n",
      "Iteration 157, loss = 0.04342805\n",
      "Iteration 158, loss = 0.04330070\n",
      "Iteration 159, loss = 0.04316630\n",
      "Iteration 160, loss = 0.04304506\n",
      "Iteration 161, loss = 0.04287814\n",
      "Iteration 162, loss = 0.04276010\n",
      "Iteration 163, loss = 0.04261998\n",
      "Iteration 164, loss = 0.04245479\n",
      "Iteration 165, loss = 0.04232309\n",
      "Iteration 166, loss = 0.04221790\n",
      "Iteration 167, loss = 0.04203666\n",
      "Iteration 168, loss = 0.04191151\n",
      "Iteration 169, loss = 0.04181706\n",
      "Iteration 170, loss = 0.04164508\n",
      "Iteration 171, loss = 0.04152097\n",
      "Iteration 172, loss = 0.04138405\n",
      "Iteration 173, loss = 0.04126651\n",
      "Iteration 174, loss = 0.04117078\n",
      "Iteration 175, loss = 0.04106545\n",
      "Iteration 176, loss = 0.04087640\n",
      "Iteration 177, loss = 0.04078506\n",
      "Iteration 178, loss = 0.04065210\n",
      "Iteration 179, loss = 0.04050113\n",
      "Iteration 180, loss = 0.04049555\n",
      "Iteration 181, loss = 0.04029357\n",
      "Iteration 182, loss = 0.04019618\n",
      "Iteration 183, loss = 0.04013131\n",
      "Iteration 184, loss = 0.03994307\n",
      "Iteration 185, loss = 0.03981598\n",
      "Iteration 186, loss = 0.03972331\n",
      "Iteration 187, loss = 0.03960075\n",
      "Iteration 188, loss = 0.03948665\n",
      "Iteration 189, loss = 0.03940889\n",
      "Iteration 190, loss = 0.03925153\n",
      "Iteration 191, loss = 0.03916076\n",
      "Iteration 192, loss = 0.03903987\n",
      "Iteration 193, loss = 0.03893873\n",
      "Iteration 194, loss = 0.03883835\n",
      "Iteration 195, loss = 0.03875290\n",
      "Iteration 196, loss = 0.03861436\n",
      "Iteration 197, loss = 0.03850053\n",
      "Iteration 198, loss = 0.03840116\n",
      "Iteration 199, loss = 0.03829293\n",
      "Iteration 200, loss = 0.03819003\n",
      "Iteration 201, loss = 0.03809355\n",
      "Iteration 202, loss = 0.03804440\n",
      "Iteration 203, loss = 0.03790174\n",
      "Iteration 204, loss = 0.03780854\n",
      "Iteration 205, loss = 0.03771678\n",
      "Iteration 206, loss = 0.03760574\n",
      "Iteration 207, loss = 0.03758518\n",
      "Iteration 208, loss = 0.03743164\n",
      "Iteration 209, loss = 0.03732661\n",
      "Iteration 210, loss = 0.03724907\n",
      "Iteration 211, loss = 0.03717651\n",
      "Iteration 212, loss = 0.03703734\n",
      "Iteration 213, loss = 0.03693816\n",
      "Iteration 214, loss = 0.03685816\n",
      "Iteration 215, loss = 0.03676316\n",
      "Iteration 216, loss = 0.03665320\n",
      "Iteration 217, loss = 0.03659699\n",
      "Iteration 218, loss = 0.03648640\n",
      "Iteration 219, loss = 0.03640556\n",
      "Iteration 220, loss = 0.03637267\n",
      "Iteration 221, loss = 0.03623673\n",
      "Iteration 222, loss = 0.03614593\n",
      "Iteration 223, loss = 0.03604746\n",
      "Iteration 224, loss = 0.03597431\n",
      "Iteration 225, loss = 0.03598498\n",
      "Iteration 226, loss = 0.03586305\n",
      "Iteration 227, loss = 0.03572993\n",
      "Iteration 228, loss = 0.03562422\n",
      "Iteration 229, loss = 0.03555323\n",
      "Iteration 230, loss = 0.03551444\n",
      "Iteration 231, loss = 0.03539114\n",
      "Iteration 232, loss = 0.03529327\n",
      "Iteration 233, loss = 0.03521253\n",
      "Iteration 234, loss = 0.03519914\n",
      "Iteration 235, loss = 0.03503410\n",
      "Iteration 236, loss = 0.03496320\n",
      "Iteration 237, loss = 0.03496294\n",
      "Iteration 238, loss = 0.03482596\n",
      "Iteration 239, loss = 0.03473339\n",
      "Iteration 240, loss = 0.03467782\n",
      "Iteration 241, loss = 0.03466998\n",
      "Iteration 242, loss = 0.03452543\n",
      "Iteration 243, loss = 0.03443908\n",
      "Iteration 244, loss = 0.03437903\n",
      "Iteration 245, loss = 0.03434864\n",
      "Iteration 246, loss = 0.03421130\n",
      "Iteration 247, loss = 0.03415502\n",
      "Iteration 248, loss = 0.03406795\n",
      "Iteration 249, loss = 0.03401219\n",
      "Iteration 250, loss = 0.03392086\n",
      "Iteration 251, loss = 0.03386710\n",
      "Iteration 252, loss = 0.03385655\n",
      "Iteration 253, loss = 0.03371880\n",
      "Iteration 254, loss = 0.03363342\n",
      "Iteration 255, loss = 0.03362342\n",
      "Iteration 256, loss = 0.03349941\n",
      "Iteration 257, loss = 0.03343968\n",
      "Iteration 258, loss = 0.03335976\n",
      "Iteration 259, loss = 0.03332186\n",
      "Iteration 260, loss = 0.03323310\n",
      "Iteration 261, loss = 0.03317313\n",
      "Iteration 262, loss = 0.03310887\n",
      "Iteration 263, loss = 0.03302735\n",
      "Iteration 264, loss = 0.03297491\n",
      "Iteration 265, loss = 0.03291639\n",
      "Iteration 266, loss = 0.03283791\n",
      "Iteration 267, loss = 0.03278234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.59651808\n",
      "Iteration 2, loss = 0.48901551\n",
      "Iteration 3, loss = 0.41105052\n",
      "Iteration 4, loss = 0.35329566\n",
      "Iteration 5, loss = 0.31260340\n",
      "Iteration 6, loss = 0.28218253\n",
      "Iteration 7, loss = 0.25847756\n",
      "Iteration 8, loss = 0.24030109\n",
      "Iteration 9, loss = 0.22409066\n",
      "Iteration 10, loss = 0.21139526\n",
      "Iteration 11, loss = 0.20034709\n",
      "Iteration 12, loss = 0.19056927\n",
      "Iteration 13, loss = 0.18236420\n",
      "Iteration 14, loss = 0.17494399\n",
      "Iteration 15, loss = 0.16836967\n",
      "Iteration 16, loss = 0.16267771\n",
      "Iteration 17, loss = 0.15728629\n",
      "Iteration 18, loss = 0.15250577\n",
      "Iteration 19, loss = 0.14790436\n",
      "Iteration 20, loss = 0.14404447\n",
      "Iteration 21, loss = 0.14015976\n",
      "Iteration 22, loss = 0.13664023\n",
      "Iteration 23, loss = 0.13358785\n",
      "Iteration 24, loss = 0.13038253\n",
      "Iteration 25, loss = 0.12757178\n",
      "Iteration 26, loss = 0.12493425\n",
      "Iteration 27, loss = 0.12245607\n",
      "Iteration 28, loss = 0.12008289\n",
      "Iteration 29, loss = 0.11788834\n",
      "Iteration 30, loss = 0.11586116\n",
      "Iteration 31, loss = 0.11389715\n",
      "Iteration 32, loss = 0.11200578\n",
      "Iteration 33, loss = 0.11024946\n",
      "Iteration 34, loss = 0.10853532\n",
      "Iteration 35, loss = 0.10688792\n",
      "Iteration 36, loss = 0.10546244\n",
      "Iteration 37, loss = 0.10407404\n",
      "Iteration 38, loss = 0.10257486\n",
      "Iteration 39, loss = 0.10123092\n",
      "Iteration 40, loss = 0.10006883\n",
      "Iteration 41, loss = 0.09880754\n",
      "Iteration 42, loss = 0.09759691\n",
      "Iteration 43, loss = 0.09654764\n",
      "Iteration 44, loss = 0.09551406\n",
      "Iteration 45, loss = 0.09440596\n",
      "Iteration 46, loss = 0.09342276\n",
      "Iteration 47, loss = 0.09252860\n",
      "Iteration 48, loss = 0.09169226\n",
      "Iteration 49, loss = 0.09072602\n",
      "Iteration 50, loss = 0.08995585\n",
      "Iteration 51, loss = 0.08909051\n",
      "Iteration 52, loss = 0.08833052\n",
      "Iteration 53, loss = 0.08755796\n",
      "Iteration 54, loss = 0.08681691\n",
      "Iteration 55, loss = 0.08608075\n",
      "Iteration 56, loss = 0.08549908\n",
      "Iteration 57, loss = 0.08474719\n",
      "Iteration 58, loss = 0.08410173\n",
      "Iteration 59, loss = 0.08346223\n",
      "Iteration 60, loss = 0.08278163\n",
      "Iteration 61, loss = 0.08219966\n",
      "Iteration 62, loss = 0.08162804\n",
      "Iteration 63, loss = 0.08098765\n",
      "Iteration 64, loss = 0.08045989\n",
      "Iteration 65, loss = 0.07990183\n",
      "Iteration 66, loss = 0.07938451\n",
      "Iteration 67, loss = 0.07886486\n",
      "Iteration 68, loss = 0.07840217\n",
      "Iteration 69, loss = 0.07780208\n",
      "Iteration 70, loss = 0.07730928\n",
      "Iteration 71, loss = 0.07682933\n",
      "Iteration 72, loss = 0.07635082\n",
      "Iteration 73, loss = 0.07588945\n",
      "Iteration 74, loss = 0.07536565\n",
      "Iteration 75, loss = 0.07498140\n",
      "Iteration 76, loss = 0.07454360\n",
      "Iteration 77, loss = 0.07406472\n",
      "Iteration 78, loss = 0.07366536\n",
      "Iteration 79, loss = 0.07321410\n",
      "Iteration 80, loss = 0.07282698\n",
      "Iteration 81, loss = 0.07249426\n",
      "Iteration 82, loss = 0.07200448\n",
      "Iteration 83, loss = 0.07160470\n",
      "Iteration 84, loss = 0.07121614\n",
      "Iteration 85, loss = 0.07085473\n",
      "Iteration 86, loss = 0.07042965\n",
      "Iteration 87, loss = 0.07008621\n",
      "Iteration 88, loss = 0.06971825\n",
      "Iteration 89, loss = 0.06934156\n",
      "Iteration 90, loss = 0.06896914\n",
      "Iteration 91, loss = 0.06859428\n",
      "Iteration 92, loss = 0.06823470\n",
      "Iteration 93, loss = 0.06795557\n",
      "Iteration 94, loss = 0.06757502\n",
      "Iteration 95, loss = 0.06723572\n",
      "Iteration 96, loss = 0.06688904\n",
      "Iteration 97, loss = 0.06656269\n",
      "Iteration 98, loss = 0.06619657\n",
      "Iteration 99, loss = 0.06587490\n",
      "Iteration 100, loss = 0.06559209\n",
      "Iteration 101, loss = 0.06523864\n",
      "Iteration 102, loss = 0.06493340\n",
      "Iteration 103, loss = 0.06463445\n",
      "Iteration 104, loss = 0.06437658\n",
      "Iteration 105, loss = 0.06402791\n",
      "Iteration 106, loss = 0.06371807\n",
      "Iteration 107, loss = 0.06346291\n",
      "Iteration 108, loss = 0.06315468\n",
      "Iteration 109, loss = 0.06291520\n",
      "Iteration 110, loss = 0.06255996\n",
      "Iteration 111, loss = 0.06228254\n",
      "Iteration 112, loss = 0.06205724\n",
      "Iteration 113, loss = 0.06169074\n",
      "Iteration 114, loss = 0.06143946\n",
      "Iteration 115, loss = 0.06123962\n",
      "Iteration 116, loss = 0.06099313\n",
      "Iteration 117, loss = 0.06068351\n",
      "Iteration 118, loss = 0.06040281\n",
      "Iteration 119, loss = 0.06017869\n",
      "Iteration 120, loss = 0.05988241\n",
      "Iteration 121, loss = 0.05973236\n",
      "Iteration 122, loss = 0.05934079\n",
      "Iteration 123, loss = 0.05909131\n",
      "Iteration 124, loss = 0.05885102\n",
      "Iteration 125, loss = 0.05861802\n",
      "Iteration 126, loss = 0.05833264\n",
      "Iteration 127, loss = 0.05810226\n",
      "Iteration 128, loss = 0.05789993\n",
      "Iteration 129, loss = 0.05763852\n",
      "Iteration 130, loss = 0.05740692\n",
      "Iteration 131, loss = 0.05716550\n",
      "Iteration 132, loss = 0.05689368\n",
      "Iteration 133, loss = 0.05669829\n",
      "Iteration 134, loss = 0.05643951\n",
      "Iteration 135, loss = 0.05617026\n",
      "Iteration 136, loss = 0.05596542\n",
      "Iteration 137, loss = 0.05582419\n",
      "Iteration 138, loss = 0.05556554\n",
      "Iteration 139, loss = 0.05537201\n",
      "Iteration 140, loss = 0.05516555\n",
      "Iteration 141, loss = 0.05487387\n",
      "Iteration 142, loss = 0.05465407\n",
      "Iteration 143, loss = 0.05446039\n",
      "Iteration 144, loss = 0.05422634\n",
      "Iteration 145, loss = 0.05404984\n",
      "Iteration 146, loss = 0.05385818\n",
      "Iteration 147, loss = 0.05363290\n",
      "Iteration 148, loss = 0.05340603\n",
      "Iteration 149, loss = 0.05321537\n",
      "Iteration 150, loss = 0.05301502\n",
      "Iteration 151, loss = 0.05281930\n",
      "Iteration 152, loss = 0.05260847\n",
      "Iteration 153, loss = 0.05256879\n",
      "Iteration 154, loss = 0.05222314\n",
      "Iteration 155, loss = 0.05209118\n",
      "Iteration 156, loss = 0.05188188\n",
      "Iteration 157, loss = 0.05166951\n",
      "Iteration 158, loss = 0.05150790\n",
      "Iteration 159, loss = 0.05131615\n",
      "Iteration 160, loss = 0.05126580\n",
      "Iteration 161, loss = 0.05101338\n",
      "Iteration 162, loss = 0.05085395\n",
      "Iteration 163, loss = 0.05070351\n",
      "Iteration 164, loss = 0.05048740\n",
      "Iteration 165, loss = 0.05030842\n",
      "Iteration 166, loss = 0.05016485\n",
      "Iteration 167, loss = 0.04996490\n",
      "Iteration 168, loss = 0.04984142\n",
      "Iteration 169, loss = 0.04975134\n",
      "Iteration 170, loss = 0.04948711\n",
      "Iteration 171, loss = 0.04931546\n",
      "Iteration 172, loss = 0.04915834\n",
      "Iteration 173, loss = 0.04901824\n",
      "Iteration 174, loss = 0.04886839\n",
      "Iteration 175, loss = 0.04874234\n",
      "Iteration 176, loss = 0.04852888\n",
      "Iteration 177, loss = 0.04844432\n",
      "Iteration 178, loss = 0.04825016\n",
      "Iteration 179, loss = 0.04808308\n",
      "Iteration 180, loss = 0.04806903\n",
      "Iteration 181, loss = 0.04786637\n",
      "Iteration 182, loss = 0.04779302\n",
      "Iteration 183, loss = 0.04767787\n",
      "Iteration 184, loss = 0.04739417\n",
      "Iteration 185, loss = 0.04727219\n",
      "Iteration 186, loss = 0.04711181\n",
      "Iteration 187, loss = 0.04696708\n",
      "Iteration 188, loss = 0.04683426\n",
      "Iteration 189, loss = 0.04679539\n",
      "Iteration 190, loss = 0.04657826\n",
      "Iteration 191, loss = 0.04648643\n",
      "Iteration 192, loss = 0.04632575\n",
      "Iteration 193, loss = 0.04622278\n",
      "Iteration 194, loss = 0.04609183\n",
      "Iteration 195, loss = 0.04598805\n",
      "Iteration 196, loss = 0.04585059\n",
      "Iteration 197, loss = 0.04569623\n",
      "Iteration 198, loss = 0.04556588\n",
      "Iteration 199, loss = 0.04542612\n",
      "Iteration 200, loss = 0.04532641\n",
      "Iteration 201, loss = 0.04520286\n",
      "Iteration 202, loss = 0.04509158\n",
      "Iteration 203, loss = 0.04498220\n",
      "Iteration 204, loss = 0.04485426\n",
      "Iteration 205, loss = 0.04482296\n",
      "Iteration 206, loss = 0.04459463\n",
      "Iteration 207, loss = 0.04460830\n",
      "Iteration 208, loss = 0.04441955\n",
      "Iteration 209, loss = 0.04425416\n",
      "Iteration 210, loss = 0.04420339\n",
      "Iteration 211, loss = 0.04409167\n",
      "Iteration 212, loss = 0.04392962\n",
      "Iteration 213, loss = 0.04380590\n",
      "Iteration 214, loss = 0.04372158\n",
      "Iteration 215, loss = 0.04362775\n",
      "Iteration 216, loss = 0.04347947\n",
      "Iteration 217, loss = 0.04344409\n",
      "Iteration 218, loss = 0.04331949\n",
      "Iteration 219, loss = 0.04322953\n",
      "Iteration 220, loss = 0.04311495\n",
      "Iteration 221, loss = 0.04303131\n",
      "Iteration 222, loss = 0.04286057\n",
      "Iteration 223, loss = 0.04278208\n",
      "Iteration 224, loss = 0.04269557\n",
      "Iteration 225, loss = 0.04264086\n",
      "Iteration 226, loss = 0.04253832\n",
      "Iteration 227, loss = 0.04240656\n",
      "Iteration 228, loss = 0.04229765\n",
      "Iteration 229, loss = 0.04220483\n",
      "Iteration 230, loss = 0.04210293\n",
      "Iteration 231, loss = 0.04202764\n",
      "Iteration 232, loss = 0.04191583\n",
      "Iteration 233, loss = 0.04178148\n",
      "Iteration 234, loss = 0.04179740\n",
      "Iteration 235, loss = 0.04163102\n",
      "Iteration 236, loss = 0.04151836\n",
      "Iteration 237, loss = 0.04153915\n",
      "Iteration 238, loss = 0.04136326\n",
      "Iteration 239, loss = 0.04132797\n",
      "Iteration 240, loss = 0.04117114\n",
      "Iteration 241, loss = 0.04117822\n",
      "Iteration 242, loss = 0.04101451\n",
      "Iteration 243, loss = 0.04093284\n",
      "Iteration 244, loss = 0.04085001\n",
      "Iteration 245, loss = 0.04076203\n",
      "Iteration 246, loss = 0.04066967\n",
      "Iteration 247, loss = 0.04057499\n",
      "Iteration 248, loss = 0.04048113\n",
      "Iteration 249, loss = 0.04040832\n",
      "Iteration 250, loss = 0.04031859\n",
      "Iteration 251, loss = 0.04025370\n",
      "Iteration 252, loss = 0.04027942\n",
      "Iteration 253, loss = 0.04009086\n",
      "Iteration 254, loss = 0.04001511\n",
      "Iteration 255, loss = 0.03998548\n",
      "Iteration 256, loss = 0.03985697\n",
      "Iteration 257, loss = 0.03982313\n",
      "Iteration 258, loss = 0.03970145\n",
      "Iteration 259, loss = 0.03964792\n",
      "Iteration 260, loss = 0.03954996\n",
      "Iteration 261, loss = 0.03951766\n",
      "Iteration 262, loss = 0.03938154\n",
      "Iteration 263, loss = 0.03930403\n",
      "Iteration 264, loss = 0.03925798\n",
      "Iteration 265, loss = 0.03919860\n",
      "Iteration 266, loss = 0.03909440\n",
      "Iteration 267, loss = 0.03910741\n",
      "Iteration 268, loss = 0.03906391\n",
      "Iteration 269, loss = 0.03888232\n",
      "Iteration 270, loss = 0.03891546\n",
      "Iteration 271, loss = 0.03876831\n",
      "Iteration 272, loss = 0.03872359\n",
      "Iteration 273, loss = 0.03861359\n",
      "Iteration 274, loss = 0.03854991\n",
      "Iteration 275, loss = 0.03848827\n",
      "Iteration 276, loss = 0.03844106\n",
      "Iteration 277, loss = 0.03833402\n",
      "Iteration 278, loss = 0.03827534\n",
      "Iteration 279, loss = 0.03820946\n",
      "Iteration 280, loss = 0.03814693\n",
      "Iteration 281, loss = 0.03805780\n",
      "Iteration 282, loss = 0.03801818\n",
      "Iteration 283, loss = 0.03792614\n",
      "Iteration 284, loss = 0.03791085\n",
      "Iteration 285, loss = 0.03782029\n",
      "Iteration 286, loss = 0.03776821\n",
      "Iteration 287, loss = 0.03769871\n",
      "Iteration 288, loss = 0.03767935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.60175993\n",
      "Iteration 2, loss = 0.49412356\n",
      "Iteration 3, loss = 0.41547065\n",
      "Iteration 4, loss = 0.35839480\n",
      "Iteration 5, loss = 0.31698660\n",
      "Iteration 6, loss = 0.28677234\n",
      "Iteration 7, loss = 0.26298494\n",
      "Iteration 8, loss = 0.24479936\n",
      "Iteration 9, loss = 0.22895234\n",
      "Iteration 10, loss = 0.21596868\n",
      "Iteration 11, loss = 0.20505221\n",
      "Iteration 12, loss = 0.19554064\n",
      "Iteration 13, loss = 0.18714591\n",
      "Iteration 14, loss = 0.17969676\n",
      "Iteration 15, loss = 0.17320636\n",
      "Iteration 16, loss = 0.16754551\n",
      "Iteration 17, loss = 0.16193479\n",
      "Iteration 18, loss = 0.15740244\n",
      "Iteration 19, loss = 0.15267722\n",
      "Iteration 20, loss = 0.14869406\n",
      "Iteration 21, loss = 0.14477203\n",
      "Iteration 22, loss = 0.14129530\n",
      "Iteration 23, loss = 0.13808464\n",
      "Iteration 24, loss = 0.13491960\n",
      "Iteration 25, loss = 0.13196954\n",
      "Iteration 26, loss = 0.12929727\n",
      "Iteration 27, loss = 0.12672508\n",
      "Iteration 28, loss = 0.12432749\n",
      "Iteration 29, loss = 0.12204502\n",
      "Iteration 30, loss = 0.11999356\n",
      "Iteration 31, loss = 0.11796211\n",
      "Iteration 32, loss = 0.11601180\n",
      "Iteration 33, loss = 0.11424771\n",
      "Iteration 34, loss = 0.11244961\n",
      "Iteration 35, loss = 0.11077063\n",
      "Iteration 36, loss = 0.10932035\n",
      "Iteration 37, loss = 0.10787105\n",
      "Iteration 38, loss = 0.10635649\n",
      "Iteration 39, loss = 0.10501760\n",
      "Iteration 40, loss = 0.10379475\n",
      "Iteration 41, loss = 0.10256253\n",
      "Iteration 42, loss = 0.10131146\n",
      "Iteration 43, loss = 0.10021967\n",
      "Iteration 44, loss = 0.09917497\n",
      "Iteration 45, loss = 0.09803274\n",
      "Iteration 46, loss = 0.09703568\n",
      "Iteration 47, loss = 0.09607430\n",
      "Iteration 48, loss = 0.09520356\n",
      "Iteration 49, loss = 0.09417184\n",
      "Iteration 50, loss = 0.09337209\n",
      "Iteration 51, loss = 0.09244195\n",
      "Iteration 52, loss = 0.09163420\n",
      "Iteration 53, loss = 0.09081478\n",
      "Iteration 54, loss = 0.09008259\n",
      "Iteration 55, loss = 0.08928349\n",
      "Iteration 56, loss = 0.08866663\n",
      "Iteration 57, loss = 0.08784994\n",
      "Iteration 58, loss = 0.08718142\n",
      "Iteration 59, loss = 0.08651955\n",
      "Iteration 60, loss = 0.08579204\n",
      "Iteration 61, loss = 0.08517570\n",
      "Iteration 62, loss = 0.08458306\n",
      "Iteration 63, loss = 0.08391075\n",
      "Iteration 64, loss = 0.08334473\n",
      "Iteration 65, loss = 0.08277321\n",
      "Iteration 66, loss = 0.08223458\n",
      "Iteration 67, loss = 0.08165795\n",
      "Iteration 68, loss = 0.08113833\n",
      "Iteration 69, loss = 0.08051183\n",
      "Iteration 70, loss = 0.07998488\n",
      "Iteration 71, loss = 0.07946971\n",
      "Iteration 72, loss = 0.07891464\n",
      "Iteration 73, loss = 0.07841501\n",
      "Iteration 74, loss = 0.07787067\n",
      "Iteration 75, loss = 0.07743422\n",
      "Iteration 76, loss = 0.07697080\n",
      "Iteration 77, loss = 0.07645297\n",
      "Iteration 78, loss = 0.07600776\n",
      "Iteration 79, loss = 0.07554510\n",
      "Iteration 80, loss = 0.07509216\n",
      "Iteration 81, loss = 0.07477584\n",
      "Iteration 82, loss = 0.07423189\n",
      "Iteration 83, loss = 0.07379002\n",
      "Iteration 84, loss = 0.07338248\n",
      "Iteration 85, loss = 0.07304065\n",
      "Iteration 86, loss = 0.07254448\n",
      "Iteration 87, loss = 0.07217053\n",
      "Iteration 88, loss = 0.07178263\n",
      "Iteration 89, loss = 0.07137938\n",
      "Iteration 90, loss = 0.07096936\n",
      "Iteration 91, loss = 0.07059668\n",
      "Iteration 92, loss = 0.07021829\n",
      "Iteration 93, loss = 0.06989508\n",
      "Iteration 94, loss = 0.06950087\n",
      "Iteration 95, loss = 0.06913218\n",
      "Iteration 96, loss = 0.06876617\n",
      "Iteration 97, loss = 0.06841279\n",
      "Iteration 98, loss = 0.06804491\n",
      "Iteration 99, loss = 0.06767833\n",
      "Iteration 100, loss = 0.06739068\n",
      "Iteration 101, loss = 0.06700969\n",
      "Iteration 102, loss = 0.06667391\n",
      "Iteration 103, loss = 0.06634761\n",
      "Iteration 104, loss = 0.06607612\n",
      "Iteration 105, loss = 0.06570822\n",
      "Iteration 106, loss = 0.06537770\n",
      "Iteration 107, loss = 0.06510829\n",
      "Iteration 108, loss = 0.06477533\n",
      "Iteration 109, loss = 0.06450320\n",
      "Iteration 110, loss = 0.06411840\n",
      "Iteration 111, loss = 0.06383136\n",
      "Iteration 112, loss = 0.06358992\n",
      "Iteration 113, loss = 0.06319436\n",
      "Iteration 114, loss = 0.06291867\n",
      "Iteration 115, loss = 0.06268774\n",
      "Iteration 116, loss = 0.06241801\n",
      "Iteration 117, loss = 0.06211076\n",
      "Iteration 118, loss = 0.06182227\n",
      "Iteration 119, loss = 0.06153874\n",
      "Iteration 120, loss = 0.06124537\n",
      "Iteration 121, loss = 0.06106554\n",
      "Iteration 122, loss = 0.06067067\n",
      "Iteration 123, loss = 0.06040463\n",
      "Iteration 124, loss = 0.06013268\n",
      "Iteration 125, loss = 0.05988461\n",
      "Iteration 126, loss = 0.05958180\n",
      "Iteration 127, loss = 0.05934030\n",
      "Iteration 128, loss = 0.05910018\n",
      "Iteration 129, loss = 0.05882992\n",
      "Iteration 130, loss = 0.05856251\n",
      "Iteration 131, loss = 0.05831334\n",
      "Iteration 132, loss = 0.05803429\n",
      "Iteration 133, loss = 0.05781399\n",
      "Iteration 134, loss = 0.05756486\n",
      "Iteration 135, loss = 0.05726898\n",
      "Iteration 136, loss = 0.05706741\n",
      "Iteration 137, loss = 0.05689093\n",
      "Iteration 138, loss = 0.05663138\n",
      "Iteration 139, loss = 0.05646487\n",
      "Iteration 140, loss = 0.05621085\n",
      "Iteration 141, loss = 0.05590883\n",
      "Iteration 142, loss = 0.05567605\n",
      "Iteration 143, loss = 0.05550532\n",
      "Iteration 144, loss = 0.05521930\n",
      "Iteration 145, loss = 0.05503011\n",
      "Iteration 146, loss = 0.05487180\n",
      "Iteration 147, loss = 0.05462497\n",
      "Iteration 148, loss = 0.05438940\n",
      "Iteration 149, loss = 0.05418394\n",
      "Iteration 150, loss = 0.05396822\n",
      "Iteration 151, loss = 0.05374966\n",
      "Iteration 152, loss = 0.05353506\n",
      "Iteration 153, loss = 0.05345953\n",
      "Iteration 154, loss = 0.05310752\n",
      "Iteration 155, loss = 0.05297389\n",
      "Iteration 156, loss = 0.05275848\n",
      "Iteration 157, loss = 0.05252434\n",
      "Iteration 158, loss = 0.05234350\n",
      "Iteration 159, loss = 0.05213553\n",
      "Iteration 160, loss = 0.05207219\n",
      "Iteration 161, loss = 0.05181134\n",
      "Iteration 162, loss = 0.05162265\n",
      "Iteration 163, loss = 0.05145886\n",
      "Iteration 164, loss = 0.05125448\n",
      "Iteration 165, loss = 0.05104291\n",
      "Iteration 166, loss = 0.05088123\n",
      "Iteration 167, loss = 0.05065932\n",
      "Iteration 168, loss = 0.05053197\n",
      "Iteration 169, loss = 0.05039896\n",
      "Iteration 170, loss = 0.05014957\n",
      "Iteration 171, loss = 0.04997133\n",
      "Iteration 172, loss = 0.04978924\n",
      "Iteration 173, loss = 0.04964027\n",
      "Iteration 174, loss = 0.04948474\n",
      "Iteration 175, loss = 0.04935895\n",
      "Iteration 176, loss = 0.04912303\n",
      "Iteration 177, loss = 0.04901789\n",
      "Iteration 178, loss = 0.04883233\n",
      "Iteration 179, loss = 0.04865366\n",
      "Iteration 180, loss = 0.04862348\n",
      "Iteration 181, loss = 0.04840702\n",
      "Iteration 182, loss = 0.04831703\n",
      "Iteration 183, loss = 0.04820795\n",
      "Iteration 184, loss = 0.04792164\n",
      "Iteration 185, loss = 0.04779318\n",
      "Iteration 186, loss = 0.04762350\n",
      "Iteration 187, loss = 0.04747050\n",
      "Iteration 188, loss = 0.04730548\n",
      "Iteration 189, loss = 0.04728588\n",
      "Iteration 190, loss = 0.04704486\n",
      "Iteration 191, loss = 0.04693738\n",
      "Iteration 192, loss = 0.04678604\n",
      "Iteration 193, loss = 0.04664982\n",
      "Iteration 194, loss = 0.04651067\n",
      "Iteration 195, loss = 0.04640664\n",
      "Iteration 196, loss = 0.04625836\n",
      "Iteration 197, loss = 0.04609311\n",
      "Iteration 198, loss = 0.04597804\n",
      "Iteration 199, loss = 0.04582425\n",
      "Iteration 200, loss = 0.04570960\n",
      "Iteration 201, loss = 0.04557881\n",
      "Iteration 202, loss = 0.04549201\n",
      "Iteration 203, loss = 0.04534180\n",
      "Iteration 204, loss = 0.04520177\n",
      "Iteration 205, loss = 0.04512355\n",
      "Iteration 206, loss = 0.04493468\n",
      "Iteration 207, loss = 0.04496522\n",
      "Iteration 208, loss = 0.04476366\n",
      "Iteration 209, loss = 0.04459983\n",
      "Iteration 210, loss = 0.04453873\n",
      "Iteration 211, loss = 0.04442630\n",
      "Iteration 212, loss = 0.04424978\n",
      "Iteration 213, loss = 0.04412167\n",
      "Iteration 214, loss = 0.04402632\n",
      "Iteration 215, loss = 0.04394134\n",
      "Iteration 216, loss = 0.04379745\n",
      "Iteration 217, loss = 0.04372018\n",
      "Iteration 218, loss = 0.04359853\n",
      "Iteration 219, loss = 0.04352659\n",
      "Iteration 220, loss = 0.04338250\n",
      "Iteration 221, loss = 0.04328649\n",
      "Iteration 222, loss = 0.04313886\n",
      "Iteration 223, loss = 0.04305934\n",
      "Iteration 224, loss = 0.04295241\n",
      "Iteration 225, loss = 0.04290623\n",
      "Iteration 226, loss = 0.04278796\n",
      "Iteration 227, loss = 0.04266767\n",
      "Iteration 228, loss = 0.04254127\n",
      "Iteration 229, loss = 0.04244039\n",
      "Iteration 230, loss = 0.04237201\n",
      "Iteration 231, loss = 0.04225332\n",
      "Iteration 232, loss = 0.04214561\n",
      "Iteration 233, loss = 0.04201176\n",
      "Iteration 234, loss = 0.04201095\n",
      "Iteration 235, loss = 0.04184770\n",
      "Iteration 236, loss = 0.04174304\n",
      "Iteration 237, loss = 0.04176501\n",
      "Iteration 238, loss = 0.04156831\n",
      "Iteration 239, loss = 0.04151532\n",
      "Iteration 240, loss = 0.04136562\n",
      "Iteration 241, loss = 0.04139355\n",
      "Iteration 242, loss = 0.04123751\n",
      "Iteration 243, loss = 0.04111068\n",
      "Iteration 244, loss = 0.04105621\n",
      "Iteration 245, loss = 0.04094284\n",
      "Iteration 246, loss = 0.04084835\n",
      "Iteration 247, loss = 0.04074468\n",
      "Iteration 248, loss = 0.04066819\n",
      "Iteration 249, loss = 0.04057809\n",
      "Iteration 250, loss = 0.04047948\n",
      "Iteration 251, loss = 0.04040854\n",
      "Iteration 252, loss = 0.04043926\n",
      "Iteration 253, loss = 0.04025578\n",
      "Iteration 254, loss = 0.04015193\n",
      "Iteration 255, loss = 0.04013797\n",
      "Iteration 256, loss = 0.03999785\n",
      "Iteration 257, loss = 0.03994769\n",
      "Iteration 258, loss = 0.03983887\n",
      "Iteration 259, loss = 0.03980161\n",
      "Iteration 260, loss = 0.03967279\n",
      "Iteration 261, loss = 0.03962600\n",
      "Iteration 262, loss = 0.03950335\n",
      "Iteration 263, loss = 0.03941598\n",
      "Iteration 264, loss = 0.03936058\n",
      "Iteration 265, loss = 0.03927807\n",
      "Iteration 266, loss = 0.03919139\n",
      "Iteration 267, loss = 0.03920153\n",
      "Iteration 268, loss = 0.03912661\n",
      "Iteration 269, loss = 0.03896475\n",
      "Iteration 270, loss = 0.03898119\n",
      "Iteration 271, loss = 0.03884342\n",
      "Iteration 272, loss = 0.03878795\n",
      "Iteration 273, loss = 0.03868119\n",
      "Iteration 274, loss = 0.03862561\n",
      "Iteration 275, loss = 0.03855263\n",
      "Iteration 276, loss = 0.03850342\n",
      "Iteration 277, loss = 0.03839461\n",
      "Iteration 278, loss = 0.03834001\n",
      "Iteration 279, loss = 0.03826074\n",
      "Iteration 280, loss = 0.03820943\n",
      "Iteration 281, loss = 0.03812826\n",
      "Iteration 282, loss = 0.03808612\n",
      "Iteration 283, loss = 0.03798169\n",
      "Iteration 284, loss = 0.03796994\n",
      "Iteration 285, loss = 0.03789505\n",
      "Iteration 286, loss = 0.03782199\n",
      "Iteration 287, loss = 0.03774926\n",
      "Iteration 288, loss = 0.03774965\n",
      "Iteration 289, loss = 0.03761600\n",
      "Iteration 290, loss = 0.03755868\n",
      "Iteration 291, loss = 0.03747204\n",
      "Iteration 292, loss = 0.03740843\n",
      "Iteration 293, loss = 0.03734145\n",
      "Iteration 294, loss = 0.03729366\n",
      "Iteration 295, loss = 0.03723615\n",
      "Iteration 296, loss = 0.03714128\n",
      "Iteration 297, loss = 0.03715130\n",
      "Iteration 298, loss = 0.03704867\n",
      "Iteration 299, loss = 0.03700389\n",
      "Iteration 300, loss = 0.03701253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.60125519\n",
      "Iteration 2, loss = 0.49495328\n",
      "Iteration 3, loss = 0.41626842\n",
      "Iteration 4, loss = 0.35965714\n",
      "Iteration 5, loss = 0.31820757\n",
      "Iteration 6, loss = 0.28792419\n",
      "Iteration 7, loss = 0.26414791\n",
      "Iteration 8, loss = 0.24546030\n",
      "Iteration 9, loss = 0.22962603\n",
      "Iteration 10, loss = 0.21628866\n",
      "Iteration 11, loss = 0.20504732\n",
      "Iteration 12, loss = 0.19559049\n",
      "Iteration 13, loss = 0.18701827\n",
      "Iteration 14, loss = 0.17927103\n",
      "Iteration 15, loss = 0.17282421\n",
      "Iteration 16, loss = 0.16710704\n",
      "Iteration 17, loss = 0.16165784\n",
      "Iteration 18, loss = 0.15676510\n",
      "Iteration 19, loss = 0.15230839\n",
      "Iteration 20, loss = 0.14830702\n",
      "Iteration 21, loss = 0.14434406\n",
      "Iteration 22, loss = 0.14096930\n",
      "Iteration 23, loss = 0.13779002\n",
      "Iteration 24, loss = 0.13477573\n",
      "Iteration 25, loss = 0.13187815\n",
      "Iteration 26, loss = 0.12920826\n",
      "Iteration 27, loss = 0.12667005\n",
      "Iteration 28, loss = 0.12437715\n",
      "Iteration 29, loss = 0.12217840\n",
      "Iteration 30, loss = 0.12015672\n",
      "Iteration 31, loss = 0.11816262\n",
      "Iteration 32, loss = 0.11626169\n",
      "Iteration 33, loss = 0.11447593\n",
      "Iteration 34, loss = 0.11275615\n",
      "Iteration 35, loss = 0.11103755\n",
      "Iteration 36, loss = 0.10957123\n",
      "Iteration 37, loss = 0.10815253\n",
      "Iteration 38, loss = 0.10665842\n",
      "Iteration 39, loss = 0.10530433\n",
      "Iteration 40, loss = 0.10410391\n",
      "Iteration 41, loss = 0.10281919\n",
      "Iteration 42, loss = 0.10158748\n",
      "Iteration 43, loss = 0.10047484\n",
      "Iteration 44, loss = 0.09939695\n",
      "Iteration 45, loss = 0.09826376\n",
      "Iteration 46, loss = 0.09727456\n",
      "Iteration 47, loss = 0.09630739\n",
      "Iteration 48, loss = 0.09540681\n",
      "Iteration 49, loss = 0.09440774\n",
      "Iteration 50, loss = 0.09360201\n",
      "Iteration 51, loss = 0.09272240\n",
      "Iteration 52, loss = 0.09188487\n",
      "Iteration 53, loss = 0.09106619\n",
      "Iteration 54, loss = 0.09032043\n",
      "Iteration 55, loss = 0.08951983\n",
      "Iteration 56, loss = 0.08888552\n",
      "Iteration 57, loss = 0.08806559\n",
      "Iteration 58, loss = 0.08737414\n",
      "Iteration 59, loss = 0.08668733\n",
      "Iteration 60, loss = 0.08593643\n",
      "Iteration 61, loss = 0.08531453\n",
      "Iteration 62, loss = 0.08470750\n",
      "Iteration 63, loss = 0.08401009\n",
      "Iteration 64, loss = 0.08343198\n",
      "Iteration 65, loss = 0.08286002\n",
      "Iteration 66, loss = 0.08229793\n",
      "Iteration 67, loss = 0.08171626\n",
      "Iteration 68, loss = 0.08121940\n",
      "Iteration 69, loss = 0.08056385\n",
      "Iteration 70, loss = 0.08009361\n",
      "Iteration 71, loss = 0.07955947\n",
      "Iteration 72, loss = 0.07900967\n",
      "Iteration 73, loss = 0.07850208\n",
      "Iteration 74, loss = 0.07796154\n",
      "Iteration 75, loss = 0.07753063\n",
      "Iteration 76, loss = 0.07707430\n",
      "Iteration 77, loss = 0.07656720\n",
      "Iteration 78, loss = 0.07614912\n",
      "Iteration 79, loss = 0.07566123\n",
      "Iteration 80, loss = 0.07522410\n",
      "Iteration 81, loss = 0.07491884\n",
      "Iteration 82, loss = 0.07437296\n",
      "Iteration 83, loss = 0.07394143\n",
      "Iteration 84, loss = 0.07353159\n",
      "Iteration 85, loss = 0.07319183\n",
      "Iteration 86, loss = 0.07267506\n",
      "Iteration 87, loss = 0.07230684\n",
      "Iteration 88, loss = 0.07195386\n",
      "Iteration 89, loss = 0.07152703\n",
      "Iteration 90, loss = 0.07113040\n",
      "Iteration 91, loss = 0.07075088\n",
      "Iteration 92, loss = 0.07037122\n",
      "Iteration 93, loss = 0.07006339\n",
      "Iteration 94, loss = 0.06966749\n",
      "Iteration 95, loss = 0.06931114\n",
      "Iteration 96, loss = 0.06894311\n",
      "Iteration 97, loss = 0.06858438\n",
      "Iteration 98, loss = 0.06821133\n",
      "Iteration 99, loss = 0.06785926\n",
      "Iteration 100, loss = 0.06755864\n",
      "Iteration 101, loss = 0.06718595\n",
      "Iteration 102, loss = 0.06685560\n",
      "Iteration 103, loss = 0.06652073\n",
      "Iteration 104, loss = 0.06624358\n",
      "Iteration 105, loss = 0.06588178\n",
      "Iteration 106, loss = 0.06554081\n",
      "Iteration 107, loss = 0.06528566\n",
      "Iteration 108, loss = 0.06493073\n",
      "Iteration 109, loss = 0.06467536\n",
      "Iteration 110, loss = 0.06428706\n",
      "Iteration 111, loss = 0.06401229\n",
      "Iteration 112, loss = 0.06375701\n",
      "Iteration 113, loss = 0.06337928\n",
      "Iteration 114, loss = 0.06309320\n",
      "Iteration 115, loss = 0.06287465\n",
      "Iteration 116, loss = 0.06260303\n",
      "Iteration 117, loss = 0.06227968\n",
      "Iteration 118, loss = 0.06197412\n",
      "Iteration 119, loss = 0.06169401\n",
      "Iteration 120, loss = 0.06142303\n",
      "Iteration 121, loss = 0.06123356\n",
      "Iteration 122, loss = 0.06083071\n",
      "Iteration 123, loss = 0.06056179\n",
      "Iteration 124, loss = 0.06030106\n",
      "Iteration 125, loss = 0.06004964\n",
      "Iteration 126, loss = 0.05976959\n",
      "Iteration 127, loss = 0.05950523\n",
      "Iteration 128, loss = 0.05926453\n",
      "Iteration 129, loss = 0.05902265\n",
      "Iteration 130, loss = 0.05873983\n",
      "Iteration 131, loss = 0.05850099\n",
      "Iteration 132, loss = 0.05822823\n",
      "Iteration 133, loss = 0.05800081\n",
      "Iteration 134, loss = 0.05775771\n",
      "Iteration 135, loss = 0.05746868\n",
      "Iteration 136, loss = 0.05725058\n",
      "Iteration 137, loss = 0.05706233\n",
      "Iteration 138, loss = 0.05683278\n",
      "Iteration 139, loss = 0.05668010\n",
      "Iteration 140, loss = 0.05643980\n",
      "Iteration 141, loss = 0.05613289\n",
      "Iteration 142, loss = 0.05589891\n",
      "Iteration 143, loss = 0.05573877\n",
      "Iteration 144, loss = 0.05545500\n",
      "Iteration 145, loss = 0.05526934\n",
      "Iteration 146, loss = 0.05513619\n",
      "Iteration 147, loss = 0.05485553\n",
      "Iteration 148, loss = 0.05461587\n",
      "Iteration 149, loss = 0.05442642\n",
      "Iteration 150, loss = 0.05421191\n",
      "Iteration 151, loss = 0.05399308\n",
      "Iteration 152, loss = 0.05378016\n",
      "Iteration 153, loss = 0.05373826\n",
      "Iteration 154, loss = 0.05337641\n",
      "Iteration 155, loss = 0.05323456\n",
      "Iteration 156, loss = 0.05302018\n",
      "Iteration 157, loss = 0.05279778\n",
      "Iteration 158, loss = 0.05262280\n",
      "Iteration 159, loss = 0.05242652\n",
      "Iteration 160, loss = 0.05235615\n",
      "Iteration 161, loss = 0.05210392\n",
      "Iteration 162, loss = 0.05191533\n",
      "Iteration 163, loss = 0.05174344\n",
      "Iteration 164, loss = 0.05152837\n",
      "Iteration 165, loss = 0.05132997\n",
      "Iteration 166, loss = 0.05117459\n",
      "Iteration 167, loss = 0.05095873\n",
      "Iteration 168, loss = 0.05082491\n",
      "Iteration 169, loss = 0.05069400\n",
      "Iteration 170, loss = 0.05044667\n",
      "Iteration 171, loss = 0.05026871\n",
      "Iteration 172, loss = 0.05007850\n",
      "Iteration 173, loss = 0.04993769\n",
      "Iteration 174, loss = 0.04977288\n",
      "Iteration 175, loss = 0.04965169\n",
      "Iteration 176, loss = 0.04941806\n",
      "Iteration 177, loss = 0.04932005\n",
      "Iteration 178, loss = 0.04912186\n",
      "Iteration 179, loss = 0.04894679\n",
      "Iteration 180, loss = 0.04891717\n",
      "Iteration 181, loss = 0.04869155\n",
      "Iteration 182, loss = 0.04860498\n",
      "Iteration 183, loss = 0.04848470\n",
      "Iteration 184, loss = 0.04820277\n",
      "Iteration 185, loss = 0.04808717\n",
      "Iteration 186, loss = 0.04790272\n",
      "Iteration 187, loss = 0.04777898\n",
      "Iteration 188, loss = 0.04760116\n",
      "Iteration 189, loss = 0.04756065\n",
      "Iteration 190, loss = 0.04734096\n",
      "Iteration 191, loss = 0.04724380\n",
      "Iteration 192, loss = 0.04708213\n",
      "Iteration 193, loss = 0.04695091\n",
      "Iteration 194, loss = 0.04681848\n",
      "Iteration 195, loss = 0.04668743\n",
      "Iteration 196, loss = 0.04655863\n",
      "Iteration 197, loss = 0.04640980\n",
      "Iteration 198, loss = 0.04627718\n",
      "Iteration 199, loss = 0.04612816\n",
      "Iteration 200, loss = 0.04601581\n",
      "Iteration 201, loss = 0.04588469\n",
      "Iteration 202, loss = 0.04578178\n",
      "Iteration 203, loss = 0.04563460\n",
      "Iteration 204, loss = 0.04551027\n",
      "Iteration 205, loss = 0.04541602\n",
      "Iteration 206, loss = 0.04523550\n",
      "Iteration 207, loss = 0.04525860\n",
      "Iteration 208, loss = 0.04508256\n",
      "Iteration 209, loss = 0.04490325\n",
      "Iteration 210, loss = 0.04485507\n",
      "Iteration 211, loss = 0.04473877\n",
      "Iteration 212, loss = 0.04455571\n",
      "Iteration 213, loss = 0.04443656\n",
      "Iteration 214, loss = 0.04432024\n",
      "Iteration 215, loss = 0.04424084\n",
      "Iteration 216, loss = 0.04409482\n",
      "Iteration 217, loss = 0.04403543\n",
      "Iteration 218, loss = 0.04390885\n",
      "Iteration 219, loss = 0.04382299\n",
      "Iteration 220, loss = 0.04368111\n",
      "Iteration 221, loss = 0.04359057\n",
      "Iteration 222, loss = 0.04344558\n",
      "Iteration 223, loss = 0.04336482\n",
      "Iteration 224, loss = 0.04327610\n",
      "Iteration 225, loss = 0.04321576\n",
      "Iteration 226, loss = 0.04311608\n",
      "Iteration 227, loss = 0.04295674\n",
      "Iteration 228, loss = 0.04283659\n",
      "Iteration 229, loss = 0.04275650\n",
      "Iteration 230, loss = 0.04269665\n",
      "Iteration 231, loss = 0.04258151\n",
      "Iteration 232, loss = 0.04246793\n",
      "Iteration 233, loss = 0.04234106\n",
      "Iteration 234, loss = 0.04234566\n",
      "Iteration 235, loss = 0.04216585\n",
      "Iteration 236, loss = 0.04206264\n",
      "Iteration 237, loss = 0.04209247\n",
      "Iteration 238, loss = 0.04191931\n",
      "Iteration 239, loss = 0.04184107\n",
      "Iteration 240, loss = 0.04171001\n",
      "Iteration 241, loss = 0.04175157\n",
      "Iteration 242, loss = 0.04157921\n",
      "Iteration 243, loss = 0.04144823\n",
      "Iteration 244, loss = 0.04139732\n",
      "Iteration 245, loss = 0.04129692\n",
      "Iteration 246, loss = 0.04120182\n",
      "Iteration 247, loss = 0.04110109\n",
      "Iteration 248, loss = 0.04102197\n",
      "Iteration 249, loss = 0.04092812\n",
      "Iteration 250, loss = 0.04083365\n",
      "Iteration 251, loss = 0.04075975\n",
      "Iteration 252, loss = 0.04079567\n",
      "Iteration 253, loss = 0.04062689\n",
      "Iteration 254, loss = 0.04051609\n",
      "Iteration 255, loss = 0.04050306\n",
      "Iteration 256, loss = 0.04035636\n",
      "Iteration 257, loss = 0.04031867\n",
      "Iteration 258, loss = 0.04020617\n",
      "Iteration 259, loss = 0.04014275\n",
      "Iteration 260, loss = 0.04004431\n",
      "Iteration 261, loss = 0.03999568\n",
      "Iteration 262, loss = 0.03986531\n",
      "Iteration 263, loss = 0.03978863\n",
      "Iteration 264, loss = 0.03972891\n",
      "Iteration 265, loss = 0.03967459\n",
      "Iteration 266, loss = 0.03956759\n",
      "Iteration 267, loss = 0.03959969\n",
      "Iteration 268, loss = 0.03952486\n",
      "Iteration 269, loss = 0.03935306\n",
      "Iteration 270, loss = 0.03938374\n",
      "Iteration 271, loss = 0.03923985\n",
      "Iteration 272, loss = 0.03918300\n",
      "Iteration 273, loss = 0.03907492\n",
      "Iteration 274, loss = 0.03902239\n",
      "Iteration 275, loss = 0.03894541\n",
      "Iteration 276, loss = 0.03889192\n",
      "Iteration 277, loss = 0.03879815\n",
      "Iteration 278, loss = 0.03873406\n",
      "Iteration 279, loss = 0.03864824\n",
      "Iteration 280, loss = 0.03860644\n",
      "Iteration 281, loss = 0.03852226\n",
      "Iteration 282, loss = 0.03847066\n",
      "Iteration 283, loss = 0.03838724\n",
      "Iteration 284, loss = 0.03836209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.pdf\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "# https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(60,)] , # [(60,),(100,),(60,10),(30,30,)]\n",
    "    'mlp__activation' : ['relu'], #['logistic','tanh','relu']\n",
    "    'mlp__solver' : ['adam'],\n",
    "    'mlp__alpha' : [0.1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1 ,*0.1\n",
    "    'mlp__batch_size' : [99], # ~297 samples for train at inner cv -> 3 equal size batches , #[16,32,64,128,'auto']\n",
    "    'mlp__learning_rate' : ['constant'],\n",
    "    'mlp__learning_rate_init' : [0.01,0.001],\n",
    "    'mlp__max_iter' : [1000],\n",
    "    'mlp__verbose' : [1]\n",
    "}\n",
    "\n",
    "search_adam = GridSearchCV(clf_mlp, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_adam, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.981     0.981     0.981       212\n",
      "   Malignant      0.981     0.981     0.981       212\n",
      "\n",
      "    accuracy                          0.981       424\n",
      "   macro avg      0.981     0.981     0.981       424\n",
      "weighted avg      0.981     0.981     0.981       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.981     0.981     0.981       212\n",
      "   Malignant      0.981     0.981     0.981       212\n",
      "\n",
      "    accuracy                          0.981       424\n",
      "   macro avg      0.981     0.981     0.981       424\n",
      "weighted avg      0.981     0.981     0.981       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_mlp_adam =  Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                ('mlp',MLPClassifier(alpha=0.1, batch_size=99,\n",
    "                               hidden_layer_sizes=(60,),\n",
    "                               learning_rate_init=0.001, max_iter=1000,\n",
    "                               random_state=13, verbose=0))])\n",
    "\n",
    "score = cross_val_score(clf_mlp_adam, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Solver : SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72968737\n",
      "Iteration 2, loss = 0.68035652\n",
      "Iteration 3, loss = 0.64010814\n",
      "Iteration 4, loss = 0.60682335\n",
      "Iteration 5, loss = 0.57806610\n",
      "Iteration 6, loss = 0.55328237\n",
      "Iteration 7, loss = 0.53161320\n",
      "Iteration 8, loss = 0.51186994\n",
      "Iteration 9, loss = 0.49413932\n",
      "Iteration 10, loss = 0.47801183\n",
      "Iteration 11, loss = 0.46319827\n",
      "Iteration 12, loss = 0.44958810\n",
      "Iteration 13, loss = 0.43715597\n",
      "Iteration 14, loss = 0.42552929\n",
      "Iteration 15, loss = 0.41471746\n",
      "Iteration 16, loss = 0.40454138\n",
      "Iteration 17, loss = 0.39510432\n",
      "Iteration 18, loss = 0.38603232\n",
      "Iteration 19, loss = 0.37755356\n",
      "Iteration 20, loss = 0.36940735\n",
      "Iteration 21, loss = 0.36182183\n",
      "Iteration 22, loss = 0.35437046\n",
      "Iteration 23, loss = 0.34737844\n",
      "Iteration 24, loss = 0.34078485\n",
      "Iteration 25, loss = 0.33451902\n",
      "Iteration 26, loss = 0.32852069\n",
      "Iteration 27, loss = 0.32272274\n",
      "Iteration 28, loss = 0.31705110\n",
      "Iteration 29, loss = 0.31165403\n",
      "Iteration 30, loss = 0.30659839\n",
      "Iteration 31, loss = 0.30168266\n",
      "Iteration 32, loss = 0.29696682\n",
      "Iteration 33, loss = 0.29240619\n",
      "Iteration 34, loss = 0.28806819\n",
      "Iteration 35, loss = 0.28376979\n",
      "Iteration 36, loss = 0.27982192\n",
      "Iteration 37, loss = 0.27591222\n",
      "Iteration 38, loss = 0.27219673\n",
      "Iteration 39, loss = 0.26854120\n",
      "Iteration 40, loss = 0.26501679\n",
      "Iteration 41, loss = 0.26167831\n",
      "Iteration 42, loss = 0.25839716\n",
      "Iteration 43, loss = 0.25529150\n",
      "Iteration 44, loss = 0.25237231\n",
      "Iteration 45, loss = 0.24934859\n",
      "Iteration 46, loss = 0.24652549\n",
      "Iteration 47, loss = 0.24371846\n",
      "Iteration 48, loss = 0.24107436\n",
      "Iteration 49, loss = 0.23845762\n",
      "Iteration 50, loss = 0.23597417\n",
      "Iteration 51, loss = 0.23346330\n",
      "Iteration 52, loss = 0.23104899\n",
      "Iteration 53, loss = 0.22874571\n",
      "Iteration 54, loss = 0.22651461\n",
      "Iteration 55, loss = 0.22434684\n",
      "Iteration 56, loss = 0.22220376\n",
      "Iteration 57, loss = 0.22014975\n",
      "Iteration 58, loss = 0.21815398\n",
      "Iteration 59, loss = 0.21621833\n",
      "Iteration 60, loss = 0.21426433\n",
      "Iteration 61, loss = 0.21241428\n",
      "Iteration 62, loss = 0.21059165\n",
      "Iteration 63, loss = 0.20887564\n",
      "Iteration 64, loss = 0.20712886\n",
      "Iteration 65, loss = 0.20548422\n",
      "Iteration 66, loss = 0.20378405\n",
      "Iteration 67, loss = 0.20213170\n",
      "Iteration 68, loss = 0.20054437\n",
      "Iteration 69, loss = 0.19915442\n",
      "Iteration 70, loss = 0.19756399\n",
      "Iteration 71, loss = 0.19610950\n",
      "Iteration 72, loss = 0.19463925\n",
      "Iteration 73, loss = 0.19319127\n",
      "Iteration 74, loss = 0.19187313\n",
      "Iteration 75, loss = 0.19052126\n",
      "Iteration 76, loss = 0.18916704\n",
      "Iteration 77, loss = 0.18786784\n",
      "Iteration 78, loss = 0.18663133\n",
      "Iteration 79, loss = 0.18536189\n",
      "Iteration 80, loss = 0.18423843\n",
      "Iteration 81, loss = 0.18298308\n",
      "Iteration 82, loss = 0.18185208\n",
      "Iteration 83, loss = 0.18070934\n",
      "Iteration 84, loss = 0.17962278\n",
      "Iteration 85, loss = 0.17852723\n",
      "Iteration 86, loss = 0.17747196\n",
      "Iteration 87, loss = 0.17641989\n",
      "Iteration 88, loss = 0.17547069\n",
      "Iteration 89, loss = 0.17451064\n",
      "Iteration 90, loss = 0.17348129\n",
      "Iteration 91, loss = 0.17256189\n",
      "Iteration 92, loss = 0.17165186\n",
      "Iteration 93, loss = 0.17071361\n",
      "Iteration 94, loss = 0.16977580\n",
      "Iteration 95, loss = 0.16890782\n",
      "Iteration 96, loss = 0.16805106\n",
      "Iteration 97, loss = 0.16716867\n",
      "Iteration 98, loss = 0.16639124\n",
      "Iteration 99, loss = 0.16552073\n",
      "Iteration 100, loss = 0.16471278\n",
      "Iteration 101, loss = 0.16388744\n",
      "Iteration 102, loss = 0.16311496\n",
      "Iteration 103, loss = 0.16232572\n",
      "Iteration 104, loss = 0.16160408\n",
      "Iteration 105, loss = 0.16078978\n",
      "Iteration 106, loss = 0.16006035\n",
      "Iteration 107, loss = 0.15934287\n",
      "Iteration 108, loss = 0.15863720\n",
      "Iteration 109, loss = 0.15795386\n",
      "Iteration 110, loss = 0.15723385\n",
      "Iteration 111, loss = 0.15659987\n",
      "Iteration 112, loss = 0.15587944\n",
      "Iteration 113, loss = 0.15522890\n",
      "Iteration 114, loss = 0.15459176\n",
      "Iteration 115, loss = 0.15394220\n",
      "Iteration 116, loss = 0.15332480\n",
      "Iteration 117, loss = 0.15273741\n",
      "Iteration 118, loss = 0.15209232\n",
      "Iteration 119, loss = 0.15148784\n",
      "Iteration 120, loss = 0.15089997\n",
      "Iteration 121, loss = 0.15032098\n",
      "Iteration 122, loss = 0.14979273\n",
      "Iteration 123, loss = 0.14919683\n",
      "Iteration 124, loss = 0.14866276\n",
      "Iteration 125, loss = 0.14811177\n",
      "Iteration 126, loss = 0.14757978\n",
      "Iteration 127, loss = 0.14705237\n",
      "Iteration 128, loss = 0.14652566\n",
      "Iteration 129, loss = 0.14604480\n",
      "Iteration 130, loss = 0.14548757\n",
      "Iteration 131, loss = 0.14502181\n",
      "Iteration 132, loss = 0.14452037\n",
      "Iteration 133, loss = 0.14403426\n",
      "Iteration 134, loss = 0.14351844\n",
      "Iteration 135, loss = 0.14304610\n",
      "Iteration 136, loss = 0.14262576\n",
      "Iteration 137, loss = 0.14207260\n",
      "Iteration 138, loss = 0.14160211\n",
      "Iteration 139, loss = 0.14113277\n",
      "Iteration 140, loss = 0.14071908\n",
      "Iteration 141, loss = 0.14028828\n",
      "Iteration 142, loss = 0.13988929\n",
      "Iteration 143, loss = 0.13934930\n",
      "Iteration 144, loss = 0.13895526\n",
      "Iteration 145, loss = 0.13853618\n",
      "Iteration 146, loss = 0.13810581\n",
      "Iteration 147, loss = 0.13769358\n",
      "Iteration 148, loss = 0.13732779\n",
      "Iteration 149, loss = 0.13690736\n",
      "Iteration 150, loss = 0.13650933\n",
      "Iteration 151, loss = 0.13610215\n",
      "Iteration 152, loss = 0.13570232\n",
      "Iteration 153, loss = 0.13536924\n",
      "Iteration 154, loss = 0.13494669\n",
      "Iteration 155, loss = 0.13460880\n",
      "Iteration 156, loss = 0.13420306\n",
      "Iteration 157, loss = 0.13383222\n",
      "Iteration 158, loss = 0.13345817\n",
      "Iteration 159, loss = 0.13311802\n",
      "Iteration 160, loss = 0.13279770\n",
      "Iteration 161, loss = 0.13238405\n",
      "Iteration 162, loss = 0.13209991\n",
      "Iteration 163, loss = 0.13167728\n",
      "Iteration 164, loss = 0.13135931\n",
      "Iteration 165, loss = 0.13100812\n",
      "Iteration 166, loss = 0.13069001\n",
      "Iteration 167, loss = 0.13032124\n",
      "Iteration 168, loss = 0.13007372\n",
      "Iteration 169, loss = 0.12967714\n",
      "Iteration 170, loss = 0.12940361\n",
      "Iteration 171, loss = 0.12903897\n",
      "Iteration 172, loss = 0.12871967\n",
      "Iteration 173, loss = 0.12845608\n",
      "Iteration 174, loss = 0.12809460\n",
      "Iteration 175, loss = 0.12779218\n",
      "Iteration 176, loss = 0.12744916\n",
      "Iteration 177, loss = 0.12715388\n",
      "Iteration 178, loss = 0.12689611\n",
      "Iteration 179, loss = 0.12656497\n",
      "Iteration 180, loss = 0.12627062\n",
      "Iteration 181, loss = 0.12600677\n",
      "Iteration 182, loss = 0.12571775\n",
      "Iteration 183, loss = 0.12538756\n",
      "Iteration 184, loss = 0.12515539\n",
      "Iteration 185, loss = 0.12481310\n",
      "Iteration 186, loss = 0.12451693\n",
      "Iteration 187, loss = 0.12428431\n",
      "Iteration 188, loss = 0.12397335\n",
      "Iteration 189, loss = 0.12372328\n",
      "Iteration 190, loss = 0.12339540\n",
      "Iteration 191, loss = 0.12315956\n",
      "Iteration 192, loss = 0.12286248\n",
      "Iteration 193, loss = 0.12261890\n",
      "Iteration 194, loss = 0.12233417\n",
      "Iteration 195, loss = 0.12209787\n",
      "Iteration 196, loss = 0.12183380\n",
      "Iteration 197, loss = 0.12158317\n",
      "Iteration 198, loss = 0.12132980\n",
      "Iteration 199, loss = 0.12105262\n",
      "Iteration 200, loss = 0.12081725\n",
      "Iteration 201, loss = 0.12056767\n",
      "Iteration 202, loss = 0.12040945\n",
      "Iteration 203, loss = 0.12009122\n",
      "Iteration 204, loss = 0.11990252\n",
      "Iteration 205, loss = 0.11967821\n",
      "Iteration 206, loss = 0.11943496\n",
      "Iteration 207, loss = 0.11918310\n",
      "Iteration 208, loss = 0.11893718\n",
      "Iteration 209, loss = 0.11871037\n",
      "Iteration 210, loss = 0.11848856\n",
      "Iteration 211, loss = 0.11826354\n",
      "Iteration 212, loss = 0.11805627\n",
      "Iteration 213, loss = 0.11782630\n",
      "Iteration 214, loss = 0.11762488\n",
      "Iteration 215, loss = 0.11741220\n",
      "Iteration 216, loss = 0.11720901\n",
      "Iteration 217, loss = 0.11697466\n",
      "Iteration 218, loss = 0.11682296\n",
      "Iteration 219, loss = 0.11663667\n",
      "Iteration 220, loss = 0.11640473\n",
      "Iteration 221, loss = 0.11616502\n",
      "Iteration 222, loss = 0.11596171\n",
      "Iteration 223, loss = 0.11574582\n",
      "Iteration 224, loss = 0.11556250\n",
      "Iteration 225, loss = 0.11535702\n",
      "Iteration 226, loss = 0.11515402\n",
      "Iteration 227, loss = 0.11497891\n",
      "Iteration 228, loss = 0.11478653\n",
      "Iteration 229, loss = 0.11456010\n",
      "Iteration 230, loss = 0.11436839\n",
      "Iteration 231, loss = 0.11418372\n",
      "Iteration 232, loss = 0.11400866\n",
      "Iteration 233, loss = 0.11383491\n",
      "Iteration 234, loss = 0.11363771\n",
      "Iteration 235, loss = 0.11344524\n",
      "Iteration 236, loss = 0.11326911\n",
      "Iteration 237, loss = 0.11309415\n",
      "Iteration 238, loss = 0.11288459\n",
      "Iteration 239, loss = 0.11271065\n",
      "Iteration 240, loss = 0.11256287\n",
      "Iteration 241, loss = 0.11233896\n",
      "Iteration 242, loss = 0.11218159\n",
      "Iteration 243, loss = 0.11201396\n",
      "Iteration 244, loss = 0.11182069\n",
      "Iteration 245, loss = 0.11164636\n",
      "Iteration 246, loss = 0.11146651\n",
      "Iteration 247, loss = 0.11128941\n",
      "Iteration 248, loss = 0.11114988\n",
      "Iteration 249, loss = 0.11097222\n",
      "Iteration 250, loss = 0.11080453\n",
      "Iteration 251, loss = 0.11063500\n",
      "Iteration 252, loss = 0.11048671\n",
      "Iteration 253, loss = 0.11034501\n",
      "Iteration 254, loss = 0.11019922\n",
      "Iteration 255, loss = 0.11000444\n",
      "Iteration 256, loss = 0.10984503\n",
      "Iteration 257, loss = 0.10973029\n",
      "Iteration 258, loss = 0.10951974\n",
      "Iteration 259, loss = 0.10935522\n",
      "Iteration 260, loss = 0.10920578\n",
      "Iteration 261, loss = 0.10905108\n",
      "Iteration 262, loss = 0.10890507\n",
      "Iteration 263, loss = 0.10876313\n",
      "Iteration 264, loss = 0.10857564\n",
      "Iteration 265, loss = 0.10850203\n",
      "Iteration 266, loss = 0.10828197\n",
      "Iteration 267, loss = 0.10818830\n",
      "Iteration 268, loss = 0.10802669\n",
      "Iteration 269, loss = 0.10786489\n",
      "Iteration 270, loss = 0.10772637\n",
      "Iteration 271, loss = 0.10758223\n",
      "Iteration 272, loss = 0.10740066\n",
      "Iteration 273, loss = 0.10724071\n",
      "Iteration 274, loss = 0.10715509\n",
      "Iteration 275, loss = 0.10697399\n",
      "Iteration 276, loss = 0.10684961\n",
      "Iteration 277, loss = 0.10668724\n",
      "Iteration 278, loss = 0.10655168\n",
      "Iteration 279, loss = 0.10640216\n",
      "Iteration 280, loss = 0.10628590\n",
      "Iteration 281, loss = 0.10612289\n",
      "Iteration 282, loss = 0.10598145\n",
      "Iteration 283, loss = 0.10584020\n",
      "Iteration 284, loss = 0.10569966\n",
      "Iteration 285, loss = 0.10560515\n",
      "Iteration 286, loss = 0.10546920\n",
      "Iteration 287, loss = 0.10535519\n",
      "Iteration 288, loss = 0.10517848\n",
      "Iteration 289, loss = 0.10513292\n",
      "Iteration 290, loss = 0.10492126\n",
      "Iteration 291, loss = 0.10484942\n",
      "Iteration 292, loss = 0.10467741\n",
      "Iteration 293, loss = 0.10453302\n",
      "Iteration 294, loss = 0.10440284\n",
      "Iteration 295, loss = 0.10427800\n",
      "Iteration 296, loss = 0.10413933\n",
      "Iteration 297, loss = 0.10401471\n",
      "Iteration 298, loss = 0.10386728\n",
      "Iteration 299, loss = 0.10373990\n",
      "Iteration 300, loss = 0.10360178\n",
      "Iteration 301, loss = 0.10349853\n",
      "Iteration 302, loss = 0.10336078\n",
      "Iteration 303, loss = 0.10325452\n",
      "Iteration 304, loss = 0.10316046\n",
      "Iteration 305, loss = 0.10309700\n",
      "Iteration 306, loss = 0.10287648\n",
      "Iteration 307, loss = 0.10276490\n",
      "Iteration 308, loss = 0.10263792\n",
      "Iteration 309, loss = 0.10253042\n",
      "Iteration 310, loss = 0.10241717\n",
      "Iteration 311, loss = 0.10232146\n",
      "Iteration 312, loss = 0.10216910\n",
      "Iteration 313, loss = 0.10203070\n",
      "Iteration 314, loss = 0.10195528\n",
      "Iteration 315, loss = 0.10182825\n",
      "Iteration 316, loss = 0.10167669\n",
      "Iteration 317, loss = 0.10161154\n",
      "Iteration 318, loss = 0.10143719\n",
      "Iteration 319, loss = 0.10134545\n",
      "Iteration 320, loss = 0.10123092\n",
      "Iteration 321, loss = 0.10114104\n",
      "Iteration 322, loss = 0.10098752\n",
      "Iteration 323, loss = 0.10087070\n",
      "Iteration 324, loss = 0.10077202\n",
      "Iteration 325, loss = 0.10069618\n",
      "Iteration 326, loss = 0.10056308\n",
      "Iteration 327, loss = 0.10046305\n",
      "Iteration 328, loss = 0.10032312\n",
      "Iteration 329, loss = 0.10023326\n",
      "Iteration 330, loss = 0.10012141\n",
      "Iteration 331, loss = 0.09999979\n",
      "Iteration 332, loss = 0.09991508\n",
      "Iteration 333, loss = 0.09980988\n",
      "Iteration 334, loss = 0.09975687\n",
      "Iteration 335, loss = 0.09960805\n",
      "Iteration 336, loss = 0.09951407\n",
      "Iteration 337, loss = 0.09938114\n",
      "Iteration 338, loss = 0.09927754\n",
      "Iteration 339, loss = 0.09911975\n",
      "Iteration 340, loss = 0.09902725\n",
      "Iteration 341, loss = 0.09898990\n",
      "Iteration 342, loss = 0.09881726\n",
      "Iteration 343, loss = 0.09871870\n",
      "Iteration 344, loss = 0.09860215\n",
      "Iteration 345, loss = 0.09852037\n",
      "Iteration 346, loss = 0.09838486\n",
      "Iteration 347, loss = 0.09829464\n",
      "Iteration 348, loss = 0.09819334\n",
      "Iteration 349, loss = 0.09807590\n",
      "Iteration 350, loss = 0.09801189\n",
      "Iteration 351, loss = 0.09788310\n",
      "Iteration 352, loss = 0.09777772\n",
      "Iteration 353, loss = 0.09768334\n",
      "Iteration 354, loss = 0.09760871\n",
      "Iteration 355, loss = 0.09748098\n",
      "Iteration 356, loss = 0.09737953\n",
      "Iteration 357, loss = 0.09726065\n",
      "Iteration 358, loss = 0.09718406\n",
      "Iteration 359, loss = 0.09711521\n",
      "Iteration 360, loss = 0.09701702\n",
      "Iteration 361, loss = 0.09687591\n",
      "Iteration 362, loss = 0.09677655\n",
      "Iteration 363, loss = 0.09669316\n",
      "Iteration 364, loss = 0.09660382\n",
      "Iteration 365, loss = 0.09650412\n",
      "Iteration 366, loss = 0.09642191\n",
      "Iteration 367, loss = 0.09632598\n",
      "Iteration 368, loss = 0.09620852\n",
      "Iteration 369, loss = 0.09616222\n",
      "Iteration 370, loss = 0.09601997\n",
      "Iteration 371, loss = 0.09591916\n",
      "Iteration 372, loss = 0.09582810\n",
      "Iteration 373, loss = 0.09572215\n",
      "Iteration 374, loss = 0.09563874\n",
      "Iteration 375, loss = 0.09555822\n",
      "Iteration 376, loss = 0.09544493\n",
      "Iteration 377, loss = 0.09536033\n",
      "Iteration 378, loss = 0.09526685\n",
      "Iteration 379, loss = 0.09517027\n",
      "Iteration 380, loss = 0.09507876\n",
      "Iteration 381, loss = 0.09500005\n",
      "Iteration 382, loss = 0.09492322\n",
      "Iteration 383, loss = 0.09481192\n",
      "Iteration 384, loss = 0.09471503\n",
      "Iteration 385, loss = 0.09463705\n",
      "Iteration 386, loss = 0.09453376\n",
      "Iteration 387, loss = 0.09446086\n",
      "Iteration 388, loss = 0.09435978\n",
      "Iteration 389, loss = 0.09428217\n",
      "Iteration 390, loss = 0.09419189\n",
      "Iteration 391, loss = 0.09408413\n",
      "Iteration 392, loss = 0.09403897\n",
      "Iteration 393, loss = 0.09392308\n",
      "Iteration 394, loss = 0.09383056\n",
      "Iteration 395, loss = 0.09373024\n",
      "Iteration 396, loss = 0.09367305\n",
      "Iteration 397, loss = 0.09356279\n",
      "Iteration 398, loss = 0.09349365\n",
      "Iteration 399, loss = 0.09338259\n",
      "Iteration 400, loss = 0.09330840\n",
      "Iteration 401, loss = 0.09322875\n",
      "Iteration 402, loss = 0.09314263\n",
      "Iteration 403, loss = 0.09305068\n",
      "Iteration 404, loss = 0.09297146\n",
      "Iteration 405, loss = 0.09288609\n",
      "Iteration 406, loss = 0.09280552\n",
      "Iteration 407, loss = 0.09275165\n",
      "Iteration 408, loss = 0.09262362\n",
      "Iteration 409, loss = 0.09255004\n",
      "Iteration 410, loss = 0.09245715\n",
      "Iteration 411, loss = 0.09239553\n",
      "Iteration 412, loss = 0.09228142\n",
      "Iteration 413, loss = 0.09224000\n",
      "Iteration 414, loss = 0.09212808\n",
      "Iteration 415, loss = 0.09205308\n",
      "Iteration 416, loss = 0.09196803\n",
      "Iteration 417, loss = 0.09191987\n",
      "Iteration 418, loss = 0.09183372\n",
      "Iteration 419, loss = 0.09172740\n",
      "Iteration 420, loss = 0.09165929\n",
      "Iteration 421, loss = 0.09163251\n",
      "Iteration 422, loss = 0.09151592\n",
      "Iteration 423, loss = 0.09140769\n",
      "Iteration 424, loss = 0.09134715\n",
      "Iteration 425, loss = 0.09128357\n",
      "Iteration 426, loss = 0.09117015\n",
      "Iteration 427, loss = 0.09112691\n",
      "Iteration 428, loss = 0.09103198\n",
      "Iteration 429, loss = 0.09095118\n",
      "Iteration 430, loss = 0.09085273\n",
      "Iteration 431, loss = 0.09080702\n",
      "Iteration 432, loss = 0.09072784\n",
      "Iteration 433, loss = 0.09064913\n",
      "Iteration 434, loss = 0.09053928\n",
      "Iteration 435, loss = 0.09048529\n",
      "Iteration 436, loss = 0.09040733\n",
      "Iteration 437, loss = 0.09033682\n",
      "Iteration 438, loss = 0.09024873\n",
      "Iteration 439, loss = 0.09017160\n",
      "Iteration 440, loss = 0.09008710\n",
      "Iteration 441, loss = 0.09003513\n",
      "Iteration 442, loss = 0.08996798\n",
      "Iteration 443, loss = 0.08986457\n",
      "Iteration 444, loss = 0.08981252\n",
      "Iteration 445, loss = 0.08974625\n",
      "Iteration 446, loss = 0.08966043\n",
      "Iteration 447, loss = 0.08958448\n",
      "Iteration 448, loss = 0.08957010\n",
      "Iteration 449, loss = 0.08943570\n",
      "Iteration 450, loss = 0.08935652\n",
      "Iteration 451, loss = 0.08928720\n",
      "Iteration 452, loss = 0.08919310\n",
      "Iteration 453, loss = 0.08913553\n",
      "Iteration 454, loss = 0.08908400\n",
      "Iteration 455, loss = 0.08906675\n",
      "Iteration 456, loss = 0.08891054\n",
      "Iteration 457, loss = 0.08887554\n",
      "Iteration 458, loss = 0.08877689\n",
      "Iteration 459, loss = 0.08873244\n",
      "Iteration 460, loss = 0.08866280\n",
      "Iteration 461, loss = 0.08858794\n",
      "Iteration 462, loss = 0.08850280\n",
      "Iteration 463, loss = 0.08840661\n",
      "Iteration 464, loss = 0.08834255\n",
      "Iteration 465, loss = 0.08826724\n",
      "Iteration 466, loss = 0.08827298\n",
      "Iteration 467, loss = 0.08814248\n",
      "Iteration 468, loss = 0.08806404\n",
      "Iteration 469, loss = 0.08799569\n",
      "Iteration 470, loss = 0.08794439\n",
      "Iteration 471, loss = 0.08786253\n",
      "Iteration 472, loss = 0.08784889\n",
      "Iteration 473, loss = 0.08772181\n",
      "Iteration 474, loss = 0.08766429\n",
      "Iteration 475, loss = 0.08761089\n",
      "Iteration 476, loss = 0.08753210\n",
      "Iteration 477, loss = 0.08747016\n",
      "Iteration 478, loss = 0.08739246\n",
      "Iteration 479, loss = 0.08731846\n",
      "Iteration 480, loss = 0.08728959\n",
      "Iteration 481, loss = 0.08720112\n",
      "Iteration 482, loss = 0.08714382\n",
      "Iteration 483, loss = 0.08710050\n",
      "Iteration 484, loss = 0.08699929\n",
      "Iteration 485, loss = 0.08697119\n",
      "Iteration 486, loss = 0.08687713\n",
      "Iteration 487, loss = 0.08681194\n",
      "Iteration 488, loss = 0.08674603\n",
      "Iteration 489, loss = 0.08666473\n",
      "Iteration 490, loss = 0.08659396\n",
      "Iteration 491, loss = 0.08659235\n",
      "Iteration 492, loss = 0.08648333\n",
      "Iteration 493, loss = 0.08642565\n",
      "Iteration 494, loss = 0.08635205\n",
      "Iteration 495, loss = 0.08627354\n",
      "Iteration 496, loss = 0.08622114\n",
      "Iteration 497, loss = 0.08617793\n",
      "Iteration 498, loss = 0.08611928\n",
      "Iteration 499, loss = 0.08607553\n",
      "Iteration 500, loss = 0.08596682\n",
      "Iteration 501, loss = 0.08590193\n",
      "Iteration 502, loss = 0.08584599\n",
      "Iteration 503, loss = 0.08577437\n",
      "Iteration 504, loss = 0.08571826\n",
      "Iteration 505, loss = 0.08566524\n",
      "Iteration 506, loss = 0.08558347\n",
      "Iteration 507, loss = 0.08553773\n",
      "Iteration 508, loss = 0.08549864\n",
      "Iteration 509, loss = 0.08546935\n",
      "Iteration 510, loss = 0.08536303\n",
      "Iteration 511, loss = 0.08531590\n",
      "Iteration 512, loss = 0.08524830\n",
      "Iteration 513, loss = 0.08518714\n",
      "Iteration 514, loss = 0.08510447\n",
      "Iteration 515, loss = 0.08506396\n",
      "Iteration 516, loss = 0.08497787\n",
      "Iteration 517, loss = 0.08492564\n",
      "Iteration 518, loss = 0.08486014\n",
      "Iteration 519, loss = 0.08481478\n",
      "Iteration 520, loss = 0.08472793\n",
      "Iteration 521, loss = 0.08471944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72420504\n",
      "Iteration 2, loss = 0.67488641\n",
      "Iteration 3, loss = 0.63450615\n",
      "Iteration 4, loss = 0.60122805\n",
      "Iteration 5, loss = 0.57222240\n",
      "Iteration 6, loss = 0.54735021\n",
      "Iteration 7, loss = 0.52557534\n",
      "Iteration 8, loss = 0.50579783\n",
      "Iteration 9, loss = 0.48796796\n",
      "Iteration 10, loss = 0.47182861\n",
      "Iteration 11, loss = 0.45702303\n",
      "Iteration 12, loss = 0.44330804\n",
      "Iteration 13, loss = 0.43068478\n",
      "Iteration 14, loss = 0.41887079\n",
      "Iteration 15, loss = 0.40787564\n",
      "Iteration 16, loss = 0.39755430\n",
      "Iteration 17, loss = 0.38797988\n",
      "Iteration 18, loss = 0.37880995\n",
      "Iteration 19, loss = 0.37015537\n",
      "Iteration 20, loss = 0.36187822\n",
      "Iteration 21, loss = 0.35407301\n",
      "Iteration 22, loss = 0.34657618\n",
      "Iteration 23, loss = 0.33951075\n",
      "Iteration 24, loss = 0.33284242\n",
      "Iteration 25, loss = 0.32652670\n",
      "Iteration 26, loss = 0.32055117\n",
      "Iteration 27, loss = 0.31479141\n",
      "Iteration 28, loss = 0.30916975\n",
      "Iteration 29, loss = 0.30379407\n",
      "Iteration 30, loss = 0.29876775\n",
      "Iteration 31, loss = 0.29392729\n",
      "Iteration 32, loss = 0.28934136\n",
      "Iteration 33, loss = 0.28486343\n",
      "Iteration 34, loss = 0.28060772\n",
      "Iteration 35, loss = 0.27636036\n",
      "Iteration 36, loss = 0.27247810\n",
      "Iteration 37, loss = 0.26853564\n",
      "Iteration 38, loss = 0.26479622\n",
      "Iteration 39, loss = 0.26111150\n",
      "Iteration 40, loss = 0.25758698\n",
      "Iteration 41, loss = 0.25420338\n",
      "Iteration 42, loss = 0.25089444\n",
      "Iteration 43, loss = 0.24777064\n",
      "Iteration 44, loss = 0.24479677\n",
      "Iteration 45, loss = 0.24172736\n",
      "Iteration 46, loss = 0.23888321\n",
      "Iteration 47, loss = 0.23610675\n",
      "Iteration 48, loss = 0.23345817\n",
      "Iteration 49, loss = 0.23076313\n",
      "Iteration 50, loss = 0.22825515\n",
      "Iteration 51, loss = 0.22573140\n",
      "Iteration 52, loss = 0.22332389\n",
      "Iteration 53, loss = 0.22101198\n",
      "Iteration 54, loss = 0.21876124\n",
      "Iteration 55, loss = 0.21660390\n",
      "Iteration 56, loss = 0.21442733\n",
      "Iteration 57, loss = 0.21232662\n",
      "Iteration 58, loss = 0.21026735\n",
      "Iteration 59, loss = 0.20831409\n",
      "Iteration 60, loss = 0.20635244\n",
      "Iteration 61, loss = 0.20445542\n",
      "Iteration 62, loss = 0.20261321\n",
      "Iteration 63, loss = 0.20086283\n",
      "Iteration 64, loss = 0.19911718\n",
      "Iteration 65, loss = 0.19739807\n",
      "Iteration 66, loss = 0.19568351\n",
      "Iteration 67, loss = 0.19401549\n",
      "Iteration 68, loss = 0.19241278\n",
      "Iteration 69, loss = 0.19091155\n",
      "Iteration 70, loss = 0.18934830\n",
      "Iteration 71, loss = 0.18787429\n",
      "Iteration 72, loss = 0.18637022\n",
      "Iteration 73, loss = 0.18489423\n",
      "Iteration 74, loss = 0.18352849\n",
      "Iteration 75, loss = 0.18214538\n",
      "Iteration 76, loss = 0.18081045\n",
      "Iteration 77, loss = 0.17952258\n",
      "Iteration 78, loss = 0.17827290\n",
      "Iteration 79, loss = 0.17701638\n",
      "Iteration 80, loss = 0.17589647\n",
      "Iteration 81, loss = 0.17465056\n",
      "Iteration 82, loss = 0.17352786\n",
      "Iteration 83, loss = 0.17237677\n",
      "Iteration 84, loss = 0.17129216\n",
      "Iteration 85, loss = 0.17022202\n",
      "Iteration 86, loss = 0.16920877\n",
      "Iteration 87, loss = 0.16811810\n",
      "Iteration 88, loss = 0.16717141\n",
      "Iteration 89, loss = 0.16621070\n",
      "Iteration 90, loss = 0.16516346\n",
      "Iteration 91, loss = 0.16423314\n",
      "Iteration 92, loss = 0.16334329\n",
      "Iteration 93, loss = 0.16234611\n",
      "Iteration 94, loss = 0.16143530\n",
      "Iteration 95, loss = 0.16055224\n",
      "Iteration 96, loss = 0.15971716\n",
      "Iteration 97, loss = 0.15884558\n",
      "Iteration 98, loss = 0.15805347\n",
      "Iteration 99, loss = 0.15719402\n",
      "Iteration 100, loss = 0.15643822\n",
      "Iteration 101, loss = 0.15559230\n",
      "Iteration 102, loss = 0.15479715\n",
      "Iteration 103, loss = 0.15402902\n",
      "Iteration 104, loss = 0.15330033\n",
      "Iteration 105, loss = 0.15253691\n",
      "Iteration 106, loss = 0.15180453\n",
      "Iteration 107, loss = 0.15108366\n",
      "Iteration 108, loss = 0.15040788\n",
      "Iteration 109, loss = 0.14973394\n",
      "Iteration 110, loss = 0.14903774\n",
      "Iteration 111, loss = 0.14843290\n",
      "Iteration 112, loss = 0.14772678\n",
      "Iteration 113, loss = 0.14709379\n",
      "Iteration 114, loss = 0.14645487\n",
      "Iteration 115, loss = 0.14585281\n",
      "Iteration 116, loss = 0.14527976\n",
      "Iteration 117, loss = 0.14467511\n",
      "Iteration 118, loss = 0.14406528\n",
      "Iteration 119, loss = 0.14347807\n",
      "Iteration 120, loss = 0.14292698\n",
      "Iteration 121, loss = 0.14234560\n",
      "Iteration 122, loss = 0.14180456\n",
      "Iteration 123, loss = 0.14125202\n",
      "Iteration 124, loss = 0.14072618\n",
      "Iteration 125, loss = 0.14021694\n",
      "Iteration 126, loss = 0.13969155\n",
      "Iteration 127, loss = 0.13917764\n",
      "Iteration 128, loss = 0.13869477\n",
      "Iteration 129, loss = 0.13817586\n",
      "Iteration 130, loss = 0.13767746\n",
      "Iteration 131, loss = 0.13725600\n",
      "Iteration 132, loss = 0.13671290\n",
      "Iteration 133, loss = 0.13626286\n",
      "Iteration 134, loss = 0.13575477\n",
      "Iteration 135, loss = 0.13528811\n",
      "Iteration 136, loss = 0.13487385\n",
      "Iteration 137, loss = 0.13433903\n",
      "Iteration 138, loss = 0.13387330\n",
      "Iteration 139, loss = 0.13342890\n",
      "Iteration 140, loss = 0.13300457\n",
      "Iteration 141, loss = 0.13259003\n",
      "Iteration 142, loss = 0.13224495\n",
      "Iteration 143, loss = 0.13168717\n",
      "Iteration 144, loss = 0.13130133\n",
      "Iteration 145, loss = 0.13087729\n",
      "Iteration 146, loss = 0.13051080\n",
      "Iteration 147, loss = 0.13010186\n",
      "Iteration 148, loss = 0.12972920\n",
      "Iteration 149, loss = 0.12932532\n",
      "Iteration 150, loss = 0.12894948\n",
      "Iteration 151, loss = 0.12858085\n",
      "Iteration 152, loss = 0.12818784\n",
      "Iteration 153, loss = 0.12785146\n",
      "Iteration 154, loss = 0.12748101\n",
      "Iteration 155, loss = 0.12714734\n",
      "Iteration 156, loss = 0.12679856\n",
      "Iteration 157, loss = 0.12645577\n",
      "Iteration 158, loss = 0.12608132\n",
      "Iteration 159, loss = 0.12575254\n",
      "Iteration 160, loss = 0.12545942\n",
      "Iteration 161, loss = 0.12507865\n",
      "Iteration 162, loss = 0.12475876\n",
      "Iteration 163, loss = 0.12442305\n",
      "Iteration 164, loss = 0.12413835\n",
      "Iteration 165, loss = 0.12379157\n",
      "Iteration 166, loss = 0.12351163\n",
      "Iteration 167, loss = 0.12317814\n",
      "Iteration 168, loss = 0.12295388\n",
      "Iteration 169, loss = 0.12258059\n",
      "Iteration 170, loss = 0.12234092\n",
      "Iteration 171, loss = 0.12201251\n",
      "Iteration 172, loss = 0.12174506\n",
      "Iteration 173, loss = 0.12147006\n",
      "Iteration 174, loss = 0.12117295\n",
      "Iteration 175, loss = 0.12092636\n",
      "Iteration 176, loss = 0.12058538\n",
      "Iteration 177, loss = 0.12032239\n",
      "Iteration 178, loss = 0.12008057\n",
      "Iteration 179, loss = 0.11979198\n",
      "Iteration 180, loss = 0.11956005\n",
      "Iteration 181, loss = 0.11927449\n",
      "Iteration 182, loss = 0.11905763\n",
      "Iteration 183, loss = 0.11874221\n",
      "Iteration 184, loss = 0.11850618\n",
      "Iteration 185, loss = 0.11824563\n",
      "Iteration 186, loss = 0.11800396\n",
      "Iteration 187, loss = 0.11776369\n",
      "Iteration 188, loss = 0.11752154\n",
      "Iteration 189, loss = 0.11726901\n",
      "Iteration 190, loss = 0.11702497\n",
      "Iteration 191, loss = 0.11682497\n",
      "Iteration 192, loss = 0.11656910\n",
      "Iteration 193, loss = 0.11636603\n",
      "Iteration 194, loss = 0.11611248\n",
      "Iteration 195, loss = 0.11590863\n",
      "Iteration 196, loss = 0.11565697\n",
      "Iteration 197, loss = 0.11544725\n",
      "Iteration 198, loss = 0.11522228\n",
      "Iteration 199, loss = 0.11498629\n",
      "Iteration 200, loss = 0.11478307\n",
      "Iteration 201, loss = 0.11456744\n",
      "Iteration 202, loss = 0.11443522\n",
      "Iteration 203, loss = 0.11414081\n",
      "Iteration 204, loss = 0.11396046\n",
      "Iteration 205, loss = 0.11378668\n",
      "Iteration 206, loss = 0.11354717\n",
      "Iteration 207, loss = 0.11335536\n",
      "Iteration 208, loss = 0.11313008\n",
      "Iteration 209, loss = 0.11290822\n",
      "Iteration 210, loss = 0.11272711\n",
      "Iteration 211, loss = 0.11252693\n",
      "Iteration 212, loss = 0.11234624\n",
      "Iteration 213, loss = 0.11214989\n",
      "Iteration 214, loss = 0.11195626\n",
      "Iteration 215, loss = 0.11177476\n",
      "Iteration 216, loss = 0.11160032\n",
      "Iteration 217, loss = 0.11139586\n",
      "Iteration 218, loss = 0.11124787\n",
      "Iteration 219, loss = 0.11107989\n",
      "Iteration 220, loss = 0.11085163\n",
      "Iteration 221, loss = 0.11067607\n",
      "Iteration 222, loss = 0.11047564\n",
      "Iteration 223, loss = 0.11031037\n",
      "Iteration 224, loss = 0.11013337\n",
      "Iteration 225, loss = 0.10994470\n",
      "Iteration 226, loss = 0.10977582\n",
      "Iteration 227, loss = 0.10960414\n",
      "Iteration 228, loss = 0.10945852\n",
      "Iteration 229, loss = 0.10923248\n",
      "Iteration 230, loss = 0.10908493\n",
      "Iteration 231, loss = 0.10890593\n",
      "Iteration 232, loss = 0.10876334\n",
      "Iteration 233, loss = 0.10859056\n",
      "Iteration 234, loss = 0.10843800\n",
      "Iteration 235, loss = 0.10824835\n",
      "Iteration 236, loss = 0.10808371\n",
      "Iteration 237, loss = 0.10792742\n",
      "Iteration 238, loss = 0.10775544\n",
      "Iteration 239, loss = 0.10759846\n",
      "Iteration 240, loss = 0.10745499\n",
      "Iteration 241, loss = 0.10725134\n",
      "Iteration 242, loss = 0.10711783\n",
      "Iteration 243, loss = 0.10696295\n",
      "Iteration 244, loss = 0.10680875\n",
      "Iteration 245, loss = 0.10663988\n",
      "Iteration 246, loss = 0.10646679\n",
      "Iteration 247, loss = 0.10631176\n",
      "Iteration 248, loss = 0.10615987\n",
      "Iteration 249, loss = 0.10602825\n",
      "Iteration 250, loss = 0.10585816\n",
      "Iteration 251, loss = 0.10571084\n",
      "Iteration 252, loss = 0.10555915\n",
      "Iteration 253, loss = 0.10544821\n",
      "Iteration 254, loss = 0.10529245\n",
      "Iteration 255, loss = 0.10515366\n",
      "Iteration 256, loss = 0.10498448\n",
      "Iteration 257, loss = 0.10486866\n",
      "Iteration 258, loss = 0.10466863\n",
      "Iteration 259, loss = 0.10454921\n",
      "Iteration 260, loss = 0.10439242\n",
      "Iteration 261, loss = 0.10425617\n",
      "Iteration 262, loss = 0.10412190\n",
      "Iteration 263, loss = 0.10398062\n",
      "Iteration 264, loss = 0.10382860\n",
      "Iteration 265, loss = 0.10373096\n",
      "Iteration 266, loss = 0.10353885\n",
      "Iteration 267, loss = 0.10345929\n",
      "Iteration 268, loss = 0.10330115\n",
      "Iteration 269, loss = 0.10315359\n",
      "Iteration 270, loss = 0.10303344\n",
      "Iteration 271, loss = 0.10290597\n",
      "Iteration 272, loss = 0.10273831\n",
      "Iteration 273, loss = 0.10261245\n",
      "Iteration 274, loss = 0.10252607\n",
      "Iteration 275, loss = 0.10236206\n",
      "Iteration 276, loss = 0.10224002\n",
      "Iteration 277, loss = 0.10208841\n",
      "Iteration 278, loss = 0.10194284\n",
      "Iteration 279, loss = 0.10184575\n",
      "Iteration 280, loss = 0.10172173\n",
      "Iteration 281, loss = 0.10157946\n",
      "Iteration 282, loss = 0.10146276\n",
      "Iteration 283, loss = 0.10131536\n",
      "Iteration 284, loss = 0.10117705\n",
      "Iteration 285, loss = 0.10108829\n",
      "Iteration 286, loss = 0.10095803\n",
      "Iteration 287, loss = 0.10086165\n",
      "Iteration 288, loss = 0.10070415\n",
      "Iteration 289, loss = 0.10060552\n",
      "Iteration 290, loss = 0.10045297\n",
      "Iteration 291, loss = 0.10037570\n",
      "Iteration 292, loss = 0.10019684\n",
      "Iteration 293, loss = 0.10009232\n",
      "Iteration 294, loss = 0.09997895\n",
      "Iteration 295, loss = 0.09986102\n",
      "Iteration 296, loss = 0.09972504\n",
      "Iteration 297, loss = 0.09964093\n",
      "Iteration 298, loss = 0.09947422\n",
      "Iteration 299, loss = 0.09935892\n",
      "Iteration 300, loss = 0.09924486\n",
      "Iteration 301, loss = 0.09912770\n",
      "Iteration 302, loss = 0.09902352\n",
      "Iteration 303, loss = 0.09890399\n",
      "Iteration 304, loss = 0.09881713\n",
      "Iteration 305, loss = 0.09871444\n",
      "Iteration 306, loss = 0.09855433\n",
      "Iteration 307, loss = 0.09843814\n",
      "Iteration 308, loss = 0.09831737\n",
      "Iteration 309, loss = 0.09821262\n",
      "Iteration 310, loss = 0.09810027\n",
      "Iteration 311, loss = 0.09801665\n",
      "Iteration 312, loss = 0.09788172\n",
      "Iteration 313, loss = 0.09774261\n",
      "Iteration 314, loss = 0.09766413\n",
      "Iteration 315, loss = 0.09754848\n",
      "Iteration 316, loss = 0.09741414\n",
      "Iteration 317, loss = 0.09733970\n",
      "Iteration 318, loss = 0.09720233\n",
      "Iteration 319, loss = 0.09710579\n",
      "Iteration 320, loss = 0.09699015\n",
      "Iteration 321, loss = 0.09688952\n",
      "Iteration 322, loss = 0.09675858\n",
      "Iteration 323, loss = 0.09664453\n",
      "Iteration 324, loss = 0.09656342\n",
      "Iteration 325, loss = 0.09649248\n",
      "Iteration 326, loss = 0.09636849\n",
      "Iteration 327, loss = 0.09624543\n",
      "Iteration 328, loss = 0.09613813\n",
      "Iteration 329, loss = 0.09604749\n",
      "Iteration 330, loss = 0.09595604\n",
      "Iteration 331, loss = 0.09583357\n",
      "Iteration 332, loss = 0.09574350\n",
      "Iteration 333, loss = 0.09562924\n",
      "Iteration 334, loss = 0.09557278\n",
      "Iteration 335, loss = 0.09543006\n",
      "Iteration 336, loss = 0.09536398\n",
      "Iteration 337, loss = 0.09522133\n",
      "Iteration 338, loss = 0.09513917\n",
      "Iteration 339, loss = 0.09501030\n",
      "Iteration 340, loss = 0.09492314\n",
      "Iteration 341, loss = 0.09486166\n",
      "Iteration 342, loss = 0.09471886\n",
      "Iteration 343, loss = 0.09465127\n",
      "Iteration 344, loss = 0.09452728\n",
      "Iteration 345, loss = 0.09445860\n",
      "Iteration 346, loss = 0.09432998\n",
      "Iteration 347, loss = 0.09425904\n",
      "Iteration 348, loss = 0.09415318\n",
      "Iteration 349, loss = 0.09404623\n",
      "Iteration 350, loss = 0.09396142\n",
      "Iteration 351, loss = 0.09386032\n",
      "Iteration 352, loss = 0.09376499\n",
      "Iteration 353, loss = 0.09368241\n",
      "Iteration 354, loss = 0.09359718\n",
      "Iteration 355, loss = 0.09348110\n",
      "Iteration 356, loss = 0.09338654\n",
      "Iteration 357, loss = 0.09328395\n",
      "Iteration 358, loss = 0.09320292\n",
      "Iteration 359, loss = 0.09312705\n",
      "Iteration 360, loss = 0.09306167\n",
      "Iteration 361, loss = 0.09292559\n",
      "Iteration 362, loss = 0.09284398\n",
      "Iteration 363, loss = 0.09274717\n",
      "Iteration 364, loss = 0.09266995\n",
      "Iteration 365, loss = 0.09257101\n",
      "Iteration 366, loss = 0.09251619\n",
      "Iteration 367, loss = 0.09242252\n",
      "Iteration 368, loss = 0.09230250\n",
      "Iteration 369, loss = 0.09224425\n",
      "Iteration 370, loss = 0.09215041\n",
      "Iteration 371, loss = 0.09203826\n",
      "Iteration 372, loss = 0.09197082\n",
      "Iteration 373, loss = 0.09186598\n",
      "Iteration 374, loss = 0.09179190\n",
      "Iteration 375, loss = 0.09169994\n",
      "Iteration 376, loss = 0.09163525\n",
      "Iteration 377, loss = 0.09153521\n",
      "Iteration 378, loss = 0.09147042\n",
      "Iteration 379, loss = 0.09137140\n",
      "Iteration 380, loss = 0.09127547\n",
      "Iteration 381, loss = 0.09120361\n",
      "Iteration 382, loss = 0.09112144\n",
      "Iteration 383, loss = 0.09104420\n",
      "Iteration 384, loss = 0.09094505\n",
      "Iteration 385, loss = 0.09086403\n",
      "Iteration 386, loss = 0.09080213\n",
      "Iteration 387, loss = 0.09071802\n",
      "Iteration 388, loss = 0.09059753\n",
      "Iteration 389, loss = 0.09053154\n",
      "Iteration 390, loss = 0.09047783\n",
      "Iteration 391, loss = 0.09036358\n",
      "Iteration 392, loss = 0.09030995\n",
      "Iteration 393, loss = 0.09022648\n",
      "Iteration 394, loss = 0.09011177\n",
      "Iteration 395, loss = 0.09005052\n",
      "Iteration 396, loss = 0.08998977\n",
      "Iteration 397, loss = 0.08988387\n",
      "Iteration 398, loss = 0.08979799\n",
      "Iteration 399, loss = 0.08972100\n",
      "Iteration 400, loss = 0.08964514\n",
      "Iteration 401, loss = 0.08955527\n",
      "Iteration 402, loss = 0.08947933\n",
      "Iteration 403, loss = 0.08941492\n",
      "Iteration 404, loss = 0.08933094\n",
      "Iteration 405, loss = 0.08925608\n",
      "Iteration 406, loss = 0.08917991\n",
      "Iteration 407, loss = 0.08911233\n",
      "Iteration 408, loss = 0.08902691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.73279737\n",
      "Iteration 2, loss = 0.68305438\n",
      "Iteration 3, loss = 0.64230899\n",
      "Iteration 4, loss = 0.60821608\n",
      "Iteration 5, loss = 0.57871892\n",
      "Iteration 6, loss = 0.55335612\n",
      "Iteration 7, loss = 0.53101510\n",
      "Iteration 8, loss = 0.51071879\n",
      "Iteration 9, loss = 0.49245124\n",
      "Iteration 10, loss = 0.47588851\n",
      "Iteration 11, loss = 0.46065652\n",
      "Iteration 12, loss = 0.44658948\n",
      "Iteration 13, loss = 0.43361290\n",
      "Iteration 14, loss = 0.42147236\n",
      "Iteration 15, loss = 0.41035505\n",
      "Iteration 16, loss = 0.39981220\n",
      "Iteration 17, loss = 0.38991617\n",
      "Iteration 18, loss = 0.38047018\n",
      "Iteration 19, loss = 0.37161876\n",
      "Iteration 20, loss = 0.36320714\n",
      "Iteration 21, loss = 0.35526254\n",
      "Iteration 22, loss = 0.34772831\n",
      "Iteration 23, loss = 0.34058057\n",
      "Iteration 24, loss = 0.33373142\n",
      "Iteration 25, loss = 0.32732604\n",
      "Iteration 26, loss = 0.32111342\n",
      "Iteration 27, loss = 0.31523303\n",
      "Iteration 28, loss = 0.30963622\n",
      "Iteration 29, loss = 0.30419543\n",
      "Iteration 30, loss = 0.29911968\n",
      "Iteration 31, loss = 0.29417456\n",
      "Iteration 32, loss = 0.28950781\n",
      "Iteration 33, loss = 0.28499263\n",
      "Iteration 34, loss = 0.28070110\n",
      "Iteration 35, loss = 0.27646123\n",
      "Iteration 36, loss = 0.27254841\n",
      "Iteration 37, loss = 0.26860380\n",
      "Iteration 38, loss = 0.26491032\n",
      "Iteration 39, loss = 0.26134366\n",
      "Iteration 40, loss = 0.25795709\n",
      "Iteration 41, loss = 0.25456597\n",
      "Iteration 42, loss = 0.25140541\n",
      "Iteration 43, loss = 0.24836338\n",
      "Iteration 44, loss = 0.24548542\n",
      "Iteration 45, loss = 0.24251138\n",
      "Iteration 46, loss = 0.23973999\n",
      "Iteration 47, loss = 0.23705481\n",
      "Iteration 48, loss = 0.23456249\n",
      "Iteration 49, loss = 0.23195381\n",
      "Iteration 50, loss = 0.22957508\n",
      "Iteration 51, loss = 0.22719372\n",
      "Iteration 52, loss = 0.22492549\n",
      "Iteration 53, loss = 0.22268329\n",
      "Iteration 54, loss = 0.22058021\n",
      "Iteration 55, loss = 0.21844192\n",
      "Iteration 56, loss = 0.21647121\n",
      "Iteration 57, loss = 0.21442377\n",
      "Iteration 58, loss = 0.21244893\n",
      "Iteration 59, loss = 0.21059474\n",
      "Iteration 60, loss = 0.20877315\n",
      "Iteration 61, loss = 0.20699104\n",
      "Iteration 62, loss = 0.20522724\n",
      "Iteration 63, loss = 0.20351954\n",
      "Iteration 64, loss = 0.20195775\n",
      "Iteration 65, loss = 0.20026954\n",
      "Iteration 66, loss = 0.19867150\n",
      "Iteration 67, loss = 0.19707672\n",
      "Iteration 68, loss = 0.19560633\n",
      "Iteration 69, loss = 0.19415211\n",
      "Iteration 70, loss = 0.19265885\n",
      "Iteration 71, loss = 0.19129700\n",
      "Iteration 72, loss = 0.18985658\n",
      "Iteration 73, loss = 0.18849455\n",
      "Iteration 74, loss = 0.18718951\n",
      "Iteration 75, loss = 0.18588154\n",
      "Iteration 76, loss = 0.18462130\n",
      "Iteration 77, loss = 0.18337477\n",
      "Iteration 78, loss = 0.18213603\n",
      "Iteration 79, loss = 0.18089013\n",
      "Iteration 80, loss = 0.17973290\n",
      "Iteration 81, loss = 0.17865120\n",
      "Iteration 82, loss = 0.17748705\n",
      "Iteration 83, loss = 0.17635828\n",
      "Iteration 84, loss = 0.17533477\n",
      "Iteration 85, loss = 0.17422388\n",
      "Iteration 86, loss = 0.17320370\n",
      "Iteration 87, loss = 0.17218894\n",
      "Iteration 88, loss = 0.17123758\n",
      "Iteration 89, loss = 0.17030643\n",
      "Iteration 90, loss = 0.16928346\n",
      "Iteration 91, loss = 0.16837992\n",
      "Iteration 92, loss = 0.16743327\n",
      "Iteration 93, loss = 0.16651101\n",
      "Iteration 94, loss = 0.16561066\n",
      "Iteration 95, loss = 0.16472496\n",
      "Iteration 96, loss = 0.16391792\n",
      "Iteration 97, loss = 0.16303567\n",
      "Iteration 98, loss = 0.16220613\n",
      "Iteration 99, loss = 0.16139028\n",
      "Iteration 100, loss = 0.16058199\n",
      "Iteration 101, loss = 0.15983338\n",
      "Iteration 102, loss = 0.15910197\n",
      "Iteration 103, loss = 0.15826055\n",
      "Iteration 104, loss = 0.15760252\n",
      "Iteration 105, loss = 0.15682304\n",
      "Iteration 106, loss = 0.15606721\n",
      "Iteration 107, loss = 0.15534169\n",
      "Iteration 108, loss = 0.15465891\n",
      "Iteration 109, loss = 0.15396368\n",
      "Iteration 110, loss = 0.15329565\n",
      "Iteration 111, loss = 0.15262811\n",
      "Iteration 112, loss = 0.15198080\n",
      "Iteration 113, loss = 0.15132616\n",
      "Iteration 114, loss = 0.15069336\n",
      "Iteration 115, loss = 0.15011385\n",
      "Iteration 116, loss = 0.14947288\n",
      "Iteration 117, loss = 0.14890389\n",
      "Iteration 118, loss = 0.14827179\n",
      "Iteration 119, loss = 0.14768401\n",
      "Iteration 120, loss = 0.14712297\n",
      "Iteration 121, loss = 0.14655449\n",
      "Iteration 122, loss = 0.14603229\n",
      "Iteration 123, loss = 0.14542914\n",
      "Iteration 124, loss = 0.14494046\n",
      "Iteration 125, loss = 0.14442480\n",
      "Iteration 126, loss = 0.14385959\n",
      "Iteration 127, loss = 0.14333303\n",
      "Iteration 128, loss = 0.14281452\n",
      "Iteration 129, loss = 0.14230983\n",
      "Iteration 130, loss = 0.14181381\n",
      "Iteration 131, loss = 0.14132025\n",
      "Iteration 132, loss = 0.14083479\n",
      "Iteration 133, loss = 0.14036358\n",
      "Iteration 134, loss = 0.13988351\n",
      "Iteration 135, loss = 0.13945816\n",
      "Iteration 136, loss = 0.13896412\n",
      "Iteration 137, loss = 0.13846861\n",
      "Iteration 138, loss = 0.13801352\n",
      "Iteration 139, loss = 0.13757844\n",
      "Iteration 140, loss = 0.13712405\n",
      "Iteration 141, loss = 0.13673860\n",
      "Iteration 142, loss = 0.13633981\n",
      "Iteration 143, loss = 0.13579140\n",
      "Iteration 144, loss = 0.13537410\n",
      "Iteration 145, loss = 0.13491636\n",
      "Iteration 146, loss = 0.13454709\n",
      "Iteration 147, loss = 0.13411307\n",
      "Iteration 148, loss = 0.13370602\n",
      "Iteration 149, loss = 0.13329706\n",
      "Iteration 150, loss = 0.13287540\n",
      "Iteration 151, loss = 0.13254902\n",
      "Iteration 152, loss = 0.13210093\n",
      "Iteration 153, loss = 0.13172084\n",
      "Iteration 154, loss = 0.13134303\n",
      "Iteration 155, loss = 0.13097442\n",
      "Iteration 156, loss = 0.13057033\n",
      "Iteration 157, loss = 0.13018095\n",
      "Iteration 158, loss = 0.12985048\n",
      "Iteration 159, loss = 0.12944499\n",
      "Iteration 160, loss = 0.12920160\n",
      "Iteration 161, loss = 0.12874579\n",
      "Iteration 162, loss = 0.12840613\n",
      "Iteration 163, loss = 0.12803486\n",
      "Iteration 164, loss = 0.12771038\n",
      "Iteration 165, loss = 0.12739795\n",
      "Iteration 166, loss = 0.12704656\n",
      "Iteration 167, loss = 0.12674794\n",
      "Iteration 168, loss = 0.12638245\n",
      "Iteration 169, loss = 0.12604885\n",
      "Iteration 170, loss = 0.12575161\n",
      "Iteration 171, loss = 0.12541128\n",
      "Iteration 172, loss = 0.12510496\n",
      "Iteration 173, loss = 0.12481725\n",
      "Iteration 174, loss = 0.12448060\n",
      "Iteration 175, loss = 0.12415678\n",
      "Iteration 176, loss = 0.12387465\n",
      "Iteration 177, loss = 0.12358316\n",
      "Iteration 178, loss = 0.12326460\n",
      "Iteration 179, loss = 0.12299754\n",
      "Iteration 180, loss = 0.12270695\n",
      "Iteration 181, loss = 0.12236870\n",
      "Iteration 182, loss = 0.12214574\n",
      "Iteration 183, loss = 0.12180408\n",
      "Iteration 184, loss = 0.12150550\n",
      "Iteration 185, loss = 0.12124909\n",
      "Iteration 186, loss = 0.12095521\n",
      "Iteration 187, loss = 0.12067010\n",
      "Iteration 188, loss = 0.12041683\n",
      "Iteration 189, loss = 0.12010588\n",
      "Iteration 190, loss = 0.11983822\n",
      "Iteration 191, loss = 0.11957756\n",
      "Iteration 192, loss = 0.11935165\n",
      "Iteration 193, loss = 0.11903894\n",
      "Iteration 194, loss = 0.11879817\n",
      "Iteration 195, loss = 0.11855523\n",
      "Iteration 196, loss = 0.11826712\n",
      "Iteration 197, loss = 0.11803139\n",
      "Iteration 198, loss = 0.11777227\n",
      "Iteration 199, loss = 0.11754839\n",
      "Iteration 200, loss = 0.11727777\n",
      "Iteration 201, loss = 0.11705458\n",
      "Iteration 202, loss = 0.11686752\n",
      "Iteration 203, loss = 0.11659483\n",
      "Iteration 204, loss = 0.11636541\n",
      "Iteration 205, loss = 0.11614962\n",
      "Iteration 206, loss = 0.11596425\n",
      "Iteration 207, loss = 0.11571753\n",
      "Iteration 208, loss = 0.11545880\n",
      "Iteration 209, loss = 0.11525580\n",
      "Iteration 210, loss = 0.11506233\n",
      "Iteration 211, loss = 0.11482067\n",
      "Iteration 212, loss = 0.11457986\n",
      "Iteration 213, loss = 0.11436143\n",
      "Iteration 214, loss = 0.11416244\n",
      "Iteration 215, loss = 0.11396474\n",
      "Iteration 216, loss = 0.11376567\n",
      "Iteration 217, loss = 0.11352540\n",
      "Iteration 218, loss = 0.11335819\n",
      "Iteration 219, loss = 0.11316157\n",
      "Iteration 220, loss = 0.11295069\n",
      "Iteration 221, loss = 0.11272780\n",
      "Iteration 222, loss = 0.11251262\n",
      "Iteration 223, loss = 0.11234492\n",
      "Iteration 224, loss = 0.11212080\n",
      "Iteration 225, loss = 0.11195468\n",
      "Iteration 226, loss = 0.11173082\n",
      "Iteration 227, loss = 0.11156652\n",
      "Iteration 228, loss = 0.11137995\n",
      "Iteration 229, loss = 0.11117960\n",
      "Iteration 230, loss = 0.11099682\n",
      "Iteration 231, loss = 0.11081581\n",
      "Iteration 232, loss = 0.11062219\n",
      "Iteration 233, loss = 0.11043992\n",
      "Iteration 234, loss = 0.11026491\n",
      "Iteration 235, loss = 0.11008274\n",
      "Iteration 236, loss = 0.10990478\n",
      "Iteration 237, loss = 0.10975942\n",
      "Iteration 238, loss = 0.10955507\n",
      "Iteration 239, loss = 0.10937546\n",
      "Iteration 240, loss = 0.10923422\n",
      "Iteration 241, loss = 0.10906320\n",
      "Iteration 242, loss = 0.10888387\n",
      "Iteration 243, loss = 0.10872044\n",
      "Iteration 244, loss = 0.10853354\n",
      "Iteration 245, loss = 0.10836319\n",
      "Iteration 246, loss = 0.10821955\n",
      "Iteration 247, loss = 0.10804731\n",
      "Iteration 248, loss = 0.10788654\n",
      "Iteration 249, loss = 0.10773346\n",
      "Iteration 250, loss = 0.10757829\n",
      "Iteration 251, loss = 0.10741199\n",
      "Iteration 252, loss = 0.10725074\n",
      "Iteration 253, loss = 0.10710121\n",
      "Iteration 254, loss = 0.10698832\n",
      "Iteration 255, loss = 0.10686258\n",
      "Iteration 256, loss = 0.10661620\n",
      "Iteration 257, loss = 0.10645637\n",
      "Iteration 258, loss = 0.10631044\n",
      "Iteration 259, loss = 0.10614804\n",
      "Iteration 260, loss = 0.10601242\n",
      "Iteration 261, loss = 0.10583689\n",
      "Iteration 262, loss = 0.10570296\n",
      "Iteration 263, loss = 0.10554919\n",
      "Iteration 264, loss = 0.10537989\n",
      "Iteration 265, loss = 0.10523088\n",
      "Iteration 266, loss = 0.10509369\n",
      "Iteration 267, loss = 0.10499273\n",
      "Iteration 268, loss = 0.10483123\n",
      "Iteration 269, loss = 0.10465739\n",
      "Iteration 270, loss = 0.10459282\n",
      "Iteration 271, loss = 0.10434189\n",
      "Iteration 272, loss = 0.10420098\n",
      "Iteration 273, loss = 0.10407753\n",
      "Iteration 274, loss = 0.10394225\n",
      "Iteration 275, loss = 0.10378967\n",
      "Iteration 276, loss = 0.10367871\n",
      "Iteration 277, loss = 0.10350663\n",
      "Iteration 278, loss = 0.10335834\n",
      "Iteration 279, loss = 0.10321328\n",
      "Iteration 280, loss = 0.10308298\n",
      "Iteration 281, loss = 0.10295498\n",
      "Iteration 282, loss = 0.10281324\n",
      "Iteration 283, loss = 0.10268365\n",
      "Iteration 284, loss = 0.10254464\n",
      "Iteration 285, loss = 0.10242876\n",
      "Iteration 286, loss = 0.10231770\n",
      "Iteration 287, loss = 0.10217805\n",
      "Iteration 288, loss = 0.10202137\n",
      "Iteration 289, loss = 0.10192346\n",
      "Iteration 290, loss = 0.10178436\n",
      "Iteration 291, loss = 0.10162655\n",
      "Iteration 292, loss = 0.10149107\n",
      "Iteration 293, loss = 0.10145474\n",
      "Iteration 294, loss = 0.10126677\n",
      "Iteration 295, loss = 0.10111856\n",
      "Iteration 296, loss = 0.10102916\n",
      "Iteration 297, loss = 0.10087832\n",
      "Iteration 298, loss = 0.10075112\n",
      "Iteration 299, loss = 0.10064741\n",
      "Iteration 300, loss = 0.10049130\n",
      "Iteration 301, loss = 0.10038732\n",
      "Iteration 302, loss = 0.10025730\n",
      "Iteration 303, loss = 0.10012041\n",
      "Iteration 304, loss = 0.10001902\n",
      "Iteration 305, loss = 0.09991800\n",
      "Iteration 306, loss = 0.09977961\n",
      "Iteration 307, loss = 0.09967861\n",
      "Iteration 308, loss = 0.09954233\n",
      "Iteration 309, loss = 0.09945677\n",
      "Iteration 310, loss = 0.09933845\n",
      "Iteration 311, loss = 0.09922788\n",
      "Iteration 312, loss = 0.09911227\n",
      "Iteration 313, loss = 0.09895875\n",
      "Iteration 314, loss = 0.09885012\n",
      "Iteration 315, loss = 0.09875153\n",
      "Iteration 316, loss = 0.09861528\n",
      "Iteration 317, loss = 0.09851107\n",
      "Iteration 318, loss = 0.09841707\n",
      "Iteration 319, loss = 0.09828788\n",
      "Iteration 320, loss = 0.09816606\n",
      "Iteration 321, loss = 0.09810834\n",
      "Iteration 322, loss = 0.09795865\n",
      "Iteration 323, loss = 0.09784778\n",
      "Iteration 324, loss = 0.09776480\n",
      "Iteration 325, loss = 0.09766581\n",
      "Iteration 326, loss = 0.09757104\n",
      "Iteration 327, loss = 0.09740985\n",
      "Iteration 328, loss = 0.09735369\n",
      "Iteration 329, loss = 0.09720496\n",
      "Iteration 330, loss = 0.09712058\n",
      "Iteration 331, loss = 0.09699792\n",
      "Iteration 332, loss = 0.09688365\n",
      "Iteration 333, loss = 0.09680307\n",
      "Iteration 334, loss = 0.09668075\n",
      "Iteration 335, loss = 0.09663868\n",
      "Iteration 336, loss = 0.09652339\n",
      "Iteration 337, loss = 0.09640412\n",
      "Iteration 338, loss = 0.09631133\n",
      "Iteration 339, loss = 0.09618958\n",
      "Iteration 340, loss = 0.09610598\n",
      "Iteration 341, loss = 0.09603451\n",
      "Iteration 342, loss = 0.09588218\n",
      "Iteration 343, loss = 0.09579819\n",
      "Iteration 344, loss = 0.09570535\n",
      "Iteration 345, loss = 0.09565076\n",
      "Iteration 346, loss = 0.09549605\n",
      "Iteration 347, loss = 0.09538606\n",
      "Iteration 348, loss = 0.09531448\n",
      "Iteration 349, loss = 0.09520060\n",
      "Iteration 350, loss = 0.09513447\n",
      "Iteration 351, loss = 0.09502558\n",
      "Iteration 352, loss = 0.09493302\n",
      "Iteration 353, loss = 0.09483639\n",
      "Iteration 354, loss = 0.09472751\n",
      "Iteration 355, loss = 0.09463464\n",
      "Iteration 356, loss = 0.09457666\n",
      "Iteration 357, loss = 0.09445919\n",
      "Iteration 358, loss = 0.09437234\n",
      "Iteration 359, loss = 0.09427140\n",
      "Iteration 360, loss = 0.09424915\n",
      "Iteration 361, loss = 0.09411547\n",
      "Iteration 362, loss = 0.09403082\n",
      "Iteration 363, loss = 0.09393221\n",
      "Iteration 364, loss = 0.09383257\n",
      "Iteration 365, loss = 0.09373430\n",
      "Iteration 366, loss = 0.09365952\n",
      "Iteration 367, loss = 0.09360982\n",
      "Iteration 368, loss = 0.09348165\n",
      "Iteration 369, loss = 0.09338212\n",
      "Iteration 370, loss = 0.09328416\n",
      "Iteration 371, loss = 0.09323227\n",
      "Iteration 372, loss = 0.09316966\n",
      "Iteration 373, loss = 0.09302657\n",
      "Iteration 374, loss = 0.09295104\n",
      "Iteration 375, loss = 0.09288202\n",
      "Iteration 376, loss = 0.09279969\n",
      "Iteration 377, loss = 0.09269520\n",
      "Iteration 378, loss = 0.09259745\n",
      "Iteration 379, loss = 0.09250505\n",
      "Iteration 380, loss = 0.09244773\n",
      "Iteration 381, loss = 0.09234146\n",
      "Iteration 382, loss = 0.09228993\n",
      "Iteration 383, loss = 0.09217714\n",
      "Iteration 384, loss = 0.09210712\n",
      "Iteration 385, loss = 0.09199263\n",
      "Iteration 386, loss = 0.09198047\n",
      "Iteration 387, loss = 0.09186050\n",
      "Iteration 388, loss = 0.09177137\n",
      "Iteration 389, loss = 0.09167730\n",
      "Iteration 390, loss = 0.09160477\n",
      "Iteration 391, loss = 0.09150208\n",
      "Iteration 392, loss = 0.09144451\n",
      "Iteration 393, loss = 0.09136519\n",
      "Iteration 394, loss = 0.09131504\n",
      "Iteration 395, loss = 0.09122180\n",
      "Iteration 396, loss = 0.09116719\n",
      "Iteration 397, loss = 0.09106646\n",
      "Iteration 398, loss = 0.09096873\n",
      "Iteration 399, loss = 0.09085758\n",
      "Iteration 400, loss = 0.09079565\n",
      "Iteration 401, loss = 0.09072992\n",
      "Iteration 402, loss = 0.09067407\n",
      "Iteration 403, loss = 0.09058545\n",
      "Iteration 404, loss = 0.09046774\n",
      "Iteration 405, loss = 0.09040111\n",
      "Iteration 406, loss = 0.09034066\n",
      "Iteration 407, loss = 0.09027041\n",
      "Iteration 408, loss = 0.09019040\n",
      "Iteration 409, loss = 0.09010612\n",
      "Iteration 410, loss = 0.09001549\n",
      "Iteration 411, loss = 0.08991695\n",
      "Iteration 412, loss = 0.08984770\n",
      "Iteration 413, loss = 0.08979734\n",
      "Iteration 414, loss = 0.08968265\n",
      "Iteration 415, loss = 0.08964807\n",
      "Iteration 416, loss = 0.08954984\n",
      "Iteration 417, loss = 0.08946638\n",
      "Iteration 418, loss = 0.08939679\n",
      "Iteration 419, loss = 0.08932382\n",
      "Iteration 420, loss = 0.08923217\n",
      "Iteration 421, loss = 0.08919063\n",
      "Iteration 422, loss = 0.08911238\n",
      "Iteration 423, loss = 0.08904017\n",
      "Iteration 424, loss = 0.08895348\n",
      "Iteration 425, loss = 0.08888301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72325331\n",
      "Iteration 2, loss = 0.67378103\n",
      "Iteration 3, loss = 0.63343703\n",
      "Iteration 4, loss = 0.59968741\n",
      "Iteration 5, loss = 0.57054502\n",
      "Iteration 6, loss = 0.54547284\n",
      "Iteration 7, loss = 0.52343091\n",
      "Iteration 8, loss = 0.50347690\n",
      "Iteration 9, loss = 0.48556085\n",
      "Iteration 10, loss = 0.46929846\n",
      "Iteration 11, loss = 0.45437053\n",
      "Iteration 12, loss = 0.44060523\n",
      "Iteration 13, loss = 0.42790559\n",
      "Iteration 14, loss = 0.41602137\n",
      "Iteration 15, loss = 0.40514355\n",
      "Iteration 16, loss = 0.39476368\n",
      "Iteration 17, loss = 0.38507003\n",
      "Iteration 18, loss = 0.37587638\n",
      "Iteration 19, loss = 0.36719336\n",
      "Iteration 20, loss = 0.35896310\n",
      "Iteration 21, loss = 0.35114058\n",
      "Iteration 22, loss = 0.34374837\n",
      "Iteration 23, loss = 0.33679978\n",
      "Iteration 24, loss = 0.33007660\n",
      "Iteration 25, loss = 0.32384362\n",
      "Iteration 26, loss = 0.31779064\n",
      "Iteration 27, loss = 0.31204075\n",
      "Iteration 28, loss = 0.30652674\n",
      "Iteration 29, loss = 0.30120087\n",
      "Iteration 30, loss = 0.29623242\n",
      "Iteration 31, loss = 0.29135293\n",
      "Iteration 32, loss = 0.28679008\n",
      "Iteration 33, loss = 0.28242804\n",
      "Iteration 34, loss = 0.27826000\n",
      "Iteration 35, loss = 0.27410653\n",
      "Iteration 36, loss = 0.27025534\n",
      "Iteration 37, loss = 0.26642734\n",
      "Iteration 38, loss = 0.26274052\n",
      "Iteration 39, loss = 0.25924761\n",
      "Iteration 40, loss = 0.25589282\n",
      "Iteration 41, loss = 0.25254736\n",
      "Iteration 42, loss = 0.24939899\n",
      "Iteration 43, loss = 0.24634461\n",
      "Iteration 44, loss = 0.24339950\n",
      "Iteration 45, loss = 0.24045982\n",
      "Iteration 46, loss = 0.23769517\n",
      "Iteration 47, loss = 0.23501262\n",
      "Iteration 48, loss = 0.23249053\n",
      "Iteration 49, loss = 0.22986752\n",
      "Iteration 50, loss = 0.22743817\n",
      "Iteration 51, loss = 0.22501925\n",
      "Iteration 52, loss = 0.22272277\n",
      "Iteration 53, loss = 0.22045125\n",
      "Iteration 54, loss = 0.21830029\n",
      "Iteration 55, loss = 0.21612852\n",
      "Iteration 56, loss = 0.21413893\n",
      "Iteration 57, loss = 0.21207439\n",
      "Iteration 58, loss = 0.21008999\n",
      "Iteration 59, loss = 0.20822624\n",
      "Iteration 60, loss = 0.20637445\n",
      "Iteration 61, loss = 0.20456517\n",
      "Iteration 62, loss = 0.20275447\n",
      "Iteration 63, loss = 0.20101367\n",
      "Iteration 64, loss = 0.19937188\n",
      "Iteration 65, loss = 0.19766865\n",
      "Iteration 66, loss = 0.19604157\n",
      "Iteration 67, loss = 0.19444405\n",
      "Iteration 68, loss = 0.19293313\n",
      "Iteration 69, loss = 0.19143054\n",
      "Iteration 70, loss = 0.18993250\n",
      "Iteration 71, loss = 0.18856461\n",
      "Iteration 72, loss = 0.18708653\n",
      "Iteration 73, loss = 0.18568834\n",
      "Iteration 74, loss = 0.18434939\n",
      "Iteration 75, loss = 0.18300415\n",
      "Iteration 76, loss = 0.18173625\n",
      "Iteration 77, loss = 0.18043551\n",
      "Iteration 78, loss = 0.17919599\n",
      "Iteration 79, loss = 0.17796012\n",
      "Iteration 80, loss = 0.17676701\n",
      "Iteration 81, loss = 0.17569111\n",
      "Iteration 82, loss = 0.17453442\n",
      "Iteration 83, loss = 0.17337736\n",
      "Iteration 84, loss = 0.17233368\n",
      "Iteration 85, loss = 0.17122820\n",
      "Iteration 86, loss = 0.17018733\n",
      "Iteration 87, loss = 0.16915947\n",
      "Iteration 88, loss = 0.16817740\n",
      "Iteration 89, loss = 0.16724016\n",
      "Iteration 90, loss = 0.16619234\n",
      "Iteration 91, loss = 0.16524656\n",
      "Iteration 92, loss = 0.16432606\n",
      "Iteration 93, loss = 0.16343030\n",
      "Iteration 94, loss = 0.16251028\n",
      "Iteration 95, loss = 0.16161587\n",
      "Iteration 96, loss = 0.16081398\n",
      "Iteration 97, loss = 0.15995310\n",
      "Iteration 98, loss = 0.15908460\n",
      "Iteration 99, loss = 0.15826381\n",
      "Iteration 100, loss = 0.15744711\n",
      "Iteration 101, loss = 0.15671621\n",
      "Iteration 102, loss = 0.15593902\n",
      "Iteration 103, loss = 0.15514123\n",
      "Iteration 104, loss = 0.15445078\n",
      "Iteration 105, loss = 0.15368636\n",
      "Iteration 106, loss = 0.15293601\n",
      "Iteration 107, loss = 0.15220440\n",
      "Iteration 108, loss = 0.15151812\n",
      "Iteration 109, loss = 0.15082723\n",
      "Iteration 110, loss = 0.15014923\n",
      "Iteration 111, loss = 0.14950616\n",
      "Iteration 112, loss = 0.14882194\n",
      "Iteration 113, loss = 0.14818207\n",
      "Iteration 114, loss = 0.14755714\n",
      "Iteration 115, loss = 0.14693024\n",
      "Iteration 116, loss = 0.14631969\n",
      "Iteration 117, loss = 0.14574651\n",
      "Iteration 118, loss = 0.14511038\n",
      "Iteration 119, loss = 0.14451790\n",
      "Iteration 120, loss = 0.14395595\n",
      "Iteration 121, loss = 0.14337963\n",
      "Iteration 122, loss = 0.14285163\n",
      "Iteration 123, loss = 0.14224229\n",
      "Iteration 124, loss = 0.14175413\n",
      "Iteration 125, loss = 0.14122888\n",
      "Iteration 126, loss = 0.14066927\n",
      "Iteration 127, loss = 0.14014832\n",
      "Iteration 128, loss = 0.13966337\n",
      "Iteration 129, loss = 0.13913808\n",
      "Iteration 130, loss = 0.13863169\n",
      "Iteration 131, loss = 0.13813593\n",
      "Iteration 132, loss = 0.13763037\n",
      "Iteration 133, loss = 0.13713450\n",
      "Iteration 134, loss = 0.13667518\n",
      "Iteration 135, loss = 0.13622944\n",
      "Iteration 136, loss = 0.13574307\n",
      "Iteration 137, loss = 0.13526529\n",
      "Iteration 138, loss = 0.13480251\n",
      "Iteration 139, loss = 0.13436400\n",
      "Iteration 140, loss = 0.13391553\n",
      "Iteration 141, loss = 0.13355255\n",
      "Iteration 142, loss = 0.13310728\n",
      "Iteration 143, loss = 0.13259210\n",
      "Iteration 144, loss = 0.13215127\n",
      "Iteration 145, loss = 0.13171800\n",
      "Iteration 146, loss = 0.13133360\n",
      "Iteration 147, loss = 0.13090108\n",
      "Iteration 148, loss = 0.13050362\n",
      "Iteration 149, loss = 0.13010469\n",
      "Iteration 150, loss = 0.12970347\n",
      "Iteration 151, loss = 0.12938713\n",
      "Iteration 152, loss = 0.12895459\n",
      "Iteration 153, loss = 0.12855647\n",
      "Iteration 154, loss = 0.12819526\n",
      "Iteration 155, loss = 0.12785644\n",
      "Iteration 156, loss = 0.12743448\n",
      "Iteration 157, loss = 0.12704498\n",
      "Iteration 158, loss = 0.12669811\n",
      "Iteration 159, loss = 0.12632865\n",
      "Iteration 160, loss = 0.12605561\n",
      "Iteration 161, loss = 0.12562673\n",
      "Iteration 162, loss = 0.12530420\n",
      "Iteration 163, loss = 0.12491938\n",
      "Iteration 164, loss = 0.12461212\n",
      "Iteration 165, loss = 0.12428461\n",
      "Iteration 166, loss = 0.12392859\n",
      "Iteration 167, loss = 0.12364499\n",
      "Iteration 168, loss = 0.12328412\n",
      "Iteration 169, loss = 0.12295833\n",
      "Iteration 170, loss = 0.12268547\n",
      "Iteration 171, loss = 0.12235930\n",
      "Iteration 172, loss = 0.12202031\n",
      "Iteration 173, loss = 0.12173770\n",
      "Iteration 174, loss = 0.12139974\n",
      "Iteration 175, loss = 0.12108502\n",
      "Iteration 176, loss = 0.12080060\n",
      "Iteration 177, loss = 0.12050607\n",
      "Iteration 178, loss = 0.12017934\n",
      "Iteration 179, loss = 0.11990276\n",
      "Iteration 180, loss = 0.11960753\n",
      "Iteration 181, loss = 0.11929743\n",
      "Iteration 182, loss = 0.11905113\n",
      "Iteration 183, loss = 0.11874663\n",
      "Iteration 184, loss = 0.11844018\n",
      "Iteration 185, loss = 0.11819139\n",
      "Iteration 186, loss = 0.11789276\n",
      "Iteration 187, loss = 0.11759623\n",
      "Iteration 188, loss = 0.11735792\n",
      "Iteration 189, loss = 0.11705471\n",
      "Iteration 190, loss = 0.11678625\n",
      "Iteration 191, loss = 0.11653687\n",
      "Iteration 192, loss = 0.11629503\n",
      "Iteration 193, loss = 0.11599810\n",
      "Iteration 194, loss = 0.11573188\n",
      "Iteration 195, loss = 0.11551850\n",
      "Iteration 196, loss = 0.11521181\n",
      "Iteration 197, loss = 0.11498031\n",
      "Iteration 198, loss = 0.11473261\n",
      "Iteration 199, loss = 0.11449340\n",
      "Iteration 200, loss = 0.11423914\n",
      "Iteration 201, loss = 0.11401136\n",
      "Iteration 202, loss = 0.11377777\n",
      "Iteration 203, loss = 0.11353757\n",
      "Iteration 204, loss = 0.11327337\n",
      "Iteration 205, loss = 0.11306594\n",
      "Iteration 206, loss = 0.11286488\n",
      "Iteration 207, loss = 0.11261315\n",
      "Iteration 208, loss = 0.11237202\n",
      "Iteration 209, loss = 0.11213302\n",
      "Iteration 210, loss = 0.11193882\n",
      "Iteration 211, loss = 0.11167148\n",
      "Iteration 212, loss = 0.11145360\n",
      "Iteration 213, loss = 0.11123119\n",
      "Iteration 214, loss = 0.11103480\n",
      "Iteration 215, loss = 0.11082022\n",
      "Iteration 216, loss = 0.11062710\n",
      "Iteration 217, loss = 0.11036152\n",
      "Iteration 218, loss = 0.11016617\n",
      "Iteration 219, loss = 0.10996256\n",
      "Iteration 220, loss = 0.10975414\n",
      "Iteration 221, loss = 0.10953563\n",
      "Iteration 222, loss = 0.10931492\n",
      "Iteration 223, loss = 0.10912982\n",
      "Iteration 224, loss = 0.10891800\n",
      "Iteration 225, loss = 0.10873601\n",
      "Iteration 226, loss = 0.10853062\n",
      "Iteration 227, loss = 0.10834228\n",
      "Iteration 228, loss = 0.10817518\n",
      "Iteration 229, loss = 0.10795395\n",
      "Iteration 230, loss = 0.10776697\n",
      "Iteration 231, loss = 0.10755886\n",
      "Iteration 232, loss = 0.10737581\n",
      "Iteration 233, loss = 0.10720515\n",
      "Iteration 234, loss = 0.10701089\n",
      "Iteration 235, loss = 0.10681601\n",
      "Iteration 236, loss = 0.10666740\n",
      "Iteration 237, loss = 0.10648725\n",
      "Iteration 238, loss = 0.10630334\n",
      "Iteration 239, loss = 0.10610649\n",
      "Iteration 240, loss = 0.10594146\n",
      "Iteration 241, loss = 0.10579020\n",
      "Iteration 242, loss = 0.10558752\n",
      "Iteration 243, loss = 0.10542322\n",
      "Iteration 244, loss = 0.10523033\n",
      "Iteration 245, loss = 0.10506731\n",
      "Iteration 246, loss = 0.10490258\n",
      "Iteration 247, loss = 0.10473353\n",
      "Iteration 248, loss = 0.10459224\n",
      "Iteration 249, loss = 0.10441017\n",
      "Iteration 250, loss = 0.10425066\n",
      "Iteration 251, loss = 0.10409261\n",
      "Iteration 252, loss = 0.10392175\n",
      "Iteration 253, loss = 0.10376578\n",
      "Iteration 254, loss = 0.10363597\n",
      "Iteration 255, loss = 0.10350233\n",
      "Iteration 256, loss = 0.10326276\n",
      "Iteration 257, loss = 0.10310159\n",
      "Iteration 258, loss = 0.10296777\n",
      "Iteration 259, loss = 0.10280756\n",
      "Iteration 260, loss = 0.10265308\n",
      "Iteration 261, loss = 0.10249518\n",
      "Iteration 262, loss = 0.10234889\n",
      "Iteration 263, loss = 0.10220769\n",
      "Iteration 264, loss = 0.10205852\n",
      "Iteration 265, loss = 0.10188256\n",
      "Iteration 266, loss = 0.10174660\n",
      "Iteration 267, loss = 0.10162708\n",
      "Iteration 268, loss = 0.10146663\n",
      "Iteration 269, loss = 0.10130079\n",
      "Iteration 270, loss = 0.10120709\n",
      "Iteration 271, loss = 0.10099536\n",
      "Iteration 272, loss = 0.10084713\n",
      "Iteration 273, loss = 0.10071949\n",
      "Iteration 274, loss = 0.10059262\n",
      "Iteration 275, loss = 0.10042318\n",
      "Iteration 276, loss = 0.10031787\n",
      "Iteration 277, loss = 0.10014351\n",
      "Iteration 278, loss = 0.10000456\n",
      "Iteration 279, loss = 0.09984156\n",
      "Iteration 280, loss = 0.09971504\n",
      "Iteration 281, loss = 0.09956987\n",
      "Iteration 282, loss = 0.09944092\n",
      "Iteration 283, loss = 0.09931529\n",
      "Iteration 284, loss = 0.09917239\n",
      "Iteration 285, loss = 0.09906305\n",
      "Iteration 286, loss = 0.09892112\n",
      "Iteration 287, loss = 0.09879407\n",
      "Iteration 288, loss = 0.09863436\n",
      "Iteration 289, loss = 0.09851290\n",
      "Iteration 290, loss = 0.09837776\n",
      "Iteration 291, loss = 0.09824085\n",
      "Iteration 292, loss = 0.09810202\n",
      "Iteration 293, loss = 0.09801313\n",
      "Iteration 294, loss = 0.09788580\n",
      "Iteration 295, loss = 0.09773184\n",
      "Iteration 296, loss = 0.09763276\n",
      "Iteration 297, loss = 0.09747060\n",
      "Iteration 298, loss = 0.09734608\n",
      "Iteration 299, loss = 0.09722733\n",
      "Iteration 300, loss = 0.09708313\n",
      "Iteration 301, loss = 0.09697647\n",
      "Iteration 302, loss = 0.09683659\n",
      "Iteration 303, loss = 0.09672266\n",
      "Iteration 304, loss = 0.09661083\n",
      "Iteration 305, loss = 0.09646669\n",
      "Iteration 306, loss = 0.09634295\n",
      "Iteration 307, loss = 0.09622698\n",
      "Iteration 308, loss = 0.09613490\n",
      "Iteration 309, loss = 0.09599021\n",
      "Iteration 310, loss = 0.09589348\n",
      "Iteration 311, loss = 0.09579405\n",
      "Iteration 312, loss = 0.09567610\n",
      "Iteration 313, loss = 0.09555163\n",
      "Iteration 314, loss = 0.09540941\n",
      "Iteration 315, loss = 0.09530812\n",
      "Iteration 316, loss = 0.09517884\n",
      "Iteration 317, loss = 0.09505657\n",
      "Iteration 318, loss = 0.09499089\n",
      "Iteration 319, loss = 0.09482003\n",
      "Iteration 320, loss = 0.09471862\n",
      "Iteration 321, loss = 0.09461765\n",
      "Iteration 322, loss = 0.09448976\n",
      "Iteration 323, loss = 0.09436982\n",
      "Iteration 324, loss = 0.09427365\n",
      "Iteration 325, loss = 0.09418659\n",
      "Iteration 326, loss = 0.09405307\n",
      "Iteration 327, loss = 0.09392145\n",
      "Iteration 328, loss = 0.09385057\n",
      "Iteration 329, loss = 0.09371193\n",
      "Iteration 330, loss = 0.09360887\n",
      "Iteration 331, loss = 0.09349319\n",
      "Iteration 332, loss = 0.09336755\n",
      "Iteration 333, loss = 0.09327626\n",
      "Iteration 334, loss = 0.09316647\n",
      "Iteration 335, loss = 0.09309931\n",
      "Iteration 336, loss = 0.09298268\n",
      "Iteration 337, loss = 0.09287355\n",
      "Iteration 338, loss = 0.09276573\n",
      "Iteration 339, loss = 0.09264106\n",
      "Iteration 340, loss = 0.09255377\n",
      "Iteration 341, loss = 0.09246655\n",
      "Iteration 342, loss = 0.09232605\n",
      "Iteration 343, loss = 0.09222726\n",
      "Iteration 344, loss = 0.09211809\n",
      "Iteration 345, loss = 0.09206780\n",
      "Iteration 346, loss = 0.09190292\n",
      "Iteration 347, loss = 0.09179953\n",
      "Iteration 348, loss = 0.09171177\n",
      "Iteration 349, loss = 0.09159918\n",
      "Iteration 350, loss = 0.09153358\n",
      "Iteration 351, loss = 0.09141273\n",
      "Iteration 352, loss = 0.09131095\n",
      "Iteration 353, loss = 0.09121754\n",
      "Iteration 354, loss = 0.09111930\n",
      "Iteration 355, loss = 0.09101008\n",
      "Iteration 356, loss = 0.09095218\n",
      "Iteration 357, loss = 0.09081806\n",
      "Iteration 358, loss = 0.09073583\n",
      "Iteration 359, loss = 0.09061831\n",
      "Iteration 360, loss = 0.09058619\n",
      "Iteration 361, loss = 0.09045649\n",
      "Iteration 362, loss = 0.09037212\n",
      "Iteration 363, loss = 0.09026127\n",
      "Iteration 364, loss = 0.09016988\n",
      "Iteration 365, loss = 0.09007681\n",
      "Iteration 366, loss = 0.08997547\n",
      "Iteration 367, loss = 0.08990513\n",
      "Iteration 368, loss = 0.08980772\n",
      "Iteration 369, loss = 0.08968449\n",
      "Iteration 370, loss = 0.08958953\n",
      "Iteration 371, loss = 0.08952929\n",
      "Iteration 372, loss = 0.08944853\n",
      "Iteration 373, loss = 0.08931399\n",
      "Iteration 374, loss = 0.08923188\n",
      "Iteration 375, loss = 0.08919256\n",
      "Iteration 376, loss = 0.08907513\n",
      "Iteration 377, loss = 0.08896192\n",
      "Iteration 378, loss = 0.08887363\n",
      "Iteration 379, loss = 0.08876454\n",
      "Iteration 380, loss = 0.08871885\n",
      "Iteration 381, loss = 0.08860387\n",
      "Iteration 382, loss = 0.08853023\n",
      "Iteration 383, loss = 0.08841375\n",
      "Iteration 384, loss = 0.08832998\n",
      "Iteration 385, loss = 0.08823588\n",
      "Iteration 386, loss = 0.08819475\n",
      "Iteration 387, loss = 0.08806222\n",
      "Iteration 388, loss = 0.08798152\n",
      "Iteration 389, loss = 0.08789520\n",
      "Iteration 390, loss = 0.08779604\n",
      "Iteration 391, loss = 0.08771829\n",
      "Iteration 392, loss = 0.08762560\n",
      "Iteration 393, loss = 0.08754798\n",
      "Iteration 394, loss = 0.08747502\n",
      "Iteration 395, loss = 0.08739410\n",
      "Iteration 396, loss = 0.08731721\n",
      "Iteration 397, loss = 0.08722233\n",
      "Iteration 398, loss = 0.08711681\n",
      "Iteration 399, loss = 0.08703996\n",
      "Iteration 400, loss = 0.08696353\n",
      "Iteration 401, loss = 0.08688224\n",
      "Iteration 402, loss = 0.08682128\n",
      "Iteration 403, loss = 0.08672207\n",
      "Iteration 404, loss = 0.08660928\n",
      "Iteration 405, loss = 0.08653400\n",
      "Iteration 406, loss = 0.08646543\n",
      "Iteration 407, loss = 0.08639594\n",
      "Iteration 408, loss = 0.08631408\n",
      "Iteration 409, loss = 0.08623654\n",
      "Iteration 410, loss = 0.08614722\n",
      "Iteration 411, loss = 0.08605545\n",
      "Iteration 412, loss = 0.08597863\n",
      "Iteration 413, loss = 0.08593857\n",
      "Iteration 414, loss = 0.08581541\n",
      "Iteration 415, loss = 0.08578874\n",
      "Iteration 416, loss = 0.08566869\n",
      "Iteration 417, loss = 0.08558185\n",
      "Iteration 418, loss = 0.08552078\n",
      "Iteration 419, loss = 0.08542580\n",
      "Iteration 420, loss = 0.08535036\n",
      "Iteration 421, loss = 0.08529508\n",
      "Iteration 422, loss = 0.08521425\n",
      "Iteration 423, loss = 0.08513963\n",
      "Iteration 424, loss = 0.08504951\n",
      "Iteration 425, loss = 0.08497288\n",
      "Iteration 426, loss = 0.08496069\n",
      "Iteration 427, loss = 0.08485731\n",
      "Iteration 428, loss = 0.08477657\n",
      "Iteration 429, loss = 0.08475709\n",
      "Iteration 430, loss = 0.08460633\n",
      "Iteration 431, loss = 0.08454055\n",
      "Iteration 432, loss = 0.08445448\n",
      "Iteration 433, loss = 0.08440561\n",
      "Iteration 434, loss = 0.08429773\n",
      "Iteration 435, loss = 0.08421845\n",
      "Iteration 436, loss = 0.08416442\n",
      "Iteration 437, loss = 0.08409078\n",
      "Iteration 438, loss = 0.08400783\n",
      "Iteration 439, loss = 0.08392777\n",
      "Iteration 440, loss = 0.08387259\n",
      "Iteration 441, loss = 0.08379751\n",
      "Iteration 442, loss = 0.08371711\n",
      "Iteration 443, loss = 0.08365195\n",
      "Iteration 444, loss = 0.08358184\n",
      "Iteration 445, loss = 0.08351642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 446, loss = 0.08342261\n",
      "Iteration 447, loss = 0.08340319\n",
      "Iteration 448, loss = 0.08339387\n",
      "Iteration 449, loss = 0.08338445\n",
      "Iteration 450, loss = 0.08336191\n",
      "Iteration 451, loss = 0.08335041\n",
      "Iteration 452, loss = 0.08333468\n",
      "Iteration 453, loss = 0.08332378\n",
      "Iteration 454, loss = 0.08331411\n",
      "Iteration 455, loss = 0.08329903\n",
      "Iteration 456, loss = 0.08328307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 457, loss = 0.08326375\n",
      "Iteration 458, loss = 0.08326079\n",
      "Iteration 459, loss = 0.08325752\n",
      "Iteration 460, loss = 0.08325541\n",
      "Iteration 461, loss = 0.08325282\n",
      "Iteration 462, loss = 0.08324958\n",
      "Iteration 463, loss = 0.08324746\n",
      "Iteration 464, loss = 0.08324354\n",
      "Iteration 465, loss = 0.08324075\n",
      "Iteration 466, loss = 0.08323834\n",
      "Iteration 467, loss = 0.08323619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 468, loss = 0.08323197\n",
      "Iteration 469, loss = 0.08323140\n",
      "Iteration 470, loss = 0.08323109\n",
      "Iteration 471, loss = 0.08323043\n",
      "Iteration 472, loss = 0.08322982\n",
      "Iteration 473, loss = 0.08322918\n",
      "Iteration 474, loss = 0.08322864\n",
      "Iteration 475, loss = 0.08322835\n",
      "Iteration 476, loss = 0.08322789\n",
      "Iteration 477, loss = 0.08322697\n",
      "Iteration 478, loss = 0.08322651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 479, loss = 0.08322567\n",
      "Iteration 480, loss = 0.08322563\n",
      "Iteration 481, loss = 0.08322546\n",
      "Iteration 482, loss = 0.08322535\n",
      "Iteration 483, loss = 0.08322527\n",
      "Iteration 484, loss = 0.08322515\n",
      "Iteration 485, loss = 0.08322503\n",
      "Iteration 486, loss = 0.08322492\n",
      "Iteration 487, loss = 0.08322483\n",
      "Iteration 488, loss = 0.08322468\n",
      "Iteration 489, loss = 0.08322463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 490, loss = 0.08322444\n",
      "Iteration 491, loss = 0.08322442\n",
      "Iteration 492, loss = 0.08322440\n",
      "Iteration 493, loss = 0.08322437\n",
      "Iteration 494, loss = 0.08322435\n",
      "Iteration 495, loss = 0.08322433\n",
      "Iteration 496, loss = 0.08322430\n",
      "Iteration 497, loss = 0.08322429\n",
      "Iteration 498, loss = 0.08322427\n",
      "Iteration 499, loss = 0.08322424\n",
      "Iteration 500, loss = 0.08322422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 501, loss = 0.08322419\n",
      "Iteration 502, loss = 0.08322419\n",
      "Iteration 503, loss = 0.08322418\n",
      "Iteration 504, loss = 0.08322418\n",
      "Iteration 505, loss = 0.08322417\n",
      "Iteration 506, loss = 0.08322417\n",
      "Iteration 507, loss = 0.08322416\n",
      "Iteration 508, loss = 0.08322416\n",
      "Iteration 509, loss = 0.08322416\n",
      "Iteration 510, loss = 0.08322415\n",
      "Iteration 511, loss = 0.08322415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72225996\n",
      "Iteration 2, loss = 0.67429048\n",
      "Iteration 3, loss = 0.63475718\n",
      "Iteration 4, loss = 0.60170504\n",
      "Iteration 5, loss = 0.57366158\n",
      "Iteration 6, loss = 0.54894725\n",
      "Iteration 7, loss = 0.52714377\n",
      "Iteration 8, loss = 0.50768992\n",
      "Iteration 9, loss = 0.49012342\n",
      "Iteration 10, loss = 0.47388077\n",
      "Iteration 11, loss = 0.45931507\n",
      "Iteration 12, loss = 0.44582576\n",
      "Iteration 13, loss = 0.43321106\n",
      "Iteration 14, loss = 0.42149957\n",
      "Iteration 15, loss = 0.41072398\n",
      "Iteration 16, loss = 0.40039458\n",
      "Iteration 17, loss = 0.39085447\n",
      "Iteration 18, loss = 0.38185038\n",
      "Iteration 19, loss = 0.37324341\n",
      "Iteration 20, loss = 0.36494267\n",
      "Iteration 21, loss = 0.35725455\n",
      "Iteration 22, loss = 0.34983351\n",
      "Iteration 23, loss = 0.34293723\n",
      "Iteration 24, loss = 0.33636557\n",
      "Iteration 25, loss = 0.32997720\n",
      "Iteration 26, loss = 0.32407957\n",
      "Iteration 27, loss = 0.31838144\n",
      "Iteration 28, loss = 0.31292747\n",
      "Iteration 29, loss = 0.30753456\n",
      "Iteration 30, loss = 0.30255126\n",
      "Iteration 31, loss = 0.29771418\n",
      "Iteration 32, loss = 0.29310735\n",
      "Iteration 33, loss = 0.28869184\n",
      "Iteration 34, loss = 0.28443065\n",
      "Iteration 35, loss = 0.28036999\n",
      "Iteration 36, loss = 0.27645667\n",
      "Iteration 37, loss = 0.27266398\n",
      "Iteration 38, loss = 0.26898727\n",
      "Iteration 39, loss = 0.26544895\n",
      "Iteration 40, loss = 0.26207188\n",
      "Iteration 41, loss = 0.25879125\n",
      "Iteration 42, loss = 0.25555801\n",
      "Iteration 43, loss = 0.25251579\n",
      "Iteration 44, loss = 0.24953330\n",
      "Iteration 45, loss = 0.24657159\n",
      "Iteration 46, loss = 0.24370711\n",
      "Iteration 47, loss = 0.24103445\n",
      "Iteration 48, loss = 0.23830864\n",
      "Iteration 49, loss = 0.23577718\n",
      "Iteration 50, loss = 0.23327859\n",
      "Iteration 51, loss = 0.23093451\n",
      "Iteration 52, loss = 0.22853801\n",
      "Iteration 53, loss = 0.22628164\n",
      "Iteration 54, loss = 0.22411733\n",
      "Iteration 55, loss = 0.22206486\n",
      "Iteration 56, loss = 0.21987491\n",
      "Iteration 57, loss = 0.21781903\n",
      "Iteration 58, loss = 0.21586513\n",
      "Iteration 59, loss = 0.21394968\n",
      "Iteration 60, loss = 0.21207183\n",
      "Iteration 61, loss = 0.21021482\n",
      "Iteration 62, loss = 0.20845993\n",
      "Iteration 63, loss = 0.20674700\n",
      "Iteration 64, loss = 0.20507817\n",
      "Iteration 65, loss = 0.20339047\n",
      "Iteration 66, loss = 0.20179439\n",
      "Iteration 67, loss = 0.20024866\n",
      "Iteration 68, loss = 0.19868224\n",
      "Iteration 69, loss = 0.19713831\n",
      "Iteration 70, loss = 0.19566105\n",
      "Iteration 71, loss = 0.19425635\n",
      "Iteration 72, loss = 0.19288339\n",
      "Iteration 73, loss = 0.19148322\n",
      "Iteration 74, loss = 0.19016660\n",
      "Iteration 75, loss = 0.18883424\n",
      "Iteration 76, loss = 0.18752294\n",
      "Iteration 77, loss = 0.18631375\n",
      "Iteration 78, loss = 0.18510891\n",
      "Iteration 79, loss = 0.18392549\n",
      "Iteration 80, loss = 0.18272266\n",
      "Iteration 81, loss = 0.18156344\n",
      "Iteration 82, loss = 0.18046516\n",
      "Iteration 83, loss = 0.17944102\n",
      "Iteration 84, loss = 0.17831058\n",
      "Iteration 85, loss = 0.17719418\n",
      "Iteration 86, loss = 0.17615045\n",
      "Iteration 87, loss = 0.17520428\n",
      "Iteration 88, loss = 0.17410939\n",
      "Iteration 89, loss = 0.17319098\n",
      "Iteration 90, loss = 0.17222674\n",
      "Iteration 91, loss = 0.17124421\n",
      "Iteration 92, loss = 0.17035366\n",
      "Iteration 93, loss = 0.16943400\n",
      "Iteration 94, loss = 0.16859370\n",
      "Iteration 95, loss = 0.16767597\n",
      "Iteration 96, loss = 0.16684185\n",
      "Iteration 97, loss = 0.16597232\n",
      "Iteration 98, loss = 0.16520584\n",
      "Iteration 99, loss = 0.16436813\n",
      "Iteration 100, loss = 0.16354445\n",
      "Iteration 101, loss = 0.16285439\n",
      "Iteration 102, loss = 0.16198641\n",
      "Iteration 103, loss = 0.16125629\n",
      "Iteration 104, loss = 0.16048017\n",
      "Iteration 105, loss = 0.15971158\n",
      "Iteration 106, loss = 0.15902150\n",
      "Iteration 107, loss = 0.15827114\n",
      "Iteration 108, loss = 0.15753995\n",
      "Iteration 109, loss = 0.15683594\n",
      "Iteration 110, loss = 0.15614910\n",
      "Iteration 111, loss = 0.15548776\n",
      "Iteration 112, loss = 0.15481478\n",
      "Iteration 113, loss = 0.15421851\n",
      "Iteration 114, loss = 0.15354783\n",
      "Iteration 115, loss = 0.15288856\n",
      "Iteration 116, loss = 0.15227909\n",
      "Iteration 117, loss = 0.15164111\n",
      "Iteration 118, loss = 0.15110931\n",
      "Iteration 119, loss = 0.15054425\n",
      "Iteration 120, loss = 0.14988948\n",
      "Iteration 121, loss = 0.14933629\n",
      "Iteration 122, loss = 0.14880386\n",
      "Iteration 123, loss = 0.14821317\n",
      "Iteration 124, loss = 0.14767964\n",
      "Iteration 125, loss = 0.14712612\n",
      "Iteration 126, loss = 0.14658881\n",
      "Iteration 127, loss = 0.14608197\n",
      "Iteration 128, loss = 0.14560401\n",
      "Iteration 129, loss = 0.14512691\n",
      "Iteration 130, loss = 0.14459373\n",
      "Iteration 131, loss = 0.14417218\n",
      "Iteration 132, loss = 0.14360569\n",
      "Iteration 133, loss = 0.14311918\n",
      "Iteration 134, loss = 0.14263971\n",
      "Iteration 135, loss = 0.14215545\n",
      "Iteration 136, loss = 0.14179282\n",
      "Iteration 137, loss = 0.14124725\n",
      "Iteration 138, loss = 0.14079215\n",
      "Iteration 139, loss = 0.14032141\n",
      "Iteration 140, loss = 0.13989147\n",
      "Iteration 141, loss = 0.13946782\n",
      "Iteration 142, loss = 0.13903780\n",
      "Iteration 143, loss = 0.13857139\n",
      "Iteration 144, loss = 0.13817216\n",
      "Iteration 145, loss = 0.13778204\n",
      "Iteration 146, loss = 0.13733107\n",
      "Iteration 147, loss = 0.13696017\n",
      "Iteration 148, loss = 0.13648817\n",
      "Iteration 149, loss = 0.13615079\n",
      "Iteration 150, loss = 0.13575687\n",
      "Iteration 151, loss = 0.13536463\n",
      "Iteration 152, loss = 0.13498499\n",
      "Iteration 153, loss = 0.13459860\n",
      "Iteration 154, loss = 0.13416397\n",
      "Iteration 155, loss = 0.13381426\n",
      "Iteration 156, loss = 0.13345933\n",
      "Iteration 157, loss = 0.13309237\n",
      "Iteration 158, loss = 0.13273532\n",
      "Iteration 159, loss = 0.13236315\n",
      "Iteration 160, loss = 0.13209880\n",
      "Iteration 161, loss = 0.13177247\n",
      "Iteration 162, loss = 0.13136487\n",
      "Iteration 163, loss = 0.13099926\n",
      "Iteration 164, loss = 0.13071911\n",
      "Iteration 165, loss = 0.13038658\n",
      "Iteration 166, loss = 0.13002441\n",
      "Iteration 167, loss = 0.12974309\n",
      "Iteration 168, loss = 0.12947309\n",
      "Iteration 169, loss = 0.12910686\n",
      "Iteration 170, loss = 0.12875733\n",
      "Iteration 171, loss = 0.12845434\n",
      "Iteration 172, loss = 0.12819111\n",
      "Iteration 173, loss = 0.12783843\n",
      "Iteration 174, loss = 0.12755766\n",
      "Iteration 175, loss = 0.12724816\n",
      "Iteration 176, loss = 0.12694114\n",
      "Iteration 177, loss = 0.12667014\n",
      "Iteration 178, loss = 0.12641867\n",
      "Iteration 179, loss = 0.12616328\n",
      "Iteration 180, loss = 0.12581005\n",
      "Iteration 181, loss = 0.12553505\n",
      "Iteration 182, loss = 0.12530294\n",
      "Iteration 183, loss = 0.12499470\n",
      "Iteration 184, loss = 0.12473251\n",
      "Iteration 185, loss = 0.12447267\n",
      "Iteration 186, loss = 0.12420762\n",
      "Iteration 187, loss = 0.12395130\n",
      "Iteration 188, loss = 0.12368232\n",
      "Iteration 189, loss = 0.12346133\n",
      "Iteration 190, loss = 0.12320156\n",
      "Iteration 191, loss = 0.12291391\n",
      "Iteration 192, loss = 0.12267836\n",
      "Iteration 193, loss = 0.12247792\n",
      "Iteration 194, loss = 0.12225310\n",
      "Iteration 195, loss = 0.12195250\n",
      "Iteration 196, loss = 0.12170978\n",
      "Iteration 197, loss = 0.12150232\n",
      "Iteration 198, loss = 0.12126540\n",
      "Iteration 199, loss = 0.12099698\n",
      "Iteration 200, loss = 0.12074731\n",
      "Iteration 201, loss = 0.12056553\n",
      "Iteration 202, loss = 0.12029572\n",
      "Iteration 203, loss = 0.12006237\n",
      "Iteration 204, loss = 0.11984422\n",
      "Iteration 205, loss = 0.11962994\n",
      "Iteration 206, loss = 0.11937783\n",
      "Iteration 207, loss = 0.11924785\n",
      "Iteration 208, loss = 0.11895592\n",
      "Iteration 209, loss = 0.11872881\n",
      "Iteration 210, loss = 0.11854450\n",
      "Iteration 211, loss = 0.11829374\n",
      "Iteration 212, loss = 0.11813911\n",
      "Iteration 213, loss = 0.11788072\n",
      "Iteration 214, loss = 0.11774201\n",
      "Iteration 215, loss = 0.11748933\n",
      "Iteration 216, loss = 0.11724593\n",
      "Iteration 217, loss = 0.11703628\n",
      "Iteration 218, loss = 0.11681748\n",
      "Iteration 219, loss = 0.11664219\n",
      "Iteration 220, loss = 0.11641728\n",
      "Iteration 221, loss = 0.11624688\n",
      "Iteration 222, loss = 0.11602029\n",
      "Iteration 223, loss = 0.11581305\n",
      "Iteration 224, loss = 0.11563101\n",
      "Iteration 225, loss = 0.11542209\n",
      "Iteration 226, loss = 0.11526640\n",
      "Iteration 227, loss = 0.11515523\n",
      "Iteration 228, loss = 0.11486765\n",
      "Iteration 229, loss = 0.11468058\n",
      "Iteration 230, loss = 0.11454056\n",
      "Iteration 231, loss = 0.11432443\n",
      "Iteration 232, loss = 0.11418042\n",
      "Iteration 233, loss = 0.11398495\n",
      "Iteration 234, loss = 0.11381789\n",
      "Iteration 235, loss = 0.11358996\n",
      "Iteration 236, loss = 0.11343910\n",
      "Iteration 237, loss = 0.11336794\n",
      "Iteration 238, loss = 0.11308226\n",
      "Iteration 239, loss = 0.11288656\n",
      "Iteration 240, loss = 0.11272624\n",
      "Iteration 241, loss = 0.11257680\n",
      "Iteration 242, loss = 0.11242677\n",
      "Iteration 243, loss = 0.11225327\n",
      "Iteration 244, loss = 0.11208625\n",
      "Iteration 245, loss = 0.11193281\n",
      "Iteration 246, loss = 0.11176286\n",
      "Iteration 247, loss = 0.11162457\n",
      "Iteration 248, loss = 0.11142644\n",
      "Iteration 249, loss = 0.11127984\n",
      "Iteration 250, loss = 0.11116604\n",
      "Iteration 251, loss = 0.11092775\n",
      "Iteration 252, loss = 0.11083578\n",
      "Iteration 253, loss = 0.11070082\n",
      "Iteration 254, loss = 0.11046889\n",
      "Iteration 255, loss = 0.11032988\n",
      "Iteration 256, loss = 0.11020143\n",
      "Iteration 257, loss = 0.11000780\n",
      "Iteration 258, loss = 0.10993424\n",
      "Iteration 259, loss = 0.10970731\n",
      "Iteration 260, loss = 0.10957240\n",
      "Iteration 261, loss = 0.10940656\n",
      "Iteration 262, loss = 0.10927157\n",
      "Iteration 263, loss = 0.10911736\n",
      "Iteration 264, loss = 0.10896459\n",
      "Iteration 265, loss = 0.10891452\n",
      "Iteration 266, loss = 0.10867462\n",
      "Iteration 267, loss = 0.10853268\n",
      "Iteration 268, loss = 0.10839763\n",
      "Iteration 269, loss = 0.10824533\n",
      "Iteration 270, loss = 0.10811563\n",
      "Iteration 271, loss = 0.10799544\n",
      "Iteration 272, loss = 0.10783307\n",
      "Iteration 273, loss = 0.10767907\n",
      "Iteration 274, loss = 0.10754221\n",
      "Iteration 275, loss = 0.10740637\n",
      "Iteration 276, loss = 0.10725167\n",
      "Iteration 277, loss = 0.10713017\n",
      "Iteration 278, loss = 0.10699448\n",
      "Iteration 279, loss = 0.10684609\n",
      "Iteration 280, loss = 0.10670466\n",
      "Iteration 281, loss = 0.10657362\n",
      "Iteration 282, loss = 0.10644305\n",
      "Iteration 283, loss = 0.10628611\n",
      "Iteration 284, loss = 0.10617705\n",
      "Iteration 285, loss = 0.10603437\n",
      "Iteration 286, loss = 0.10587860\n",
      "Iteration 287, loss = 0.10579997\n",
      "Iteration 288, loss = 0.10562329\n",
      "Iteration 289, loss = 0.10549805\n",
      "Iteration 290, loss = 0.10537024\n",
      "Iteration 291, loss = 0.10522755\n",
      "Iteration 292, loss = 0.10510992\n",
      "Iteration 293, loss = 0.10497711\n",
      "Iteration 294, loss = 0.10485002\n",
      "Iteration 295, loss = 0.10474536\n",
      "Iteration 296, loss = 0.10459972\n",
      "Iteration 297, loss = 0.10451390\n",
      "Iteration 298, loss = 0.10435714\n",
      "Iteration 299, loss = 0.10422827\n",
      "Iteration 300, loss = 0.10415693\n",
      "Iteration 301, loss = 0.10403645\n",
      "Iteration 302, loss = 0.10385643\n",
      "Iteration 303, loss = 0.10373184\n",
      "Iteration 304, loss = 0.10363705\n",
      "Iteration 305, loss = 0.10351343\n",
      "Iteration 306, loss = 0.10341434\n",
      "Iteration 307, loss = 0.10325356\n",
      "Iteration 308, loss = 0.10312452\n",
      "Iteration 309, loss = 0.10303044\n",
      "Iteration 310, loss = 0.10288224\n",
      "Iteration 311, loss = 0.10279097\n",
      "Iteration 312, loss = 0.10269058\n",
      "Iteration 313, loss = 0.10257339\n",
      "Iteration 314, loss = 0.10242268\n",
      "Iteration 315, loss = 0.10233112\n",
      "Iteration 316, loss = 0.10219651\n",
      "Iteration 317, loss = 0.10206654\n",
      "Iteration 318, loss = 0.10194848\n",
      "Iteration 319, loss = 0.10184772\n",
      "Iteration 320, loss = 0.10172599\n",
      "Iteration 321, loss = 0.10163844\n",
      "Iteration 322, loss = 0.10153618\n",
      "Iteration 323, loss = 0.10144088\n",
      "Iteration 324, loss = 0.10125169\n",
      "Iteration 325, loss = 0.10118095\n",
      "Iteration 326, loss = 0.10106388\n",
      "Iteration 327, loss = 0.10092278\n",
      "Iteration 328, loss = 0.10080359\n",
      "Iteration 329, loss = 0.10071609\n",
      "Iteration 330, loss = 0.10057389\n",
      "Iteration 331, loss = 0.10049144\n",
      "Iteration 332, loss = 0.10037510\n",
      "Iteration 333, loss = 0.10023728\n",
      "Iteration 334, loss = 0.10013690\n",
      "Iteration 335, loss = 0.10002336\n",
      "Iteration 336, loss = 0.09997668\n",
      "Iteration 337, loss = 0.09980297\n",
      "Iteration 338, loss = 0.09971373\n",
      "Iteration 339, loss = 0.09958857\n",
      "Iteration 340, loss = 0.09949807\n",
      "Iteration 341, loss = 0.09937200\n",
      "Iteration 342, loss = 0.09926936\n",
      "Iteration 343, loss = 0.09914431\n",
      "Iteration 344, loss = 0.09904701\n",
      "Iteration 345, loss = 0.09897094\n",
      "Iteration 346, loss = 0.09882261\n",
      "Iteration 347, loss = 0.09877631\n",
      "Iteration 348, loss = 0.09864990\n",
      "Iteration 349, loss = 0.09853605\n",
      "Iteration 350, loss = 0.09842715\n",
      "Iteration 351, loss = 0.09832272\n",
      "Iteration 352, loss = 0.09822068\n",
      "Iteration 353, loss = 0.09809804\n",
      "Iteration 354, loss = 0.09802099\n",
      "Iteration 355, loss = 0.09790559\n",
      "Iteration 356, loss = 0.09779221\n",
      "Iteration 357, loss = 0.09769639\n",
      "Iteration 358, loss = 0.09760340\n",
      "Iteration 359, loss = 0.09753454\n",
      "Iteration 360, loss = 0.09744469\n",
      "Iteration 361, loss = 0.09729143\n",
      "Iteration 362, loss = 0.09722972\n",
      "Iteration 363, loss = 0.09710384\n",
      "Iteration 364, loss = 0.09699648\n",
      "Iteration 365, loss = 0.09691424\n",
      "Iteration 366, loss = 0.09679608\n",
      "Iteration 367, loss = 0.09670140\n",
      "Iteration 368, loss = 0.09665043\n",
      "Iteration 369, loss = 0.09651344\n",
      "Iteration 370, loss = 0.09641756\n",
      "Iteration 371, loss = 0.09633383\n",
      "Iteration 372, loss = 0.09623859\n",
      "Iteration 373, loss = 0.09611783\n",
      "Iteration 374, loss = 0.09604479\n",
      "Iteration 375, loss = 0.09593780\n",
      "Iteration 376, loss = 0.09583820\n",
      "Iteration 377, loss = 0.09573773\n",
      "Iteration 378, loss = 0.09565389\n",
      "Iteration 379, loss = 0.09555325\n",
      "Iteration 380, loss = 0.09549608\n",
      "Iteration 381, loss = 0.09537977\n",
      "Iteration 382, loss = 0.09533881\n",
      "Iteration 383, loss = 0.09520164\n",
      "Iteration 384, loss = 0.09508598\n",
      "Iteration 385, loss = 0.09499496\n",
      "Iteration 386, loss = 0.09488727\n",
      "Iteration 387, loss = 0.09481059\n",
      "Iteration 388, loss = 0.09471153\n",
      "Iteration 389, loss = 0.09462316\n",
      "Iteration 390, loss = 0.09455116\n",
      "Iteration 391, loss = 0.09442490\n",
      "Iteration 392, loss = 0.09435107\n",
      "Iteration 393, loss = 0.09426431\n",
      "Iteration 394, loss = 0.09416478\n",
      "Iteration 395, loss = 0.09412890\n",
      "Iteration 396, loss = 0.09399214\n",
      "Iteration 397, loss = 0.09394775\n",
      "Iteration 398, loss = 0.09382299\n",
      "Iteration 399, loss = 0.09372114\n",
      "Iteration 400, loss = 0.09363854\n",
      "Iteration 401, loss = 0.09356660\n",
      "Iteration 402, loss = 0.09346697\n",
      "Iteration 403, loss = 0.09336834\n",
      "Iteration 404, loss = 0.09331124\n",
      "Iteration 405, loss = 0.09322031\n",
      "Iteration 406, loss = 0.09313355\n",
      "Iteration 407, loss = 0.09306993\n",
      "Iteration 408, loss = 0.09296983\n",
      "Iteration 409, loss = 0.09289165\n",
      "Iteration 410, loss = 0.09280133\n",
      "Iteration 411, loss = 0.09273593\n",
      "Iteration 412, loss = 0.09261994\n",
      "Iteration 413, loss = 0.09254887\n",
      "Iteration 414, loss = 0.09244395\n",
      "Iteration 415, loss = 0.09237452\n",
      "Iteration 416, loss = 0.09229058\n",
      "Iteration 417, loss = 0.09219636\n",
      "Iteration 418, loss = 0.09212082\n",
      "Iteration 419, loss = 0.09204591\n",
      "Iteration 420, loss = 0.09196820\n",
      "Iteration 421, loss = 0.09192408\n",
      "Iteration 422, loss = 0.09178568\n",
      "Iteration 423, loss = 0.09173728\n",
      "Iteration 424, loss = 0.09164348\n",
      "Iteration 425, loss = 0.09157771\n",
      "Iteration 426, loss = 0.09146572\n",
      "Iteration 427, loss = 0.09139776\n",
      "Iteration 428, loss = 0.09134038\n",
      "Iteration 429, loss = 0.09128895\n",
      "Iteration 430, loss = 0.09125642\n",
      "Iteration 431, loss = 0.09109683\n",
      "Iteration 432, loss = 0.09101198\n",
      "Iteration 433, loss = 0.09091862\n",
      "Iteration 434, loss = 0.09083386\n",
      "Iteration 435, loss = 0.09077234\n",
      "Iteration 436, loss = 0.09071318\n",
      "Iteration 437, loss = 0.09063298\n",
      "Iteration 438, loss = 0.09052794\n",
      "Iteration 439, loss = 0.09046805\n",
      "Iteration 440, loss = 0.09042309\n",
      "Iteration 441, loss = 0.09034877\n",
      "Iteration 442, loss = 0.09024986\n",
      "Iteration 443, loss = 0.09015100\n",
      "Iteration 444, loss = 0.09009747\n",
      "Iteration 445, loss = 0.09000001\n",
      "Iteration 446, loss = 0.08994946\n",
      "Iteration 447, loss = 0.08983987\n",
      "Iteration 448, loss = 0.08978419\n",
      "Iteration 449, loss = 0.08972794\n",
      "Iteration 450, loss = 0.08964369\n",
      "Iteration 451, loss = 0.08956051\n",
      "Iteration 452, loss = 0.08946524\n",
      "Iteration 453, loss = 0.08942671\n",
      "Iteration 454, loss = 0.08932006\n",
      "Iteration 455, loss = 0.08926495\n",
      "Iteration 456, loss = 0.08926273\n",
      "Iteration 457, loss = 0.08916900\n",
      "Iteration 458, loss = 0.08906975\n",
      "Iteration 459, loss = 0.08897885\n",
      "Iteration 460, loss = 0.08891759\n",
      "Iteration 461, loss = 0.08884856\n",
      "Iteration 462, loss = 0.08875120\n",
      "Iteration 463, loss = 0.08873255\n",
      "Iteration 464, loss = 0.08863634\n",
      "Iteration 465, loss = 0.08854283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72250151\n",
      "Iteration 2, loss = 0.67389257\n",
      "Iteration 3, loss = 0.63413714\n",
      "Iteration 4, loss = 0.60115967\n",
      "Iteration 5, loss = 0.57313914\n",
      "Iteration 6, loss = 0.54838100\n",
      "Iteration 7, loss = 0.52661621\n",
      "Iteration 8, loss = 0.50727482\n",
      "Iteration 9, loss = 0.48974779\n",
      "Iteration 10, loss = 0.47377008\n",
      "Iteration 11, loss = 0.45937120\n",
      "Iteration 12, loss = 0.44622305\n",
      "Iteration 13, loss = 0.43400907\n",
      "Iteration 14, loss = 0.42258147\n",
      "Iteration 15, loss = 0.41195560\n",
      "Iteration 16, loss = 0.40174335\n",
      "Iteration 17, loss = 0.39223099\n",
      "Iteration 18, loss = 0.38327019\n",
      "Iteration 19, loss = 0.37470830\n",
      "Iteration 20, loss = 0.36657509\n",
      "Iteration 21, loss = 0.35894882\n",
      "Iteration 22, loss = 0.35162314\n",
      "Iteration 23, loss = 0.34481503\n",
      "Iteration 24, loss = 0.33823939\n",
      "Iteration 25, loss = 0.33193789\n",
      "Iteration 26, loss = 0.32600049\n",
      "Iteration 27, loss = 0.32035828\n",
      "Iteration 28, loss = 0.31487232\n",
      "Iteration 29, loss = 0.30959662\n",
      "Iteration 30, loss = 0.30461965\n",
      "Iteration 31, loss = 0.29982141\n",
      "Iteration 32, loss = 0.29523164\n",
      "Iteration 33, loss = 0.29088049\n",
      "Iteration 34, loss = 0.28667213\n",
      "Iteration 35, loss = 0.28266244\n",
      "Iteration 36, loss = 0.27872154\n",
      "Iteration 37, loss = 0.27496693\n",
      "Iteration 38, loss = 0.27132390\n",
      "Iteration 39, loss = 0.26784812\n",
      "Iteration 40, loss = 0.26453613\n",
      "Iteration 41, loss = 0.26125743\n",
      "Iteration 42, loss = 0.25803223\n",
      "Iteration 43, loss = 0.25502323\n",
      "Iteration 44, loss = 0.25204028\n",
      "Iteration 45, loss = 0.24912575\n",
      "Iteration 46, loss = 0.24628940\n",
      "Iteration 47, loss = 0.24363489\n",
      "Iteration 48, loss = 0.24100201\n",
      "Iteration 49, loss = 0.23851352\n",
      "Iteration 50, loss = 0.23600795\n",
      "Iteration 51, loss = 0.23371324\n",
      "Iteration 52, loss = 0.23141220\n",
      "Iteration 53, loss = 0.22914658\n",
      "Iteration 54, loss = 0.22702457\n",
      "Iteration 55, loss = 0.22494050\n",
      "Iteration 56, loss = 0.22285446\n",
      "Iteration 57, loss = 0.22081304\n",
      "Iteration 58, loss = 0.21888104\n",
      "Iteration 59, loss = 0.21698553\n",
      "Iteration 60, loss = 0.21513997\n",
      "Iteration 61, loss = 0.21328507\n",
      "Iteration 62, loss = 0.21155199\n",
      "Iteration 63, loss = 0.20982936\n",
      "Iteration 64, loss = 0.20813551\n",
      "Iteration 65, loss = 0.20646451\n",
      "Iteration 66, loss = 0.20489804\n",
      "Iteration 67, loss = 0.20327835\n",
      "Iteration 68, loss = 0.20174443\n",
      "Iteration 69, loss = 0.20016859\n",
      "Iteration 70, loss = 0.19870533\n",
      "Iteration 71, loss = 0.19726236\n",
      "Iteration 72, loss = 0.19588700\n",
      "Iteration 73, loss = 0.19445301\n",
      "Iteration 74, loss = 0.19311204\n",
      "Iteration 75, loss = 0.19178170\n",
      "Iteration 76, loss = 0.19045146\n",
      "Iteration 77, loss = 0.18920399\n",
      "Iteration 78, loss = 0.18795878\n",
      "Iteration 79, loss = 0.18671788\n",
      "Iteration 80, loss = 0.18552459\n",
      "Iteration 81, loss = 0.18431047\n",
      "Iteration 82, loss = 0.18320375\n",
      "Iteration 83, loss = 0.18214518\n",
      "Iteration 84, loss = 0.18102925\n",
      "Iteration 85, loss = 0.17986651\n",
      "Iteration 86, loss = 0.17880839\n",
      "Iteration 87, loss = 0.17780260\n",
      "Iteration 88, loss = 0.17667547\n",
      "Iteration 89, loss = 0.17569947\n",
      "Iteration 90, loss = 0.17468024\n",
      "Iteration 91, loss = 0.17365560\n",
      "Iteration 92, loss = 0.17271583\n",
      "Iteration 93, loss = 0.17174014\n",
      "Iteration 94, loss = 0.17087556\n",
      "Iteration 95, loss = 0.16990072\n",
      "Iteration 96, loss = 0.16901188\n",
      "Iteration 97, loss = 0.16813584\n",
      "Iteration 98, loss = 0.16732550\n",
      "Iteration 99, loss = 0.16648812\n",
      "Iteration 100, loss = 0.16563459\n",
      "Iteration 101, loss = 0.16488932\n",
      "Iteration 102, loss = 0.16404806\n",
      "Iteration 103, loss = 0.16329155\n",
      "Iteration 104, loss = 0.16252299\n",
      "Iteration 105, loss = 0.16173637\n",
      "Iteration 106, loss = 0.16102909\n",
      "Iteration 107, loss = 0.16028597\n",
      "Iteration 108, loss = 0.15956893\n",
      "Iteration 109, loss = 0.15886616\n",
      "Iteration 110, loss = 0.15817374\n",
      "Iteration 111, loss = 0.15751316\n",
      "Iteration 112, loss = 0.15683653\n",
      "Iteration 113, loss = 0.15622768\n",
      "Iteration 114, loss = 0.15553196\n",
      "Iteration 115, loss = 0.15488797\n",
      "Iteration 116, loss = 0.15424882\n",
      "Iteration 117, loss = 0.15363447\n",
      "Iteration 118, loss = 0.15306994\n",
      "Iteration 119, loss = 0.15249838\n",
      "Iteration 120, loss = 0.15182693\n",
      "Iteration 121, loss = 0.15126565\n",
      "Iteration 122, loss = 0.15071355\n",
      "Iteration 123, loss = 0.15016016\n",
      "Iteration 124, loss = 0.14958864\n",
      "Iteration 125, loss = 0.14903822\n",
      "Iteration 126, loss = 0.14847677\n",
      "Iteration 127, loss = 0.14796031\n",
      "Iteration 128, loss = 0.14746566\n",
      "Iteration 129, loss = 0.14695623\n",
      "Iteration 130, loss = 0.14640333\n",
      "Iteration 131, loss = 0.14597406\n",
      "Iteration 132, loss = 0.14539768\n",
      "Iteration 133, loss = 0.14491496\n",
      "Iteration 134, loss = 0.14442095\n",
      "Iteration 135, loss = 0.14395162\n",
      "Iteration 136, loss = 0.14356316\n",
      "Iteration 137, loss = 0.14304659\n",
      "Iteration 138, loss = 0.14257100\n",
      "Iteration 139, loss = 0.14210740\n",
      "Iteration 140, loss = 0.14166549\n",
      "Iteration 141, loss = 0.14121911\n",
      "Iteration 142, loss = 0.14081058\n",
      "Iteration 143, loss = 0.14034198\n",
      "Iteration 144, loss = 0.13994213\n",
      "Iteration 145, loss = 0.13955684\n",
      "Iteration 146, loss = 0.13909715\n",
      "Iteration 147, loss = 0.13870066\n",
      "Iteration 148, loss = 0.13821504\n",
      "Iteration 149, loss = 0.13787336\n",
      "Iteration 150, loss = 0.13744271\n",
      "Iteration 151, loss = 0.13703515\n",
      "Iteration 152, loss = 0.13666579\n",
      "Iteration 153, loss = 0.13624947\n",
      "Iteration 154, loss = 0.13582596\n",
      "Iteration 155, loss = 0.13545264\n",
      "Iteration 156, loss = 0.13508089\n",
      "Iteration 157, loss = 0.13471364\n",
      "Iteration 158, loss = 0.13432678\n",
      "Iteration 159, loss = 0.13394761\n",
      "Iteration 160, loss = 0.13364628\n",
      "Iteration 161, loss = 0.13334628\n",
      "Iteration 162, loss = 0.13290481\n",
      "Iteration 163, loss = 0.13250692\n",
      "Iteration 164, loss = 0.13219258\n",
      "Iteration 165, loss = 0.13185119\n",
      "Iteration 166, loss = 0.13147513\n",
      "Iteration 167, loss = 0.13118382\n",
      "Iteration 168, loss = 0.13088937\n",
      "Iteration 169, loss = 0.13051388\n",
      "Iteration 170, loss = 0.13014933\n",
      "Iteration 171, loss = 0.12982389\n",
      "Iteration 172, loss = 0.12952135\n",
      "Iteration 173, loss = 0.12916299\n",
      "Iteration 174, loss = 0.12886455\n",
      "Iteration 175, loss = 0.12854656\n",
      "Iteration 176, loss = 0.12821310\n",
      "Iteration 177, loss = 0.12792944\n",
      "Iteration 178, loss = 0.12763412\n",
      "Iteration 179, loss = 0.12735966\n",
      "Iteration 180, loss = 0.12700202\n",
      "Iteration 181, loss = 0.12670106\n",
      "Iteration 182, loss = 0.12644683\n",
      "Iteration 183, loss = 0.12612209\n",
      "Iteration 184, loss = 0.12584959\n",
      "Iteration 185, loss = 0.12556555\n",
      "Iteration 186, loss = 0.12526471\n",
      "Iteration 187, loss = 0.12499555\n",
      "Iteration 188, loss = 0.12471045\n",
      "Iteration 189, loss = 0.12447732\n",
      "Iteration 190, loss = 0.12418489\n",
      "Iteration 191, loss = 0.12388855\n",
      "Iteration 192, loss = 0.12361976\n",
      "Iteration 193, loss = 0.12339241\n",
      "Iteration 194, loss = 0.12314429\n",
      "Iteration 195, loss = 0.12283694\n",
      "Iteration 196, loss = 0.12258380\n",
      "Iteration 197, loss = 0.12235357\n",
      "Iteration 198, loss = 0.12208718\n",
      "Iteration 199, loss = 0.12182216\n",
      "Iteration 200, loss = 0.12154551\n",
      "Iteration 201, loss = 0.12132866\n",
      "Iteration 202, loss = 0.12106634\n",
      "Iteration 203, loss = 0.12081172\n",
      "Iteration 204, loss = 0.12057431\n",
      "Iteration 205, loss = 0.12035811\n",
      "Iteration 206, loss = 0.12007766\n",
      "Iteration 207, loss = 0.11990890\n",
      "Iteration 208, loss = 0.11961204\n",
      "Iteration 209, loss = 0.11938639\n",
      "Iteration 210, loss = 0.11917676\n",
      "Iteration 211, loss = 0.11891650\n",
      "Iteration 212, loss = 0.11876962\n",
      "Iteration 213, loss = 0.11848210\n",
      "Iteration 214, loss = 0.11837052\n",
      "Iteration 215, loss = 0.11807504\n",
      "Iteration 216, loss = 0.11784745\n",
      "Iteration 217, loss = 0.11763094\n",
      "Iteration 218, loss = 0.11740218\n",
      "Iteration 219, loss = 0.11721826\n",
      "Iteration 220, loss = 0.11698184\n",
      "Iteration 221, loss = 0.11680609\n",
      "Iteration 222, loss = 0.11658783\n",
      "Iteration 223, loss = 0.11637960\n",
      "Iteration 224, loss = 0.11620105\n",
      "Iteration 225, loss = 0.11597782\n",
      "Iteration 226, loss = 0.11580974\n",
      "Iteration 227, loss = 0.11568361\n",
      "Iteration 228, loss = 0.11538883\n",
      "Iteration 229, loss = 0.11521462\n",
      "Iteration 230, loss = 0.11505226\n",
      "Iteration 231, loss = 0.11483043\n",
      "Iteration 232, loss = 0.11466358\n",
      "Iteration 233, loss = 0.11447659\n",
      "Iteration 234, loss = 0.11429088\n",
      "Iteration 235, loss = 0.11404089\n",
      "Iteration 236, loss = 0.11390053\n",
      "Iteration 237, loss = 0.11380367\n",
      "Iteration 238, loss = 0.11351746\n",
      "Iteration 239, loss = 0.11330985\n",
      "Iteration 240, loss = 0.11313888\n",
      "Iteration 241, loss = 0.11298250\n",
      "Iteration 242, loss = 0.11281967\n",
      "Iteration 243, loss = 0.11264032\n",
      "Iteration 244, loss = 0.11245581\n",
      "Iteration 245, loss = 0.11231030\n",
      "Iteration 246, loss = 0.11211569\n",
      "Iteration 247, loss = 0.11196713\n",
      "Iteration 248, loss = 0.11177127\n",
      "Iteration 249, loss = 0.11160091\n",
      "Iteration 250, loss = 0.11148818\n",
      "Iteration 251, loss = 0.11124619\n",
      "Iteration 252, loss = 0.11113187\n",
      "Iteration 253, loss = 0.11100215\n",
      "Iteration 254, loss = 0.11074927\n",
      "Iteration 255, loss = 0.11059996\n",
      "Iteration 256, loss = 0.11046172\n",
      "Iteration 257, loss = 0.11027070\n",
      "Iteration 258, loss = 0.11019366\n",
      "Iteration 259, loss = 0.10994891\n",
      "Iteration 260, loss = 0.10982132\n",
      "Iteration 261, loss = 0.10963641\n",
      "Iteration 262, loss = 0.10949611\n",
      "Iteration 263, loss = 0.10934299\n",
      "Iteration 264, loss = 0.10917696\n",
      "Iteration 265, loss = 0.10912952\n",
      "Iteration 266, loss = 0.10887759\n",
      "Iteration 267, loss = 0.10873566\n",
      "Iteration 268, loss = 0.10859681\n",
      "Iteration 269, loss = 0.10843342\n",
      "Iteration 270, loss = 0.10831260\n",
      "Iteration 271, loss = 0.10817896\n",
      "Iteration 272, loss = 0.10801266\n",
      "Iteration 273, loss = 0.10787739\n",
      "Iteration 274, loss = 0.10772653\n",
      "Iteration 275, loss = 0.10758152\n",
      "Iteration 276, loss = 0.10742162\n",
      "Iteration 277, loss = 0.10728970\n",
      "Iteration 278, loss = 0.10716406\n",
      "Iteration 279, loss = 0.10700707\n",
      "Iteration 280, loss = 0.10687179\n",
      "Iteration 281, loss = 0.10673864\n",
      "Iteration 282, loss = 0.10659725\n",
      "Iteration 283, loss = 0.10644643\n",
      "Iteration 284, loss = 0.10632897\n",
      "Iteration 285, loss = 0.10619670\n",
      "Iteration 286, loss = 0.10602284\n",
      "Iteration 287, loss = 0.10594814\n",
      "Iteration 288, loss = 0.10576802\n",
      "Iteration 289, loss = 0.10564244\n",
      "Iteration 290, loss = 0.10551484\n",
      "Iteration 291, loss = 0.10536188\n",
      "Iteration 292, loss = 0.10524628\n",
      "Iteration 293, loss = 0.10510339\n",
      "Iteration 294, loss = 0.10498253\n",
      "Iteration 295, loss = 0.10486947\n",
      "Iteration 296, loss = 0.10472026\n",
      "Iteration 297, loss = 0.10462643\n",
      "Iteration 298, loss = 0.10447237\n",
      "Iteration 299, loss = 0.10433788\n",
      "Iteration 300, loss = 0.10425183\n",
      "Iteration 301, loss = 0.10414466\n",
      "Iteration 302, loss = 0.10395483\n",
      "Iteration 303, loss = 0.10382703\n",
      "Iteration 304, loss = 0.10374104\n",
      "Iteration 305, loss = 0.10360861\n",
      "Iteration 306, loss = 0.10351578\n",
      "Iteration 307, loss = 0.10333859\n",
      "Iteration 308, loss = 0.10321656\n",
      "Iteration 309, loss = 0.10312787\n",
      "Iteration 310, loss = 0.10298549\n",
      "Iteration 311, loss = 0.10288536\n",
      "Iteration 312, loss = 0.10278511\n",
      "Iteration 313, loss = 0.10268176\n",
      "Iteration 314, loss = 0.10252465\n",
      "Iteration 315, loss = 0.10242973\n",
      "Iteration 316, loss = 0.10230964\n",
      "Iteration 317, loss = 0.10218040\n",
      "Iteration 318, loss = 0.10206599\n",
      "Iteration 319, loss = 0.10197719\n",
      "Iteration 320, loss = 0.10184715\n",
      "Iteration 321, loss = 0.10176778\n",
      "Iteration 322, loss = 0.10166636\n",
      "Iteration 323, loss = 0.10154443\n",
      "Iteration 324, loss = 0.10137807\n",
      "Iteration 325, loss = 0.10132398\n",
      "Iteration 326, loss = 0.10119594\n",
      "Iteration 327, loss = 0.10106401\n",
      "Iteration 328, loss = 0.10095303\n",
      "Iteration 329, loss = 0.10086843\n",
      "Iteration 330, loss = 0.10073865\n",
      "Iteration 331, loss = 0.10065723\n",
      "Iteration 332, loss = 0.10055852\n",
      "Iteration 333, loss = 0.10041364\n",
      "Iteration 334, loss = 0.10031078\n",
      "Iteration 335, loss = 0.10021874\n",
      "Iteration 336, loss = 0.10017985\n",
      "Iteration 337, loss = 0.10000773\n",
      "Iteration 338, loss = 0.09992181\n",
      "Iteration 339, loss = 0.09979518\n",
      "Iteration 340, loss = 0.09970948\n",
      "Iteration 341, loss = 0.09958831\n",
      "Iteration 342, loss = 0.09949778\n",
      "Iteration 343, loss = 0.09938075\n",
      "Iteration 344, loss = 0.09929950\n",
      "Iteration 345, loss = 0.09922611\n",
      "Iteration 346, loss = 0.09907198\n",
      "Iteration 347, loss = 0.09903854\n",
      "Iteration 348, loss = 0.09890401\n",
      "Iteration 349, loss = 0.09880361\n",
      "Iteration 350, loss = 0.09868892\n",
      "Iteration 351, loss = 0.09860430\n",
      "Iteration 352, loss = 0.09851176\n",
      "Iteration 353, loss = 0.09839576\n",
      "Iteration 354, loss = 0.09833252\n",
      "Iteration 355, loss = 0.09822122\n",
      "Iteration 356, loss = 0.09810713\n",
      "Iteration 357, loss = 0.09801811\n",
      "Iteration 358, loss = 0.09793163\n",
      "Iteration 359, loss = 0.09786682\n",
      "Iteration 360, loss = 0.09778742\n",
      "Iteration 361, loss = 0.09764855\n",
      "Iteration 362, loss = 0.09759100\n",
      "Iteration 363, loss = 0.09747596\n",
      "Iteration 364, loss = 0.09737536\n",
      "Iteration 365, loss = 0.09729882\n",
      "Iteration 366, loss = 0.09719132\n",
      "Iteration 367, loss = 0.09711011\n",
      "Iteration 368, loss = 0.09705884\n",
      "Iteration 369, loss = 0.09692334\n",
      "Iteration 370, loss = 0.09683695\n",
      "Iteration 371, loss = 0.09677438\n",
      "Iteration 372, loss = 0.09667905\n",
      "Iteration 373, loss = 0.09656430\n",
      "Iteration 374, loss = 0.09650586\n",
      "Iteration 375, loss = 0.09640260\n",
      "Iteration 376, loss = 0.09631391\n",
      "Iteration 377, loss = 0.09622393\n",
      "Iteration 378, loss = 0.09614154\n",
      "Iteration 379, loss = 0.09605508\n",
      "Iteration 380, loss = 0.09599555\n",
      "Iteration 381, loss = 0.09589405\n",
      "Iteration 382, loss = 0.09586969\n",
      "Iteration 383, loss = 0.09573411\n",
      "Iteration 384, loss = 0.09562341\n",
      "Iteration 385, loss = 0.09554595\n",
      "Iteration 386, loss = 0.09544665\n",
      "Iteration 387, loss = 0.09537773\n",
      "Iteration 388, loss = 0.09529318\n",
      "Iteration 389, loss = 0.09520437\n",
      "Iteration 390, loss = 0.09514459\n",
      "Iteration 391, loss = 0.09502607\n",
      "Iteration 392, loss = 0.09496603\n",
      "Iteration 393, loss = 0.09488645\n",
      "Iteration 394, loss = 0.09479813\n",
      "Iteration 395, loss = 0.09475700\n",
      "Iteration 396, loss = 0.09463965\n",
      "Iteration 397, loss = 0.09458013\n",
      "Iteration 398, loss = 0.09447662\n",
      "Iteration 399, loss = 0.09438660\n",
      "Iteration 400, loss = 0.09431421\n",
      "Iteration 401, loss = 0.09424649\n",
      "Iteration 402, loss = 0.09415469\n",
      "Iteration 403, loss = 0.09405048\n",
      "Iteration 404, loss = 0.09399335\n",
      "Iteration 405, loss = 0.09391853\n",
      "Iteration 406, loss = 0.09383917\n",
      "Iteration 407, loss = 0.09376692\n",
      "Iteration 408, loss = 0.09368372\n",
      "Iteration 409, loss = 0.09361257\n",
      "Iteration 410, loss = 0.09351524\n",
      "Iteration 411, loss = 0.09346405\n",
      "Iteration 412, loss = 0.09335867\n",
      "Iteration 413, loss = 0.09328056\n",
      "Iteration 414, loss = 0.09319063\n",
      "Iteration 415, loss = 0.09311503\n",
      "Iteration 416, loss = 0.09303986\n",
      "Iteration 417, loss = 0.09295305\n",
      "Iteration 418, loss = 0.09287710\n",
      "Iteration 419, loss = 0.09279688\n",
      "Iteration 420, loss = 0.09272654\n",
      "Iteration 421, loss = 0.09269783\n",
      "Iteration 422, loss = 0.09256026\n",
      "Iteration 423, loss = 0.09251328\n",
      "Iteration 424, loss = 0.09242517\n",
      "Iteration 425, loss = 0.09234190\n",
      "Iteration 426, loss = 0.09225615\n",
      "Iteration 427, loss = 0.09219367\n",
      "Iteration 428, loss = 0.09213397\n",
      "Iteration 429, loss = 0.09207397\n",
      "Iteration 430, loss = 0.09202316\n",
      "Iteration 431, loss = 0.09190068\n",
      "Iteration 432, loss = 0.09182903\n",
      "Iteration 433, loss = 0.09173199\n",
      "Iteration 434, loss = 0.09164495\n",
      "Iteration 435, loss = 0.09159047\n",
      "Iteration 436, loss = 0.09153675\n",
      "Iteration 437, loss = 0.09145507\n",
      "Iteration 438, loss = 0.09135117\n",
      "Iteration 439, loss = 0.09131354\n",
      "Iteration 440, loss = 0.09125214\n",
      "Iteration 441, loss = 0.09117811\n",
      "Iteration 442, loss = 0.09108971\n",
      "Iteration 443, loss = 0.09100223\n",
      "Iteration 444, loss = 0.09095477\n",
      "Iteration 445, loss = 0.09085584\n",
      "Iteration 446, loss = 0.09080554\n",
      "Iteration 447, loss = 0.09069598\n",
      "Iteration 448, loss = 0.09064202\n",
      "Iteration 449, loss = 0.09057561\n",
      "Iteration 450, loss = 0.09051509\n",
      "Iteration 451, loss = 0.09042279\n",
      "Iteration 452, loss = 0.09032996\n",
      "Iteration 453, loss = 0.09027816\n",
      "Iteration 454, loss = 0.09018465\n",
      "Iteration 455, loss = 0.09012493\n",
      "Iteration 456, loss = 0.09013365\n",
      "Iteration 457, loss = 0.09005179\n",
      "Iteration 458, loss = 0.08992859\n",
      "Iteration 459, loss = 0.08985127\n",
      "Iteration 460, loss = 0.08978650\n",
      "Iteration 461, loss = 0.08971796\n",
      "Iteration 462, loss = 0.08962008\n",
      "Iteration 463, loss = 0.08960962\n",
      "Iteration 464, loss = 0.08950249\n",
      "Iteration 465, loss = 0.08940362\n",
      "Iteration 466, loss = 0.08934861\n",
      "Iteration 467, loss = 0.08927206\n",
      "Iteration 468, loss = 0.08922578\n",
      "Iteration 469, loss = 0.08919457\n",
      "Iteration 470, loss = 0.08908018\n",
      "Iteration 471, loss = 0.08901107\n",
      "Iteration 472, loss = 0.08896376\n",
      "Iteration 473, loss = 0.08885976\n",
      "Iteration 474, loss = 0.08879688\n",
      "Iteration 475, loss = 0.08874992\n",
      "Iteration 476, loss = 0.08865963\n",
      "Iteration 477, loss = 0.08860219\n",
      "Iteration 478, loss = 0.08852242\n",
      "Iteration 479, loss = 0.08846466\n",
      "Iteration 480, loss = 0.08840487\n",
      "Iteration 481, loss = 0.08836747\n",
      "Iteration 482, loss = 0.08827873\n",
      "Iteration 483, loss = 0.08821878\n",
      "Iteration 484, loss = 0.08814618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.71675898\n",
      "Iteration 2, loss = 0.66778529\n",
      "Iteration 3, loss = 0.62735563\n",
      "Iteration 4, loss = 0.59371446\n",
      "Iteration 5, loss = 0.56503705\n",
      "Iteration 6, loss = 0.53988213\n",
      "Iteration 7, loss = 0.51771668\n",
      "Iteration 8, loss = 0.49790041\n",
      "Iteration 9, loss = 0.47999361\n",
      "Iteration 10, loss = 0.46361357\n",
      "Iteration 11, loss = 0.44880776\n",
      "Iteration 12, loss = 0.43512631\n",
      "Iteration 13, loss = 0.42260008\n",
      "Iteration 14, loss = 0.41089145\n",
      "Iteration 15, loss = 0.40004371\n",
      "Iteration 16, loss = 0.38973081\n",
      "Iteration 17, loss = 0.38009197\n",
      "Iteration 18, loss = 0.37094797\n",
      "Iteration 19, loss = 0.36230342\n",
      "Iteration 20, loss = 0.35406392\n",
      "Iteration 21, loss = 0.34635771\n",
      "Iteration 22, loss = 0.33899020\n",
      "Iteration 23, loss = 0.33216198\n",
      "Iteration 24, loss = 0.32554509\n",
      "Iteration 25, loss = 0.31919105\n",
      "Iteration 26, loss = 0.31323644\n",
      "Iteration 27, loss = 0.30758297\n",
      "Iteration 28, loss = 0.30212540\n",
      "Iteration 29, loss = 0.29681238\n",
      "Iteration 30, loss = 0.29183982\n",
      "Iteration 31, loss = 0.28702502\n",
      "Iteration 32, loss = 0.28242977\n",
      "Iteration 33, loss = 0.27809954\n",
      "Iteration 34, loss = 0.27391503\n",
      "Iteration 35, loss = 0.26994607\n",
      "Iteration 36, loss = 0.26598282\n",
      "Iteration 37, loss = 0.26227254\n",
      "Iteration 38, loss = 0.25866536\n",
      "Iteration 39, loss = 0.25521151\n",
      "Iteration 40, loss = 0.25186026\n",
      "Iteration 41, loss = 0.24860853\n",
      "Iteration 42, loss = 0.24539007\n",
      "Iteration 43, loss = 0.24236048\n",
      "Iteration 44, loss = 0.23935990\n",
      "Iteration 45, loss = 0.23641425\n",
      "Iteration 46, loss = 0.23354187\n",
      "Iteration 47, loss = 0.23081563\n",
      "Iteration 48, loss = 0.22818096\n",
      "Iteration 49, loss = 0.22561566\n",
      "Iteration 50, loss = 0.22304351\n",
      "Iteration 51, loss = 0.22071796\n",
      "Iteration 52, loss = 0.21830665\n",
      "Iteration 53, loss = 0.21595867\n",
      "Iteration 54, loss = 0.21374077\n",
      "Iteration 55, loss = 0.21161972\n",
      "Iteration 56, loss = 0.20944802\n",
      "Iteration 57, loss = 0.20739097\n",
      "Iteration 58, loss = 0.20535178\n",
      "Iteration 59, loss = 0.20338723\n",
      "Iteration 60, loss = 0.20150353\n",
      "Iteration 61, loss = 0.19958731\n",
      "Iteration 62, loss = 0.19774942\n",
      "Iteration 63, loss = 0.19601552\n",
      "Iteration 64, loss = 0.19429900\n",
      "Iteration 65, loss = 0.19255264\n",
      "Iteration 66, loss = 0.19098395\n",
      "Iteration 67, loss = 0.18929246\n",
      "Iteration 68, loss = 0.18770082\n",
      "Iteration 69, loss = 0.18610834\n",
      "Iteration 70, loss = 0.18458439\n",
      "Iteration 71, loss = 0.18308405\n",
      "Iteration 72, loss = 0.18166939\n",
      "Iteration 73, loss = 0.18018004\n",
      "Iteration 74, loss = 0.17880488\n",
      "Iteration 75, loss = 0.17738891\n",
      "Iteration 76, loss = 0.17603209\n",
      "Iteration 77, loss = 0.17469280\n",
      "Iteration 78, loss = 0.17342069\n",
      "Iteration 79, loss = 0.17208638\n",
      "Iteration 80, loss = 0.17083985\n",
      "Iteration 81, loss = 0.16955736\n",
      "Iteration 82, loss = 0.16839086\n",
      "Iteration 83, loss = 0.16725206\n",
      "Iteration 84, loss = 0.16609624\n",
      "Iteration 85, loss = 0.16498756\n",
      "Iteration 86, loss = 0.16386037\n",
      "Iteration 87, loss = 0.16281620\n",
      "Iteration 88, loss = 0.16171027\n",
      "Iteration 89, loss = 0.16068329\n",
      "Iteration 90, loss = 0.15964075\n",
      "Iteration 91, loss = 0.15864390\n",
      "Iteration 92, loss = 0.15767601\n",
      "Iteration 93, loss = 0.15672357\n",
      "Iteration 94, loss = 0.15579683\n",
      "Iteration 95, loss = 0.15485447\n",
      "Iteration 96, loss = 0.15396112\n",
      "Iteration 97, loss = 0.15305131\n",
      "Iteration 98, loss = 0.15220378\n",
      "Iteration 99, loss = 0.15135579\n",
      "Iteration 100, loss = 0.15050465\n",
      "Iteration 101, loss = 0.14971147\n",
      "Iteration 102, loss = 0.14887661\n",
      "Iteration 103, loss = 0.14809528\n",
      "Iteration 104, loss = 0.14731320\n",
      "Iteration 105, loss = 0.14653872\n",
      "Iteration 106, loss = 0.14582941\n",
      "Iteration 107, loss = 0.14506338\n",
      "Iteration 108, loss = 0.14432583\n",
      "Iteration 109, loss = 0.14360655\n",
      "Iteration 110, loss = 0.14292278\n",
      "Iteration 111, loss = 0.14225354\n",
      "Iteration 112, loss = 0.14157052\n",
      "Iteration 113, loss = 0.14091676\n",
      "Iteration 114, loss = 0.14023213\n",
      "Iteration 115, loss = 0.13958301\n",
      "Iteration 116, loss = 0.13892647\n",
      "Iteration 117, loss = 0.13830358\n",
      "Iteration 118, loss = 0.13771289\n",
      "Iteration 119, loss = 0.13715554\n",
      "Iteration 120, loss = 0.13647329\n",
      "Iteration 121, loss = 0.13590898\n",
      "Iteration 122, loss = 0.13534190\n",
      "Iteration 123, loss = 0.13477087\n",
      "Iteration 124, loss = 0.13420240\n",
      "Iteration 125, loss = 0.13363168\n",
      "Iteration 126, loss = 0.13304045\n",
      "Iteration 127, loss = 0.13251973\n",
      "Iteration 128, loss = 0.13197989\n",
      "Iteration 129, loss = 0.13144159\n",
      "Iteration 130, loss = 0.13090976\n",
      "Iteration 131, loss = 0.13042898\n",
      "Iteration 132, loss = 0.12990688\n",
      "Iteration 133, loss = 0.12940065\n",
      "Iteration 134, loss = 0.12889171\n",
      "Iteration 135, loss = 0.12842166\n",
      "Iteration 136, loss = 0.12794524\n",
      "Iteration 137, loss = 0.12748634\n",
      "Iteration 138, loss = 0.12700742\n",
      "Iteration 139, loss = 0.12656693\n",
      "Iteration 140, loss = 0.12609566\n",
      "Iteration 141, loss = 0.12564564\n",
      "Iteration 142, loss = 0.12525287\n",
      "Iteration 143, loss = 0.12478979\n",
      "Iteration 144, loss = 0.12438110\n",
      "Iteration 145, loss = 0.12397103\n",
      "Iteration 146, loss = 0.12352433\n",
      "Iteration 147, loss = 0.12309373\n",
      "Iteration 148, loss = 0.12266559\n",
      "Iteration 149, loss = 0.12230831\n",
      "Iteration 150, loss = 0.12190028\n",
      "Iteration 151, loss = 0.12145496\n",
      "Iteration 152, loss = 0.12110553\n",
      "Iteration 153, loss = 0.12068741\n",
      "Iteration 154, loss = 0.12029517\n",
      "Iteration 155, loss = 0.11990999\n",
      "Iteration 156, loss = 0.11954875\n",
      "Iteration 157, loss = 0.11915885\n",
      "Iteration 158, loss = 0.11880348\n",
      "Iteration 159, loss = 0.11844282\n",
      "Iteration 160, loss = 0.11812165\n",
      "Iteration 161, loss = 0.11781300\n",
      "Iteration 162, loss = 0.11740072\n",
      "Iteration 163, loss = 0.11703525\n",
      "Iteration 164, loss = 0.11669437\n",
      "Iteration 165, loss = 0.11636709\n",
      "Iteration 166, loss = 0.11602388\n",
      "Iteration 167, loss = 0.11572041\n",
      "Iteration 168, loss = 0.11540840\n",
      "Iteration 169, loss = 0.11506212\n",
      "Iteration 170, loss = 0.11473630\n",
      "Iteration 171, loss = 0.11441347\n",
      "Iteration 172, loss = 0.11409199\n",
      "Iteration 173, loss = 0.11379940\n",
      "Iteration 174, loss = 0.11350983\n",
      "Iteration 175, loss = 0.11319259\n",
      "Iteration 176, loss = 0.11289348\n",
      "Iteration 177, loss = 0.11259127\n",
      "Iteration 178, loss = 0.11231577\n",
      "Iteration 179, loss = 0.11202573\n",
      "Iteration 180, loss = 0.11173223\n",
      "Iteration 181, loss = 0.11144182\n",
      "Iteration 182, loss = 0.11116852\n",
      "Iteration 183, loss = 0.11086834\n",
      "Iteration 184, loss = 0.11058074\n",
      "Iteration 185, loss = 0.11032525\n",
      "Iteration 186, loss = 0.11001001\n",
      "Iteration 187, loss = 0.10973192\n",
      "Iteration 188, loss = 0.10946165\n",
      "Iteration 189, loss = 0.10921325\n",
      "Iteration 190, loss = 0.10893378\n",
      "Iteration 191, loss = 0.10866692\n",
      "Iteration 192, loss = 0.10837223\n",
      "Iteration 193, loss = 0.10813002\n",
      "Iteration 194, loss = 0.10789665\n",
      "Iteration 195, loss = 0.10759370\n",
      "Iteration 196, loss = 0.10735116\n",
      "Iteration 197, loss = 0.10710573\n",
      "Iteration 198, loss = 0.10685266\n",
      "Iteration 199, loss = 0.10660860\n",
      "Iteration 200, loss = 0.10635330\n",
      "Iteration 201, loss = 0.10613659\n",
      "Iteration 202, loss = 0.10589378\n",
      "Iteration 203, loss = 0.10565219\n",
      "Iteration 204, loss = 0.10541327\n",
      "Iteration 205, loss = 0.10520669\n",
      "Iteration 206, loss = 0.10496233\n",
      "Iteration 207, loss = 0.10476882\n",
      "Iteration 208, loss = 0.10451242\n",
      "Iteration 209, loss = 0.10426626\n",
      "Iteration 210, loss = 0.10406470\n",
      "Iteration 211, loss = 0.10385272\n",
      "Iteration 212, loss = 0.10365986\n",
      "Iteration 213, loss = 0.10338933\n",
      "Iteration 214, loss = 0.10323582\n",
      "Iteration 215, loss = 0.10299890\n",
      "Iteration 216, loss = 0.10276540\n",
      "Iteration 217, loss = 0.10256324\n",
      "Iteration 218, loss = 0.10234751\n",
      "Iteration 219, loss = 0.10219019\n",
      "Iteration 220, loss = 0.10194417\n",
      "Iteration 221, loss = 0.10175849\n",
      "Iteration 222, loss = 0.10156776\n",
      "Iteration 223, loss = 0.10136349\n",
      "Iteration 224, loss = 0.10119484\n",
      "Iteration 225, loss = 0.10097154\n",
      "Iteration 226, loss = 0.10081477\n",
      "Iteration 227, loss = 0.10062368\n",
      "Iteration 228, loss = 0.10040124\n",
      "Iteration 229, loss = 0.10026289\n",
      "Iteration 230, loss = 0.10007207\n",
      "Iteration 231, loss = 0.09985650\n",
      "Iteration 232, loss = 0.09968306\n",
      "Iteration 233, loss = 0.09951969\n",
      "Iteration 234, loss = 0.09932626\n",
      "Iteration 235, loss = 0.09914748\n",
      "Iteration 236, loss = 0.09898489\n",
      "Iteration 237, loss = 0.09886476\n",
      "Iteration 238, loss = 0.09863336\n",
      "Iteration 239, loss = 0.09845255\n",
      "Iteration 240, loss = 0.09827960\n",
      "Iteration 241, loss = 0.09811807\n",
      "Iteration 242, loss = 0.09795394\n",
      "Iteration 243, loss = 0.09779135\n",
      "Iteration 244, loss = 0.09761152\n",
      "Iteration 245, loss = 0.09747949\n",
      "Iteration 246, loss = 0.09728775\n",
      "Iteration 247, loss = 0.09713743\n",
      "Iteration 248, loss = 0.09699127\n",
      "Iteration 249, loss = 0.09680045\n",
      "Iteration 250, loss = 0.09666639\n",
      "Iteration 251, loss = 0.09651647\n",
      "Iteration 252, loss = 0.09635172\n",
      "Iteration 253, loss = 0.09623352\n",
      "Iteration 254, loss = 0.09602030\n",
      "Iteration 255, loss = 0.09586804\n",
      "Iteration 256, loss = 0.09572903\n",
      "Iteration 257, loss = 0.09556588\n",
      "Iteration 258, loss = 0.09545455\n",
      "Iteration 259, loss = 0.09527203\n",
      "Iteration 260, loss = 0.09515902\n",
      "Iteration 261, loss = 0.09497753\n",
      "Iteration 262, loss = 0.09484557\n",
      "Iteration 263, loss = 0.09470623\n",
      "Iteration 264, loss = 0.09455753\n",
      "Iteration 265, loss = 0.09445224\n",
      "Iteration 266, loss = 0.09426976\n",
      "Iteration 267, loss = 0.09414204\n",
      "Iteration 268, loss = 0.09400293\n",
      "Iteration 269, loss = 0.09387534\n",
      "Iteration 270, loss = 0.09373266\n",
      "Iteration 271, loss = 0.09360984\n",
      "Iteration 272, loss = 0.09345597\n",
      "Iteration 273, loss = 0.09335812\n",
      "Iteration 274, loss = 0.09317324\n",
      "Iteration 275, loss = 0.09307397\n",
      "Iteration 276, loss = 0.09291046\n",
      "Iteration 277, loss = 0.09277663\n",
      "Iteration 278, loss = 0.09264458\n",
      "Iteration 279, loss = 0.09253198\n",
      "Iteration 280, loss = 0.09239315\n",
      "Iteration 281, loss = 0.09224690\n",
      "Iteration 282, loss = 0.09212725\n",
      "Iteration 283, loss = 0.09200560\n",
      "Iteration 284, loss = 0.09187385\n",
      "Iteration 285, loss = 0.09175697\n",
      "Iteration 286, loss = 0.09160818\n",
      "Iteration 287, loss = 0.09149992\n",
      "Iteration 288, loss = 0.09136608\n",
      "Iteration 289, loss = 0.09126165\n",
      "Iteration 290, loss = 0.09114570\n",
      "Iteration 291, loss = 0.09100670\n",
      "Iteration 292, loss = 0.09092237\n",
      "Iteration 293, loss = 0.09075986\n",
      "Iteration 294, loss = 0.09066237\n",
      "Iteration 295, loss = 0.09052754\n",
      "Iteration 296, loss = 0.09041148\n",
      "Iteration 297, loss = 0.09029968\n",
      "Iteration 298, loss = 0.09015741\n",
      "Iteration 299, loss = 0.09004654\n",
      "Iteration 300, loss = 0.08992981\n",
      "Iteration 301, loss = 0.08985316\n",
      "Iteration 302, loss = 0.08969034\n",
      "Iteration 303, loss = 0.08957492\n",
      "Iteration 304, loss = 0.08951533\n",
      "Iteration 305, loss = 0.08937226\n",
      "Iteration 306, loss = 0.08927451\n",
      "Iteration 307, loss = 0.08912843\n",
      "Iteration 308, loss = 0.08901555\n",
      "Iteration 309, loss = 0.08892817\n",
      "Iteration 310, loss = 0.08878040\n",
      "Iteration 311, loss = 0.08868796\n",
      "Iteration 312, loss = 0.08859316\n",
      "Iteration 313, loss = 0.08846588\n",
      "Iteration 314, loss = 0.08831847\n",
      "Iteration 315, loss = 0.08821688\n",
      "Iteration 316, loss = 0.08810465\n",
      "Iteration 317, loss = 0.08798715\n",
      "Iteration 318, loss = 0.08787893\n",
      "Iteration 319, loss = 0.08776067\n",
      "Iteration 320, loss = 0.08768452\n",
      "Iteration 321, loss = 0.08756008\n",
      "Iteration 322, loss = 0.08751013\n",
      "Iteration 323, loss = 0.08734743\n",
      "Iteration 324, loss = 0.08723516\n",
      "Iteration 325, loss = 0.08714847\n",
      "Iteration 326, loss = 0.08703250\n",
      "Iteration 327, loss = 0.08692411\n",
      "Iteration 328, loss = 0.08679609\n",
      "Iteration 329, loss = 0.08669921\n",
      "Iteration 330, loss = 0.08660097\n",
      "Iteration 331, loss = 0.08650946\n",
      "Iteration 332, loss = 0.08640318\n",
      "Iteration 333, loss = 0.08629501\n",
      "Iteration 334, loss = 0.08618592\n",
      "Iteration 335, loss = 0.08609453\n",
      "Iteration 336, loss = 0.08601917\n",
      "Iteration 337, loss = 0.08589398\n",
      "Iteration 338, loss = 0.08576951\n",
      "Iteration 339, loss = 0.08567601\n",
      "Iteration 340, loss = 0.08557817\n",
      "Iteration 341, loss = 0.08547319\n",
      "Iteration 342, loss = 0.08541345\n",
      "Iteration 343, loss = 0.08528746\n",
      "Iteration 344, loss = 0.08518907\n",
      "Iteration 345, loss = 0.08509185\n",
      "Iteration 346, loss = 0.08498851\n",
      "Iteration 347, loss = 0.08488867\n",
      "Iteration 348, loss = 0.08478947\n",
      "Iteration 349, loss = 0.08469423\n",
      "Iteration 350, loss = 0.08459222\n",
      "Iteration 351, loss = 0.08450073\n",
      "Iteration 352, loss = 0.08443711\n",
      "Iteration 353, loss = 0.08431696\n",
      "Iteration 354, loss = 0.08423542\n",
      "Iteration 355, loss = 0.08411608\n",
      "Iteration 356, loss = 0.08402528\n",
      "Iteration 357, loss = 0.08394886\n",
      "Iteration 358, loss = 0.08384873\n",
      "Iteration 359, loss = 0.08375888\n",
      "Iteration 360, loss = 0.08369821\n",
      "Iteration 361, loss = 0.08356572\n",
      "Iteration 362, loss = 0.08349075\n",
      "Iteration 363, loss = 0.08341155\n",
      "Iteration 364, loss = 0.08329519\n",
      "Iteration 365, loss = 0.08321005\n",
      "Iteration 366, loss = 0.08311377\n",
      "Iteration 367, loss = 0.08302566\n",
      "Iteration 368, loss = 0.08295489\n",
      "Iteration 369, loss = 0.08284505\n",
      "Iteration 370, loss = 0.08276426\n",
      "Iteration 371, loss = 0.08271662\n",
      "Iteration 372, loss = 0.08259903\n",
      "Iteration 373, loss = 0.08250174\n",
      "Iteration 374, loss = 0.08243180\n",
      "Iteration 375, loss = 0.08236192\n",
      "Iteration 376, loss = 0.08225610\n",
      "Iteration 377, loss = 0.08216321\n",
      "Iteration 378, loss = 0.08207023\n",
      "Iteration 379, loss = 0.08201645\n",
      "Iteration 380, loss = 0.08193262\n",
      "Iteration 381, loss = 0.08184249\n",
      "Iteration 382, loss = 0.08177215\n",
      "Iteration 383, loss = 0.08174910\n",
      "Iteration 384, loss = 0.08159314\n",
      "Iteration 385, loss = 0.08150872\n",
      "Iteration 386, loss = 0.08142540\n",
      "Iteration 387, loss = 0.08134721\n",
      "Iteration 388, loss = 0.08127891\n",
      "Iteration 389, loss = 0.08118546\n",
      "Iteration 390, loss = 0.08111008\n",
      "Iteration 391, loss = 0.08102528\n",
      "Iteration 392, loss = 0.08094834\n",
      "Iteration 393, loss = 0.08087909\n",
      "Iteration 394, loss = 0.08079140\n",
      "Iteration 395, loss = 0.08074289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72227563\n",
      "Iteration 2, loss = 0.67260726\n",
      "Iteration 3, loss = 0.63172304\n",
      "Iteration 4, loss = 0.59777882\n",
      "Iteration 5, loss = 0.56894890\n",
      "Iteration 6, loss = 0.54366440\n",
      "Iteration 7, loss = 0.52127088\n",
      "Iteration 8, loss = 0.50140420\n",
      "Iteration 9, loss = 0.48342332\n",
      "Iteration 10, loss = 0.46699187\n",
      "Iteration 11, loss = 0.45215438\n",
      "Iteration 12, loss = 0.43842063\n",
      "Iteration 13, loss = 0.42578518\n",
      "Iteration 14, loss = 0.41388930\n",
      "Iteration 15, loss = 0.40294898\n",
      "Iteration 16, loss = 0.39238387\n",
      "Iteration 17, loss = 0.38250220\n",
      "Iteration 18, loss = 0.37321850\n",
      "Iteration 19, loss = 0.36442326\n",
      "Iteration 20, loss = 0.35602495\n",
      "Iteration 21, loss = 0.34816789\n",
      "Iteration 22, loss = 0.34064263\n",
      "Iteration 23, loss = 0.33373627\n",
      "Iteration 24, loss = 0.32695608\n",
      "Iteration 25, loss = 0.32050542\n",
      "Iteration 26, loss = 0.31444446\n",
      "Iteration 27, loss = 0.30867606\n",
      "Iteration 28, loss = 0.30311923\n",
      "Iteration 29, loss = 0.29776734\n",
      "Iteration 30, loss = 0.29269582\n",
      "Iteration 31, loss = 0.28783359\n",
      "Iteration 32, loss = 0.28317717\n",
      "Iteration 33, loss = 0.27877702\n",
      "Iteration 34, loss = 0.27448431\n",
      "Iteration 35, loss = 0.27035019\n",
      "Iteration 36, loss = 0.26643881\n",
      "Iteration 37, loss = 0.26256471\n",
      "Iteration 38, loss = 0.25887015\n",
      "Iteration 39, loss = 0.25533720\n",
      "Iteration 40, loss = 0.25203518\n",
      "Iteration 41, loss = 0.24878434\n",
      "Iteration 42, loss = 0.24554483\n",
      "Iteration 43, loss = 0.24256970\n",
      "Iteration 44, loss = 0.23963831\n",
      "Iteration 45, loss = 0.23669174\n",
      "Iteration 46, loss = 0.23394895\n",
      "Iteration 47, loss = 0.23129595\n",
      "Iteration 48, loss = 0.22874384\n",
      "Iteration 49, loss = 0.22629668\n",
      "Iteration 50, loss = 0.22377054\n",
      "Iteration 51, loss = 0.22152785\n",
      "Iteration 52, loss = 0.21917240\n",
      "Iteration 53, loss = 0.21693145\n",
      "Iteration 54, loss = 0.21480351\n",
      "Iteration 55, loss = 0.21272738\n",
      "Iteration 56, loss = 0.21073478\n",
      "Iteration 57, loss = 0.20873245\n",
      "Iteration 58, loss = 0.20681985\n",
      "Iteration 59, loss = 0.20496481\n",
      "Iteration 60, loss = 0.20317312\n",
      "Iteration 61, loss = 0.20142320\n",
      "Iteration 62, loss = 0.19964446\n",
      "Iteration 63, loss = 0.19798909\n",
      "Iteration 64, loss = 0.19637246\n",
      "Iteration 65, loss = 0.19471679\n",
      "Iteration 66, loss = 0.19315572\n",
      "Iteration 67, loss = 0.19161030\n",
      "Iteration 68, loss = 0.19011371\n",
      "Iteration 69, loss = 0.18859564\n",
      "Iteration 70, loss = 0.18711134\n",
      "Iteration 71, loss = 0.18572537\n",
      "Iteration 72, loss = 0.18436492\n",
      "Iteration 73, loss = 0.18291920\n",
      "Iteration 74, loss = 0.18160566\n",
      "Iteration 75, loss = 0.18025463\n",
      "Iteration 76, loss = 0.17894372\n",
      "Iteration 77, loss = 0.17770513\n",
      "Iteration 78, loss = 0.17653134\n",
      "Iteration 79, loss = 0.17529371\n",
      "Iteration 80, loss = 0.17409141\n",
      "Iteration 81, loss = 0.17292418\n",
      "Iteration 82, loss = 0.17186318\n",
      "Iteration 83, loss = 0.17071378\n",
      "Iteration 84, loss = 0.16960912\n",
      "Iteration 85, loss = 0.16849522\n",
      "Iteration 86, loss = 0.16743582\n",
      "Iteration 87, loss = 0.16646423\n",
      "Iteration 88, loss = 0.16538622\n",
      "Iteration 89, loss = 0.16440529\n",
      "Iteration 90, loss = 0.16337873\n",
      "Iteration 91, loss = 0.16240058\n",
      "Iteration 92, loss = 0.16148876\n",
      "Iteration 93, loss = 0.16054441\n",
      "Iteration 94, loss = 0.15965488\n",
      "Iteration 95, loss = 0.15880175\n",
      "Iteration 96, loss = 0.15793012\n",
      "Iteration 97, loss = 0.15708259\n",
      "Iteration 98, loss = 0.15626642\n",
      "Iteration 99, loss = 0.15547248\n",
      "Iteration 100, loss = 0.15471325\n",
      "Iteration 101, loss = 0.15398912\n",
      "Iteration 102, loss = 0.15316449\n",
      "Iteration 103, loss = 0.15242368\n",
      "Iteration 104, loss = 0.15167890\n",
      "Iteration 105, loss = 0.15097168\n",
      "Iteration 106, loss = 0.15027789\n",
      "Iteration 107, loss = 0.14955318\n",
      "Iteration 108, loss = 0.14885831\n",
      "Iteration 109, loss = 0.14819946\n",
      "Iteration 110, loss = 0.14754569\n",
      "Iteration 111, loss = 0.14692973\n",
      "Iteration 112, loss = 0.14625401\n",
      "Iteration 113, loss = 0.14565913\n",
      "Iteration 114, loss = 0.14502950\n",
      "Iteration 115, loss = 0.14440064\n",
      "Iteration 116, loss = 0.14383566\n",
      "Iteration 117, loss = 0.14322179\n",
      "Iteration 118, loss = 0.14263455\n",
      "Iteration 119, loss = 0.14223347\n",
      "Iteration 120, loss = 0.14151518\n",
      "Iteration 121, loss = 0.14095880\n",
      "Iteration 122, loss = 0.14042606\n",
      "Iteration 123, loss = 0.13987501\n",
      "Iteration 124, loss = 0.13935746\n",
      "Iteration 125, loss = 0.13886018\n",
      "Iteration 126, loss = 0.13833727\n",
      "Iteration 127, loss = 0.13783110\n",
      "Iteration 128, loss = 0.13736183\n",
      "Iteration 129, loss = 0.13683497\n",
      "Iteration 130, loss = 0.13635325\n",
      "Iteration 131, loss = 0.13593777\n",
      "Iteration 132, loss = 0.13544061\n",
      "Iteration 133, loss = 0.13496476\n",
      "Iteration 134, loss = 0.13448454\n",
      "Iteration 135, loss = 0.13405272\n",
      "Iteration 136, loss = 0.13359541\n",
      "Iteration 137, loss = 0.13317773\n",
      "Iteration 138, loss = 0.13274524\n",
      "Iteration 139, loss = 0.13231441\n",
      "Iteration 140, loss = 0.13189435\n",
      "Iteration 141, loss = 0.13146968\n",
      "Iteration 142, loss = 0.13115703\n",
      "Iteration 143, loss = 0.13067993\n",
      "Iteration 144, loss = 0.13031290\n",
      "Iteration 145, loss = 0.12993269\n",
      "Iteration 146, loss = 0.12952795\n",
      "Iteration 147, loss = 0.12911357\n",
      "Iteration 148, loss = 0.12874561\n",
      "Iteration 149, loss = 0.12845955\n",
      "Iteration 150, loss = 0.12806196\n",
      "Iteration 151, loss = 0.12761422\n",
      "Iteration 152, loss = 0.12733663\n",
      "Iteration 153, loss = 0.12690542\n",
      "Iteration 154, loss = 0.12656989\n",
      "Iteration 155, loss = 0.12616656\n",
      "Iteration 156, loss = 0.12585630\n",
      "Iteration 157, loss = 0.12549141\n",
      "Iteration 158, loss = 0.12516524\n",
      "Iteration 159, loss = 0.12482200\n",
      "Iteration 160, loss = 0.12450238\n",
      "Iteration 161, loss = 0.12417344\n",
      "Iteration 162, loss = 0.12380907\n",
      "Iteration 163, loss = 0.12350633\n",
      "Iteration 164, loss = 0.12313732\n",
      "Iteration 165, loss = 0.12283945\n",
      "Iteration 166, loss = 0.12250114\n",
      "Iteration 167, loss = 0.12218701\n",
      "Iteration 168, loss = 0.12193699\n",
      "Iteration 169, loss = 0.12157829\n",
      "Iteration 170, loss = 0.12122810\n",
      "Iteration 171, loss = 0.12091713\n",
      "Iteration 172, loss = 0.12063842\n",
      "Iteration 173, loss = 0.12033002\n",
      "Iteration 174, loss = 0.12001416\n",
      "Iteration 175, loss = 0.11971743\n",
      "Iteration 176, loss = 0.11945288\n",
      "Iteration 177, loss = 0.11912046\n",
      "Iteration 178, loss = 0.11889692\n",
      "Iteration 179, loss = 0.11857811\n",
      "Iteration 180, loss = 0.11827986\n",
      "Iteration 181, loss = 0.11803794\n",
      "Iteration 182, loss = 0.11771404\n",
      "Iteration 183, loss = 0.11745974\n",
      "Iteration 184, loss = 0.11717886\n",
      "Iteration 185, loss = 0.11691397\n",
      "Iteration 186, loss = 0.11663533\n",
      "Iteration 187, loss = 0.11639358\n",
      "Iteration 188, loss = 0.11611967\n",
      "Iteration 189, loss = 0.11590934\n",
      "Iteration 190, loss = 0.11562666\n",
      "Iteration 191, loss = 0.11538356\n",
      "Iteration 192, loss = 0.11512421\n",
      "Iteration 193, loss = 0.11491402\n",
      "Iteration 194, loss = 0.11471193\n",
      "Iteration 195, loss = 0.11440512\n",
      "Iteration 196, loss = 0.11420120\n",
      "Iteration 197, loss = 0.11396552\n",
      "Iteration 198, loss = 0.11373080\n",
      "Iteration 199, loss = 0.11351152\n",
      "Iteration 200, loss = 0.11326515\n",
      "Iteration 201, loss = 0.11305674\n",
      "Iteration 202, loss = 0.11282419\n",
      "Iteration 203, loss = 0.11263733\n",
      "Iteration 204, loss = 0.11238477\n",
      "Iteration 205, loss = 0.11217204\n",
      "Iteration 206, loss = 0.11198538\n",
      "Iteration 207, loss = 0.11176822\n",
      "Iteration 208, loss = 0.11154653\n",
      "Iteration 209, loss = 0.11135511\n",
      "Iteration 210, loss = 0.11113615\n",
      "Iteration 211, loss = 0.11097671\n",
      "Iteration 212, loss = 0.11077061\n",
      "Iteration 213, loss = 0.11052878\n",
      "Iteration 214, loss = 0.11039861\n",
      "Iteration 215, loss = 0.11022127\n",
      "Iteration 216, loss = 0.10994824\n",
      "Iteration 217, loss = 0.10976785\n",
      "Iteration 218, loss = 0.10955764\n",
      "Iteration 219, loss = 0.10938967\n",
      "Iteration 220, loss = 0.10917235\n",
      "Iteration 221, loss = 0.10903547\n",
      "Iteration 222, loss = 0.10883857\n",
      "Iteration 223, loss = 0.10864125\n",
      "Iteration 224, loss = 0.10848183\n",
      "Iteration 225, loss = 0.10827689\n",
      "Iteration 226, loss = 0.10813942\n",
      "Iteration 227, loss = 0.10792398\n",
      "Iteration 228, loss = 0.10773964\n",
      "Iteration 229, loss = 0.10759411\n",
      "Iteration 230, loss = 0.10743951\n",
      "Iteration 231, loss = 0.10723348\n",
      "Iteration 232, loss = 0.10707642\n",
      "Iteration 233, loss = 0.10689919\n",
      "Iteration 234, loss = 0.10672827\n",
      "Iteration 235, loss = 0.10658882\n",
      "Iteration 236, loss = 0.10642143\n",
      "Iteration 237, loss = 0.10625480\n",
      "Iteration 238, loss = 0.10607622\n",
      "Iteration 239, loss = 0.10594289\n",
      "Iteration 240, loss = 0.10576072\n",
      "Iteration 241, loss = 0.10559144\n",
      "Iteration 242, loss = 0.10542929\n",
      "Iteration 243, loss = 0.10531407\n",
      "Iteration 244, loss = 0.10512215\n",
      "Iteration 245, loss = 0.10502149\n",
      "Iteration 246, loss = 0.10482949\n",
      "Iteration 247, loss = 0.10470716\n",
      "Iteration 248, loss = 0.10456605\n",
      "Iteration 249, loss = 0.10438283\n",
      "Iteration 250, loss = 0.10425729\n",
      "Iteration 251, loss = 0.10409700\n",
      "Iteration 252, loss = 0.10393919\n",
      "Iteration 253, loss = 0.10379101\n",
      "Iteration 254, loss = 0.10362890\n",
      "Iteration 255, loss = 0.10349990\n",
      "Iteration 256, loss = 0.10337522\n",
      "Iteration 257, loss = 0.10320415\n",
      "Iteration 258, loss = 0.10308103\n",
      "Iteration 259, loss = 0.10294523\n",
      "Iteration 260, loss = 0.10282629\n",
      "Iteration 261, loss = 0.10264159\n",
      "Iteration 262, loss = 0.10250507\n",
      "Iteration 263, loss = 0.10237999\n",
      "Iteration 264, loss = 0.10224140\n",
      "Iteration 265, loss = 0.10213147\n",
      "Iteration 266, loss = 0.10196566\n",
      "Iteration 267, loss = 0.10183682\n",
      "Iteration 268, loss = 0.10172065\n",
      "Iteration 269, loss = 0.10157850\n",
      "Iteration 270, loss = 0.10147655\n",
      "Iteration 271, loss = 0.10134594\n",
      "Iteration 272, loss = 0.10117860\n",
      "Iteration 273, loss = 0.10107422\n",
      "Iteration 274, loss = 0.10093146\n",
      "Iteration 275, loss = 0.10082526\n",
      "Iteration 276, loss = 0.10067738\n",
      "Iteration 277, loss = 0.10057698\n",
      "Iteration 278, loss = 0.10042961\n",
      "Iteration 279, loss = 0.10030771\n",
      "Iteration 280, loss = 0.10016493\n",
      "Iteration 281, loss = 0.10006171\n",
      "Iteration 282, loss = 0.09990561\n",
      "Iteration 283, loss = 0.09980458\n",
      "Iteration 284, loss = 0.09968743\n",
      "Iteration 285, loss = 0.09955080\n",
      "Iteration 286, loss = 0.09947149\n",
      "Iteration 287, loss = 0.09930844\n",
      "Iteration 288, loss = 0.09919233\n",
      "Iteration 289, loss = 0.09906791\n",
      "Iteration 290, loss = 0.09898082\n",
      "Iteration 291, loss = 0.09883016\n",
      "Iteration 292, loss = 0.09873957\n",
      "Iteration 293, loss = 0.09861485\n",
      "Iteration 294, loss = 0.09852409\n",
      "Iteration 295, loss = 0.09838524\n",
      "Iteration 296, loss = 0.09825660\n",
      "Iteration 297, loss = 0.09813577\n",
      "Iteration 298, loss = 0.09802205\n",
      "Iteration 299, loss = 0.09789401\n",
      "Iteration 300, loss = 0.09778473\n",
      "Iteration 301, loss = 0.09775216\n",
      "Iteration 302, loss = 0.09756792\n",
      "Iteration 303, loss = 0.09745165\n",
      "Iteration 304, loss = 0.09738424\n",
      "Iteration 305, loss = 0.09728050\n",
      "Iteration 306, loss = 0.09715667\n",
      "Iteration 307, loss = 0.09703856\n",
      "Iteration 308, loss = 0.09692010\n",
      "Iteration 309, loss = 0.09681576\n",
      "Iteration 310, loss = 0.09669468\n",
      "Iteration 311, loss = 0.09659275\n",
      "Iteration 312, loss = 0.09652152\n",
      "Iteration 313, loss = 0.09643919\n",
      "Iteration 314, loss = 0.09625123\n",
      "Iteration 315, loss = 0.09615522\n",
      "Iteration 316, loss = 0.09605411\n",
      "Iteration 317, loss = 0.09594069\n",
      "Iteration 318, loss = 0.09582123\n",
      "Iteration 319, loss = 0.09575438\n",
      "Iteration 320, loss = 0.09566340\n",
      "Iteration 321, loss = 0.09552512\n",
      "Iteration 322, loss = 0.09546797\n",
      "Iteration 323, loss = 0.09529940\n",
      "Iteration 324, loss = 0.09519928\n",
      "Iteration 325, loss = 0.09511966\n",
      "Iteration 326, loss = 0.09502255\n",
      "Iteration 327, loss = 0.09492219\n",
      "Iteration 328, loss = 0.09479536\n",
      "Iteration 329, loss = 0.09474716\n",
      "Iteration 330, loss = 0.09460324\n",
      "Iteration 331, loss = 0.09449122\n",
      "Iteration 332, loss = 0.09441229\n",
      "Iteration 333, loss = 0.09430220\n",
      "Iteration 334, loss = 0.09421607\n",
      "Iteration 335, loss = 0.09411393\n",
      "Iteration 336, loss = 0.09404841\n",
      "Iteration 337, loss = 0.09391815\n",
      "Iteration 338, loss = 0.09382229\n",
      "Iteration 339, loss = 0.09371707\n",
      "Iteration 340, loss = 0.09363330\n",
      "Iteration 341, loss = 0.09354531\n",
      "Iteration 342, loss = 0.09351547\n",
      "Iteration 343, loss = 0.09335590\n",
      "Iteration 344, loss = 0.09326004\n",
      "Iteration 345, loss = 0.09319312\n",
      "Iteration 346, loss = 0.09304580\n",
      "Iteration 347, loss = 0.09300904\n",
      "Iteration 348, loss = 0.09287026\n",
      "Iteration 349, loss = 0.09279879\n",
      "Iteration 350, loss = 0.09269713\n",
      "Iteration 351, loss = 0.09259663\n",
      "Iteration 352, loss = 0.09253041\n",
      "Iteration 353, loss = 0.09246062\n",
      "Iteration 354, loss = 0.09237655\n",
      "Iteration 355, loss = 0.09225167\n",
      "Iteration 356, loss = 0.09215667\n",
      "Iteration 357, loss = 0.09214673\n",
      "Iteration 358, loss = 0.09199245\n",
      "Iteration 359, loss = 0.09192454\n",
      "Iteration 360, loss = 0.09185413\n",
      "Iteration 361, loss = 0.09172568\n",
      "Iteration 362, loss = 0.09164810\n",
      "Iteration 363, loss = 0.09159896\n",
      "Iteration 364, loss = 0.09145702\n",
      "Iteration 365, loss = 0.09139681\n",
      "Iteration 366, loss = 0.09131043\n",
      "Iteration 367, loss = 0.09122473\n",
      "Iteration 368, loss = 0.09115927\n",
      "Iteration 369, loss = 0.09104832\n",
      "Iteration 370, loss = 0.09096735\n",
      "Iteration 371, loss = 0.09095459\n",
      "Iteration 372, loss = 0.09080634\n",
      "Iteration 373, loss = 0.09073216\n",
      "Iteration 374, loss = 0.09064545\n",
      "Iteration 375, loss = 0.09056550\n",
      "Iteration 376, loss = 0.09047315\n",
      "Iteration 377, loss = 0.09040433\n",
      "Iteration 378, loss = 0.09032553\n",
      "Iteration 379, loss = 0.09027284\n",
      "Iteration 380, loss = 0.09018686\n",
      "Iteration 381, loss = 0.09009741\n",
      "Iteration 382, loss = 0.09004325\n",
      "Iteration 383, loss = 0.08998153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72955320\n",
      "Iteration 2, loss = 0.67886651\n",
      "Iteration 3, loss = 0.63779674\n",
      "Iteration 4, loss = 0.60377010\n",
      "Iteration 5, loss = 0.57490111\n",
      "Iteration 6, loss = 0.54979323\n",
      "Iteration 7, loss = 0.52749304\n",
      "Iteration 8, loss = 0.50763776\n",
      "Iteration 9, loss = 0.48974157\n",
      "Iteration 10, loss = 0.47350359\n",
      "Iteration 11, loss = 0.45888338\n",
      "Iteration 12, loss = 0.44543399\n",
      "Iteration 13, loss = 0.43305296\n",
      "Iteration 14, loss = 0.42135891\n",
      "Iteration 15, loss = 0.41051796\n",
      "Iteration 16, loss = 0.40017152\n",
      "Iteration 17, loss = 0.39049162\n",
      "Iteration 18, loss = 0.38130684\n",
      "Iteration 19, loss = 0.37274733\n",
      "Iteration 20, loss = 0.36452581\n",
      "Iteration 21, loss = 0.35679747\n",
      "Iteration 22, loss = 0.34944755\n",
      "Iteration 23, loss = 0.34267161\n",
      "Iteration 24, loss = 0.33610714\n",
      "Iteration 25, loss = 0.32982264\n",
      "Iteration 26, loss = 0.32392348\n",
      "Iteration 27, loss = 0.31823429\n",
      "Iteration 28, loss = 0.31281576\n",
      "Iteration 29, loss = 0.30757848\n",
      "Iteration 30, loss = 0.30257053\n",
      "Iteration 31, loss = 0.29783079\n",
      "Iteration 32, loss = 0.29330105\n",
      "Iteration 33, loss = 0.28898395\n",
      "Iteration 34, loss = 0.28481696\n",
      "Iteration 35, loss = 0.28079966\n",
      "Iteration 36, loss = 0.27696930\n",
      "Iteration 37, loss = 0.27326592\n",
      "Iteration 38, loss = 0.26972116\n",
      "Iteration 39, loss = 0.26624598\n",
      "Iteration 40, loss = 0.26298648\n",
      "Iteration 41, loss = 0.25977513\n",
      "Iteration 42, loss = 0.25673851\n",
      "Iteration 43, loss = 0.25379426\n",
      "Iteration 44, loss = 0.25091490\n",
      "Iteration 45, loss = 0.24810625\n",
      "Iteration 46, loss = 0.24538764\n",
      "Iteration 47, loss = 0.24280147\n",
      "Iteration 48, loss = 0.24023260\n",
      "Iteration 49, loss = 0.23779706\n",
      "Iteration 50, loss = 0.23538559\n",
      "Iteration 51, loss = 0.23308598\n",
      "Iteration 52, loss = 0.23074758\n",
      "Iteration 53, loss = 0.22857174\n",
      "Iteration 54, loss = 0.22641892\n",
      "Iteration 55, loss = 0.22433918\n",
      "Iteration 56, loss = 0.22233646\n",
      "Iteration 57, loss = 0.22036787\n",
      "Iteration 58, loss = 0.21844110\n",
      "Iteration 59, loss = 0.21659545\n",
      "Iteration 60, loss = 0.21480977\n",
      "Iteration 61, loss = 0.21309885\n",
      "Iteration 62, loss = 0.21129438\n",
      "Iteration 63, loss = 0.20965664\n",
      "Iteration 64, loss = 0.20801134\n",
      "Iteration 65, loss = 0.20641455\n",
      "Iteration 66, loss = 0.20482956\n",
      "Iteration 67, loss = 0.20328000\n",
      "Iteration 68, loss = 0.20179426\n",
      "Iteration 69, loss = 0.20029914\n",
      "Iteration 70, loss = 0.19886746\n",
      "Iteration 71, loss = 0.19751957\n",
      "Iteration 72, loss = 0.19612106\n",
      "Iteration 73, loss = 0.19472613\n",
      "Iteration 74, loss = 0.19342105\n",
      "Iteration 75, loss = 0.19206414\n",
      "Iteration 76, loss = 0.19076343\n",
      "Iteration 77, loss = 0.18952553\n",
      "Iteration 78, loss = 0.18830111\n",
      "Iteration 79, loss = 0.18710221\n",
      "Iteration 80, loss = 0.18589941\n",
      "Iteration 81, loss = 0.18472566\n",
      "Iteration 82, loss = 0.18362365\n",
      "Iteration 83, loss = 0.18253488\n",
      "Iteration 84, loss = 0.18141839\n",
      "Iteration 85, loss = 0.18023907\n",
      "Iteration 86, loss = 0.17922485\n",
      "Iteration 87, loss = 0.17814887\n",
      "Iteration 88, loss = 0.17713474\n",
      "Iteration 89, loss = 0.17608627\n",
      "Iteration 90, loss = 0.17507579\n",
      "Iteration 91, loss = 0.17407464\n",
      "Iteration 92, loss = 0.17311401\n",
      "Iteration 93, loss = 0.17217331\n",
      "Iteration 94, loss = 0.17125582\n",
      "Iteration 95, loss = 0.17032587\n",
      "Iteration 96, loss = 0.16942844\n",
      "Iteration 97, loss = 0.16853433\n",
      "Iteration 98, loss = 0.16769763\n",
      "Iteration 99, loss = 0.16682014\n",
      "Iteration 100, loss = 0.16602881\n",
      "Iteration 101, loss = 0.16526181\n",
      "Iteration 102, loss = 0.16436548\n",
      "Iteration 103, loss = 0.16361641\n",
      "Iteration 104, loss = 0.16277520\n",
      "Iteration 105, loss = 0.16204433\n",
      "Iteration 106, loss = 0.16128615\n",
      "Iteration 107, loss = 0.16050597\n",
      "Iteration 108, loss = 0.15978606\n",
      "Iteration 109, loss = 0.15905827\n",
      "Iteration 110, loss = 0.15836599\n",
      "Iteration 111, loss = 0.15767579\n",
      "Iteration 112, loss = 0.15697364\n",
      "Iteration 113, loss = 0.15632681\n",
      "Iteration 114, loss = 0.15567858\n",
      "Iteration 115, loss = 0.15497813\n",
      "Iteration 116, loss = 0.15439616\n",
      "Iteration 117, loss = 0.15372180\n",
      "Iteration 118, loss = 0.15307510\n",
      "Iteration 119, loss = 0.15259473\n",
      "Iteration 120, loss = 0.15187021\n",
      "Iteration 121, loss = 0.15128018\n",
      "Iteration 122, loss = 0.15067169\n",
      "Iteration 123, loss = 0.15008516\n",
      "Iteration 124, loss = 0.14955360\n",
      "Iteration 125, loss = 0.14900196\n",
      "Iteration 126, loss = 0.14844003\n",
      "Iteration 127, loss = 0.14788084\n",
      "Iteration 128, loss = 0.14740715\n",
      "Iteration 129, loss = 0.14680480\n",
      "Iteration 130, loss = 0.14628420\n",
      "Iteration 131, loss = 0.14586796\n",
      "Iteration 132, loss = 0.14530085\n",
      "Iteration 133, loss = 0.14478104\n",
      "Iteration 134, loss = 0.14428909\n",
      "Iteration 135, loss = 0.14381094\n",
      "Iteration 136, loss = 0.14330473\n",
      "Iteration 137, loss = 0.14284629\n",
      "Iteration 138, loss = 0.14239080\n",
      "Iteration 139, loss = 0.14190940\n",
      "Iteration 140, loss = 0.14144431\n",
      "Iteration 141, loss = 0.14098835\n",
      "Iteration 142, loss = 0.14064398\n",
      "Iteration 143, loss = 0.14011599\n",
      "Iteration 144, loss = 0.13970591\n",
      "Iteration 145, loss = 0.13929251\n",
      "Iteration 146, loss = 0.13885046\n",
      "Iteration 147, loss = 0.13840527\n",
      "Iteration 148, loss = 0.13800096\n",
      "Iteration 149, loss = 0.13766219\n",
      "Iteration 150, loss = 0.13723040\n",
      "Iteration 151, loss = 0.13674968\n",
      "Iteration 152, loss = 0.13644062\n",
      "Iteration 153, loss = 0.13595805\n",
      "Iteration 154, loss = 0.13559158\n",
      "Iteration 155, loss = 0.13515307\n",
      "Iteration 156, loss = 0.13478303\n",
      "Iteration 157, loss = 0.13438544\n",
      "Iteration 158, loss = 0.13402998\n",
      "Iteration 159, loss = 0.13366106\n",
      "Iteration 160, loss = 0.13330518\n",
      "Iteration 161, loss = 0.13292810\n",
      "Iteration 162, loss = 0.13251812\n",
      "Iteration 163, loss = 0.13218955\n",
      "Iteration 164, loss = 0.13180992\n",
      "Iteration 165, loss = 0.13146700\n",
      "Iteration 166, loss = 0.13111009\n",
      "Iteration 167, loss = 0.13077789\n",
      "Iteration 168, loss = 0.13047008\n",
      "Iteration 169, loss = 0.13010462\n",
      "Iteration 170, loss = 0.12974004\n",
      "Iteration 171, loss = 0.12942973\n",
      "Iteration 172, loss = 0.12910410\n",
      "Iteration 173, loss = 0.12878437\n",
      "Iteration 174, loss = 0.12844410\n",
      "Iteration 175, loss = 0.12812140\n",
      "Iteration 176, loss = 0.12784054\n",
      "Iteration 177, loss = 0.12749969\n",
      "Iteration 178, loss = 0.12722417\n",
      "Iteration 179, loss = 0.12692328\n",
      "Iteration 180, loss = 0.12661139\n",
      "Iteration 181, loss = 0.12632695\n",
      "Iteration 182, loss = 0.12597612\n",
      "Iteration 183, loss = 0.12568786\n",
      "Iteration 184, loss = 0.12539228\n",
      "Iteration 185, loss = 0.12509173\n",
      "Iteration 186, loss = 0.12478180\n",
      "Iteration 187, loss = 0.12451097\n",
      "Iteration 188, loss = 0.12422132\n",
      "Iteration 189, loss = 0.12396390\n",
      "Iteration 190, loss = 0.12364526\n",
      "Iteration 191, loss = 0.12337763\n",
      "Iteration 192, loss = 0.12307429\n",
      "Iteration 193, loss = 0.12282679\n",
      "Iteration 194, loss = 0.12261030\n",
      "Iteration 195, loss = 0.12225932\n",
      "Iteration 196, loss = 0.12204153\n",
      "Iteration 197, loss = 0.12173288\n",
      "Iteration 198, loss = 0.12147514\n",
      "Iteration 199, loss = 0.12120886\n",
      "Iteration 200, loss = 0.12091893\n",
      "Iteration 201, loss = 0.12066164\n",
      "Iteration 202, loss = 0.12038866\n",
      "Iteration 203, loss = 0.12015157\n",
      "Iteration 204, loss = 0.11988201\n",
      "Iteration 205, loss = 0.11963547\n",
      "Iteration 206, loss = 0.11939068\n",
      "Iteration 207, loss = 0.11915931\n",
      "Iteration 208, loss = 0.11889957\n",
      "Iteration 209, loss = 0.11864657\n",
      "Iteration 210, loss = 0.11839149\n",
      "Iteration 211, loss = 0.11822326\n",
      "Iteration 212, loss = 0.11794810\n",
      "Iteration 213, loss = 0.11768932\n",
      "Iteration 214, loss = 0.11749466\n",
      "Iteration 215, loss = 0.11727990\n",
      "Iteration 216, loss = 0.11700134\n",
      "Iteration 217, loss = 0.11678949\n",
      "Iteration 218, loss = 0.11654102\n",
      "Iteration 219, loss = 0.11636098\n",
      "Iteration 220, loss = 0.11610852\n",
      "Iteration 221, loss = 0.11594625\n",
      "Iteration 222, loss = 0.11570164\n",
      "Iteration 223, loss = 0.11549454\n",
      "Iteration 224, loss = 0.11531318\n",
      "Iteration 225, loss = 0.11506726\n",
      "Iteration 226, loss = 0.11489383\n",
      "Iteration 227, loss = 0.11464769\n",
      "Iteration 228, loss = 0.11443247\n",
      "Iteration 229, loss = 0.11425310\n",
      "Iteration 230, loss = 0.11408589\n",
      "Iteration 231, loss = 0.11384638\n",
      "Iteration 232, loss = 0.11364838\n",
      "Iteration 233, loss = 0.11344003\n",
      "Iteration 234, loss = 0.11325227\n",
      "Iteration 235, loss = 0.11307822\n",
      "Iteration 236, loss = 0.11288108\n",
      "Iteration 237, loss = 0.11268994\n",
      "Iteration 238, loss = 0.11249677\n",
      "Iteration 239, loss = 0.11232156\n",
      "Iteration 240, loss = 0.11211177\n",
      "Iteration 241, loss = 0.11191118\n",
      "Iteration 242, loss = 0.11173700\n",
      "Iteration 243, loss = 0.11159659\n",
      "Iteration 244, loss = 0.11137515\n",
      "Iteration 245, loss = 0.11128441\n",
      "Iteration 246, loss = 0.11102614\n",
      "Iteration 247, loss = 0.11091803\n",
      "Iteration 248, loss = 0.11072238\n",
      "Iteration 249, loss = 0.11051686\n",
      "Iteration 250, loss = 0.11037993\n",
      "Iteration 251, loss = 0.11018388\n",
      "Iteration 252, loss = 0.11000442\n",
      "Iteration 253, loss = 0.10984719\n",
      "Iteration 254, loss = 0.10964222\n",
      "Iteration 255, loss = 0.10948025\n",
      "Iteration 256, loss = 0.10934680\n",
      "Iteration 257, loss = 0.10915036\n",
      "Iteration 258, loss = 0.10900351\n",
      "Iteration 259, loss = 0.10885073\n",
      "Iteration 260, loss = 0.10869877\n",
      "Iteration 261, loss = 0.10850604\n",
      "Iteration 262, loss = 0.10835109\n",
      "Iteration 263, loss = 0.10820890\n",
      "Iteration 264, loss = 0.10804653\n",
      "Iteration 265, loss = 0.10792761\n",
      "Iteration 266, loss = 0.10774053\n",
      "Iteration 267, loss = 0.10759085\n",
      "Iteration 268, loss = 0.10746680\n",
      "Iteration 269, loss = 0.10730987\n",
      "Iteration 270, loss = 0.10720010\n",
      "Iteration 271, loss = 0.10703701\n",
      "Iteration 272, loss = 0.10684694\n",
      "Iteration 273, loss = 0.10675559\n",
      "Iteration 274, loss = 0.10658198\n",
      "Iteration 275, loss = 0.10645781\n",
      "Iteration 276, loss = 0.10629573\n",
      "Iteration 277, loss = 0.10617007\n",
      "Iteration 278, loss = 0.10601537\n",
      "Iteration 279, loss = 0.10588046\n",
      "Iteration 280, loss = 0.10572115\n",
      "Iteration 281, loss = 0.10560676\n",
      "Iteration 282, loss = 0.10544205\n",
      "Iteration 283, loss = 0.10533157\n",
      "Iteration 284, loss = 0.10522665\n",
      "Iteration 285, loss = 0.10505705\n",
      "Iteration 286, loss = 0.10495503\n",
      "Iteration 287, loss = 0.10477652\n",
      "Iteration 288, loss = 0.10467866\n",
      "Iteration 289, loss = 0.10452488\n",
      "Iteration 290, loss = 0.10441957\n",
      "Iteration 291, loss = 0.10426467\n",
      "Iteration 292, loss = 0.10417141\n",
      "Iteration 293, loss = 0.10401226\n",
      "Iteration 294, loss = 0.10394400\n",
      "Iteration 295, loss = 0.10377396\n",
      "Iteration 296, loss = 0.10364991\n",
      "Iteration 297, loss = 0.10351618\n",
      "Iteration 298, loss = 0.10337556\n",
      "Iteration 299, loss = 0.10324501\n",
      "Iteration 300, loss = 0.10313102\n",
      "Iteration 301, loss = 0.10307025\n",
      "Iteration 302, loss = 0.10288031\n",
      "Iteration 303, loss = 0.10276247\n",
      "Iteration 304, loss = 0.10269554\n",
      "Iteration 305, loss = 0.10256829\n",
      "Iteration 306, loss = 0.10244048\n",
      "Iteration 307, loss = 0.10228885\n",
      "Iteration 308, loss = 0.10217841\n",
      "Iteration 309, loss = 0.10204489\n",
      "Iteration 310, loss = 0.10194448\n",
      "Iteration 311, loss = 0.10181727\n",
      "Iteration 312, loss = 0.10174945\n",
      "Iteration 313, loss = 0.10165430\n",
      "Iteration 314, loss = 0.10144758\n",
      "Iteration 315, loss = 0.10135230\n",
      "Iteration 316, loss = 0.10122317\n",
      "Iteration 317, loss = 0.10110838\n",
      "Iteration 318, loss = 0.10097980\n",
      "Iteration 319, loss = 0.10089452\n",
      "Iteration 320, loss = 0.10080723\n",
      "Iteration 321, loss = 0.10065876\n",
      "Iteration 322, loss = 0.10057012\n",
      "Iteration 323, loss = 0.10040857\n",
      "Iteration 324, loss = 0.10030924\n",
      "Iteration 325, loss = 0.10020215\n",
      "Iteration 326, loss = 0.10009686\n",
      "Iteration 327, loss = 0.09999008\n",
      "Iteration 328, loss = 0.09984926\n",
      "Iteration 329, loss = 0.09979924\n",
      "Iteration 330, loss = 0.09964022\n",
      "Iteration 331, loss = 0.09951482\n",
      "Iteration 332, loss = 0.09943295\n",
      "Iteration 333, loss = 0.09930995\n",
      "Iteration 334, loss = 0.09921298\n",
      "Iteration 335, loss = 0.09909405\n",
      "Iteration 336, loss = 0.09902919\n",
      "Iteration 337, loss = 0.09889641\n",
      "Iteration 338, loss = 0.09878671\n",
      "Iteration 339, loss = 0.09866697\n",
      "Iteration 340, loss = 0.09857819\n",
      "Iteration 341, loss = 0.09847115\n",
      "Iteration 342, loss = 0.09846339\n",
      "Iteration 343, loss = 0.09826348\n",
      "Iteration 344, loss = 0.09816015\n",
      "Iteration 345, loss = 0.09806829\n",
      "Iteration 346, loss = 0.09794985\n",
      "Iteration 347, loss = 0.09790176\n",
      "Iteration 348, loss = 0.09775415\n",
      "Iteration 349, loss = 0.09767516\n",
      "Iteration 350, loss = 0.09756236\n",
      "Iteration 351, loss = 0.09744668\n",
      "Iteration 352, loss = 0.09739299\n",
      "Iteration 353, loss = 0.09729155\n",
      "Iteration 354, loss = 0.09720890\n",
      "Iteration 355, loss = 0.09706331\n",
      "Iteration 356, loss = 0.09695916\n",
      "Iteration 357, loss = 0.09694898\n",
      "Iteration 358, loss = 0.09677584\n",
      "Iteration 359, loss = 0.09668289\n",
      "Iteration 360, loss = 0.09663160\n",
      "Iteration 361, loss = 0.09647515\n",
      "Iteration 362, loss = 0.09640188\n",
      "Iteration 363, loss = 0.09631438\n",
      "Iteration 364, loss = 0.09618689\n",
      "Iteration 365, loss = 0.09611968\n",
      "Iteration 366, loss = 0.09601674\n",
      "Iteration 367, loss = 0.09591552\n",
      "Iteration 368, loss = 0.09583807\n",
      "Iteration 369, loss = 0.09572690\n",
      "Iteration 370, loss = 0.09562950\n",
      "Iteration 371, loss = 0.09563401\n",
      "Iteration 372, loss = 0.09544620\n",
      "Iteration 373, loss = 0.09536773\n",
      "Iteration 374, loss = 0.09527142\n",
      "Iteration 375, loss = 0.09518075\n",
      "Iteration 376, loss = 0.09508458\n",
      "Iteration 377, loss = 0.09499859\n",
      "Iteration 378, loss = 0.09491743\n",
      "Iteration 379, loss = 0.09484461\n",
      "Iteration 380, loss = 0.09477395\n",
      "Iteration 381, loss = 0.09466032\n",
      "Iteration 382, loss = 0.09461346\n",
      "Iteration 383, loss = 0.09450560\n",
      "Iteration 384, loss = 0.09445967\n",
      "Iteration 385, loss = 0.09429867\n",
      "Iteration 386, loss = 0.09425387\n",
      "Iteration 387, loss = 0.09415690\n",
      "Iteration 388, loss = 0.09404969\n",
      "Iteration 389, loss = 0.09396284\n",
      "Iteration 390, loss = 0.09386642\n",
      "Iteration 391, loss = 0.09384550\n",
      "Iteration 392, loss = 0.09373861\n",
      "Iteration 393, loss = 0.09364859\n",
      "Iteration 394, loss = 0.09352701\n",
      "Iteration 395, loss = 0.09344796\n",
      "Iteration 396, loss = 0.09335985\n",
      "Iteration 397, loss = 0.09331606\n",
      "Iteration 398, loss = 0.09322265\n",
      "Iteration 399, loss = 0.09312057\n",
      "Iteration 400, loss = 0.09304712\n",
      "Iteration 401, loss = 0.09295881\n",
      "Iteration 402, loss = 0.09287467\n",
      "Iteration 403, loss = 0.09279979\n",
      "Iteration 404, loss = 0.09278449\n",
      "Iteration 405, loss = 0.09261764\n",
      "Iteration 406, loss = 0.09254923\n",
      "Iteration 407, loss = 0.09251872\n",
      "Iteration 408, loss = 0.09241213\n",
      "Iteration 409, loss = 0.09236514\n",
      "Iteration 410, loss = 0.09224184\n",
      "Iteration 411, loss = 0.09215266\n",
      "Iteration 412, loss = 0.09210259\n",
      "Iteration 413, loss = 0.09203431\n",
      "Iteration 414, loss = 0.09199481\n",
      "Iteration 415, loss = 0.09190241\n",
      "Iteration 416, loss = 0.09179337\n",
      "Iteration 417, loss = 0.09175636\n",
      "Iteration 418, loss = 0.09166044\n",
      "Iteration 419, loss = 0.09153777\n",
      "Iteration 420, loss = 0.09152815\n",
      "Iteration 421, loss = 0.09143305\n",
      "Iteration 422, loss = 0.09131454\n",
      "Iteration 423, loss = 0.09123582\n",
      "Iteration 424, loss = 0.09115083\n",
      "Iteration 425, loss = 0.09108218\n",
      "Iteration 426, loss = 0.09101694\n",
      "Iteration 427, loss = 0.09093382\n",
      "Iteration 428, loss = 0.09086913\n",
      "Iteration 429, loss = 0.09081455\n",
      "Iteration 430, loss = 0.09078298\n",
      "Iteration 431, loss = 0.09063525\n",
      "Iteration 432, loss = 0.09057529\n",
      "Iteration 433, loss = 0.09051500\n",
      "Iteration 434, loss = 0.09041526\n",
      "Iteration 435, loss = 0.09038324\n",
      "Iteration 436, loss = 0.09033093\n",
      "Iteration 437, loss = 0.09021634\n",
      "Iteration 438, loss = 0.09013277\n",
      "Iteration 439, loss = 0.09013501\n",
      "Iteration 440, loss = 0.08999235\n",
      "Iteration 441, loss = 0.08994282\n",
      "Iteration 442, loss = 0.08984665\n",
      "Iteration 443, loss = 0.08976716\n",
      "Iteration 444, loss = 0.08970821\n",
      "Iteration 445, loss = 0.08962799\n",
      "Iteration 446, loss = 0.08959326\n",
      "Iteration 447, loss = 0.08949233\n",
      "Iteration 448, loss = 0.08944082\n",
      "Iteration 449, loss = 0.08938937\n",
      "Iteration 450, loss = 0.08930083\n",
      "Iteration 451, loss = 0.08925166\n",
      "Iteration 452, loss = 0.08916047\n",
      "Iteration 453, loss = 0.08908487\n",
      "Iteration 454, loss = 0.08903207\n",
      "Iteration 455, loss = 0.08894367\n",
      "Iteration 456, loss = 0.08890173\n",
      "Iteration 457, loss = 0.08882975\n",
      "Iteration 458, loss = 0.08880412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 459, loss = 0.08865505\n",
      "Iteration 460, loss = 0.08864007\n",
      "Iteration 461, loss = 0.08863096\n",
      "Iteration 462, loss = 0.08861522\n",
      "Iteration 463, loss = 0.08860348\n",
      "Iteration 464, loss = 0.08860018\n",
      "Iteration 465, loss = 0.08857239\n",
      "Iteration 466, loss = 0.08856050\n",
      "Iteration 467, loss = 0.08854877\n",
      "Iteration 468, loss = 0.08853614\n",
      "Iteration 469, loss = 0.08852046\n",
      "Iteration 470, loss = 0.08850882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 471, loss = 0.08848812\n",
      "Iteration 472, loss = 0.08848592\n",
      "Iteration 473, loss = 0.08848272\n",
      "Iteration 474, loss = 0.08848068\n",
      "Iteration 475, loss = 0.08847885\n",
      "Iteration 476, loss = 0.08847534\n",
      "Iteration 477, loss = 0.08847289\n",
      "Iteration 478, loss = 0.08846993\n",
      "Iteration 479, loss = 0.08846818\n",
      "Iteration 480, loss = 0.08846570\n",
      "Iteration 481, loss = 0.08846342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 482, loss = 0.08845838\n",
      "Iteration 483, loss = 0.08845785\n",
      "Iteration 484, loss = 0.08845756\n",
      "Iteration 485, loss = 0.08845676\n",
      "Iteration 486, loss = 0.08845612\n",
      "Iteration 487, loss = 0.08845557\n",
      "Iteration 488, loss = 0.08845515\n",
      "Iteration 489, loss = 0.08845457\n",
      "Iteration 490, loss = 0.08845404\n",
      "Iteration 491, loss = 0.08845359\n",
      "Iteration 492, loss = 0.08845295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 493, loss = 0.08845222\n",
      "Iteration 494, loss = 0.08845219\n",
      "Iteration 495, loss = 0.08845210\n",
      "Iteration 496, loss = 0.08845195\n",
      "Iteration 497, loss = 0.08845179\n",
      "Iteration 498, loss = 0.08845174\n",
      "Iteration 499, loss = 0.08845160\n",
      "Iteration 500, loss = 0.08845146\n",
      "Iteration 501, loss = 0.08845143\n",
      "Iteration 502, loss = 0.08845130\n",
      "Iteration 503, loss = 0.08845118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 504, loss = 0.08845103\n",
      "Iteration 505, loss = 0.08845100\n",
      "Iteration 506, loss = 0.08845098\n",
      "Iteration 507, loss = 0.08845096\n",
      "Iteration 508, loss = 0.08845097\n",
      "Iteration 509, loss = 0.08845091\n",
      "Iteration 510, loss = 0.08845090\n",
      "Iteration 511, loss = 0.08845088\n",
      "Iteration 512, loss = 0.08845085\n",
      "Iteration 513, loss = 0.08845084\n",
      "Iteration 514, loss = 0.08845082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 515, loss = 0.08845078\n",
      "Iteration 516, loss = 0.08845078\n",
      "Iteration 517, loss = 0.08845078\n",
      "Iteration 518, loss = 0.08845077\n",
      "Iteration 519, loss = 0.08845077\n",
      "Iteration 520, loss = 0.08845076\n",
      "Iteration 521, loss = 0.08845076\n",
      "Iteration 522, loss = 0.08845075\n",
      "Iteration 523, loss = 0.08845075\n",
      "Iteration 524, loss = 0.08845075\n",
      "Iteration 525, loss = 0.08845074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.72359588\n",
      "Iteration 2, loss = 0.67462863\n",
      "Iteration 3, loss = 0.63459608\n",
      "Iteration 4, loss = 0.60118777\n",
      "Iteration 5, loss = 0.57261210\n",
      "Iteration 6, loss = 0.54767200\n",
      "Iteration 7, loss = 0.52560270\n",
      "Iteration 8, loss = 0.50599034\n",
      "Iteration 9, loss = 0.48830600\n",
      "Iteration 10, loss = 0.47210876\n",
      "Iteration 11, loss = 0.45741342\n",
      "Iteration 12, loss = 0.44385683\n",
      "Iteration 13, loss = 0.43138374\n",
      "Iteration 14, loss = 0.41964147\n",
      "Iteration 15, loss = 0.40880152\n",
      "Iteration 16, loss = 0.39848812\n",
      "Iteration 17, loss = 0.38880056\n",
      "Iteration 18, loss = 0.37966319\n",
      "Iteration 19, loss = 0.37108435\n",
      "Iteration 20, loss = 0.36279568\n",
      "Iteration 21, loss = 0.35497673\n",
      "Iteration 22, loss = 0.34760312\n",
      "Iteration 23, loss = 0.34073660\n",
      "Iteration 24, loss = 0.33401108\n",
      "Iteration 25, loss = 0.32760870\n",
      "Iteration 26, loss = 0.32152525\n",
      "Iteration 27, loss = 0.31577786\n",
      "Iteration 28, loss = 0.31033984\n",
      "Iteration 29, loss = 0.30502090\n",
      "Iteration 30, loss = 0.30001009\n",
      "Iteration 31, loss = 0.29524798\n",
      "Iteration 32, loss = 0.29074649\n",
      "Iteration 33, loss = 0.28634641\n",
      "Iteration 34, loss = 0.28211771\n",
      "Iteration 35, loss = 0.27802207\n",
      "Iteration 36, loss = 0.27418566\n",
      "Iteration 37, loss = 0.27040852\n",
      "Iteration 38, loss = 0.26680667\n",
      "Iteration 39, loss = 0.26336608\n",
      "Iteration 40, loss = 0.26002575\n",
      "Iteration 41, loss = 0.25676888\n",
      "Iteration 42, loss = 0.25364387\n",
      "Iteration 43, loss = 0.25058034\n",
      "Iteration 44, loss = 0.24765674\n",
      "Iteration 45, loss = 0.24477696\n",
      "Iteration 46, loss = 0.24190987\n",
      "Iteration 47, loss = 0.23927737\n",
      "Iteration 48, loss = 0.23667135\n",
      "Iteration 49, loss = 0.23420625\n",
      "Iteration 50, loss = 0.23175898\n",
      "Iteration 51, loss = 0.22948256\n",
      "Iteration 52, loss = 0.22707556\n",
      "Iteration 53, loss = 0.22487238\n",
      "Iteration 54, loss = 0.22275199\n",
      "Iteration 55, loss = 0.22065820\n",
      "Iteration 56, loss = 0.21858738\n",
      "Iteration 57, loss = 0.21661582\n",
      "Iteration 58, loss = 0.21470119\n",
      "Iteration 59, loss = 0.21286856\n",
      "Iteration 60, loss = 0.21098714\n",
      "Iteration 61, loss = 0.20928038\n",
      "Iteration 62, loss = 0.20743638\n",
      "Iteration 63, loss = 0.20575351\n",
      "Iteration 64, loss = 0.20407622\n",
      "Iteration 65, loss = 0.20244129\n",
      "Iteration 66, loss = 0.20083396\n",
      "Iteration 67, loss = 0.19924536\n",
      "Iteration 68, loss = 0.19773815\n",
      "Iteration 69, loss = 0.19618971\n",
      "Iteration 70, loss = 0.19474168\n",
      "Iteration 71, loss = 0.19332265\n",
      "Iteration 72, loss = 0.19189811\n",
      "Iteration 73, loss = 0.19054466\n",
      "Iteration 74, loss = 0.18917508\n",
      "Iteration 75, loss = 0.18781345\n",
      "Iteration 76, loss = 0.18648424\n",
      "Iteration 77, loss = 0.18520161\n",
      "Iteration 78, loss = 0.18398054\n",
      "Iteration 79, loss = 0.18275219\n",
      "Iteration 80, loss = 0.18153778\n",
      "Iteration 81, loss = 0.18038090\n",
      "Iteration 82, loss = 0.17928343\n",
      "Iteration 83, loss = 0.17814786\n",
      "Iteration 84, loss = 0.17707792\n",
      "Iteration 85, loss = 0.17592846\n",
      "Iteration 86, loss = 0.17493178\n",
      "Iteration 87, loss = 0.17389783\n",
      "Iteration 88, loss = 0.17286642\n",
      "Iteration 89, loss = 0.17190915\n",
      "Iteration 90, loss = 0.17089714\n",
      "Iteration 91, loss = 0.16992270\n",
      "Iteration 92, loss = 0.16899265\n",
      "Iteration 93, loss = 0.16807180\n",
      "Iteration 94, loss = 0.16717689\n",
      "Iteration 95, loss = 0.16623789\n",
      "Iteration 96, loss = 0.16536266\n",
      "Iteration 97, loss = 0.16449262\n",
      "Iteration 98, loss = 0.16367871\n",
      "Iteration 99, loss = 0.16284790\n",
      "Iteration 100, loss = 0.16206347\n",
      "Iteration 101, loss = 0.16132333\n",
      "Iteration 102, loss = 0.16044920\n",
      "Iteration 103, loss = 0.15970126\n",
      "Iteration 104, loss = 0.15892312\n",
      "Iteration 105, loss = 0.15820356\n",
      "Iteration 106, loss = 0.15748494\n",
      "Iteration 107, loss = 0.15674752\n",
      "Iteration 108, loss = 0.15605222\n",
      "Iteration 109, loss = 0.15536371\n",
      "Iteration 110, loss = 0.15468256\n",
      "Iteration 111, loss = 0.15402681\n",
      "Iteration 112, loss = 0.15334574\n",
      "Iteration 113, loss = 0.15273353\n",
      "Iteration 114, loss = 0.15210891\n",
      "Iteration 115, loss = 0.15143934\n",
      "Iteration 116, loss = 0.15088500\n",
      "Iteration 117, loss = 0.15020618\n",
      "Iteration 118, loss = 0.14958550\n",
      "Iteration 119, loss = 0.14913410\n",
      "Iteration 120, loss = 0.14842310\n",
      "Iteration 121, loss = 0.14787885\n",
      "Iteration 122, loss = 0.14729804\n",
      "Iteration 123, loss = 0.14673817\n",
      "Iteration 124, loss = 0.14621265\n",
      "Iteration 125, loss = 0.14567929\n",
      "Iteration 126, loss = 0.14514761\n",
      "Iteration 127, loss = 0.14459420\n",
      "Iteration 128, loss = 0.14411499\n",
      "Iteration 129, loss = 0.14356139\n",
      "Iteration 130, loss = 0.14308020\n",
      "Iteration 131, loss = 0.14264311\n",
      "Iteration 132, loss = 0.14209685\n",
      "Iteration 133, loss = 0.14162704\n",
      "Iteration 134, loss = 0.14112819\n",
      "Iteration 135, loss = 0.14067114\n",
      "Iteration 136, loss = 0.14018090\n",
      "Iteration 137, loss = 0.13975267\n",
      "Iteration 138, loss = 0.13930162\n",
      "Iteration 139, loss = 0.13883466\n",
      "Iteration 140, loss = 0.13838880\n",
      "Iteration 141, loss = 0.13794733\n",
      "Iteration 142, loss = 0.13763270\n",
      "Iteration 143, loss = 0.13706493\n",
      "Iteration 144, loss = 0.13667986\n",
      "Iteration 145, loss = 0.13628012\n",
      "Iteration 146, loss = 0.13582281\n",
      "Iteration 147, loss = 0.13538057\n",
      "Iteration 148, loss = 0.13499996\n",
      "Iteration 149, loss = 0.13465514\n",
      "Iteration 150, loss = 0.13424874\n",
      "Iteration 151, loss = 0.13380628\n",
      "Iteration 152, loss = 0.13349780\n",
      "Iteration 153, loss = 0.13304268\n",
      "Iteration 154, loss = 0.13270254\n",
      "Iteration 155, loss = 0.13227795\n",
      "Iteration 156, loss = 0.13193215\n",
      "Iteration 157, loss = 0.13154608\n",
      "Iteration 158, loss = 0.13122572\n",
      "Iteration 159, loss = 0.13086109\n",
      "Iteration 160, loss = 0.13050767\n",
      "Iteration 161, loss = 0.13014931\n",
      "Iteration 162, loss = 0.12979279\n",
      "Iteration 163, loss = 0.12948060\n",
      "Iteration 164, loss = 0.12911934\n",
      "Iteration 165, loss = 0.12878914\n",
      "Iteration 166, loss = 0.12844770\n",
      "Iteration 167, loss = 0.12813733\n",
      "Iteration 168, loss = 0.12784301\n",
      "Iteration 169, loss = 0.12750984\n",
      "Iteration 170, loss = 0.12715179\n",
      "Iteration 171, loss = 0.12685510\n",
      "Iteration 172, loss = 0.12655511\n",
      "Iteration 173, loss = 0.12625018\n",
      "Iteration 174, loss = 0.12592510\n",
      "Iteration 175, loss = 0.12560870\n",
      "Iteration 176, loss = 0.12534676\n",
      "Iteration 177, loss = 0.12503384\n",
      "Iteration 178, loss = 0.12476343\n",
      "Iteration 179, loss = 0.12446960\n",
      "Iteration 180, loss = 0.12419786\n",
      "Iteration 181, loss = 0.12392510\n",
      "Iteration 182, loss = 0.12360579\n",
      "Iteration 183, loss = 0.12336819\n",
      "Iteration 184, loss = 0.12306314\n",
      "Iteration 185, loss = 0.12279949\n",
      "Iteration 186, loss = 0.12251527\n",
      "Iteration 187, loss = 0.12226661\n",
      "Iteration 188, loss = 0.12201020\n",
      "Iteration 189, loss = 0.12178749\n",
      "Iteration 190, loss = 0.12147795\n",
      "Iteration 191, loss = 0.12125389\n",
      "Iteration 192, loss = 0.12095350\n",
      "Iteration 193, loss = 0.12072946\n",
      "Iteration 194, loss = 0.12055624\n",
      "Iteration 195, loss = 0.12020446\n",
      "Iteration 196, loss = 0.12001143\n",
      "Iteration 197, loss = 0.11975175\n",
      "Iteration 198, loss = 0.11952049\n",
      "Iteration 199, loss = 0.11926299\n",
      "Iteration 200, loss = 0.11903165\n",
      "Iteration 201, loss = 0.11878892\n",
      "Iteration 202, loss = 0.11854891\n",
      "Iteration 203, loss = 0.11834012\n",
      "Iteration 204, loss = 0.11808918\n",
      "Iteration 205, loss = 0.11785667\n",
      "Iteration 206, loss = 0.11763975\n",
      "Iteration 207, loss = 0.11741896\n",
      "Iteration 208, loss = 0.11721708\n",
      "Iteration 209, loss = 0.11699656\n",
      "Iteration 210, loss = 0.11677074\n",
      "Iteration 211, loss = 0.11661481\n",
      "Iteration 212, loss = 0.11640392\n",
      "Iteration 213, loss = 0.11614855\n",
      "Iteration 214, loss = 0.11597528\n",
      "Iteration 215, loss = 0.11578011\n",
      "Iteration 216, loss = 0.11553471\n",
      "Iteration 217, loss = 0.11534570\n",
      "Iteration 218, loss = 0.11511857\n",
      "Iteration 219, loss = 0.11495973\n",
      "Iteration 220, loss = 0.11472734\n",
      "Iteration 221, loss = 0.11457671\n",
      "Iteration 222, loss = 0.11436250\n",
      "Iteration 223, loss = 0.11417635\n",
      "Iteration 224, loss = 0.11400016\n",
      "Iteration 225, loss = 0.11377595\n",
      "Iteration 226, loss = 0.11362770\n",
      "Iteration 227, loss = 0.11340240\n",
      "Iteration 228, loss = 0.11322601\n",
      "Iteration 229, loss = 0.11305048\n",
      "Iteration 230, loss = 0.11289707\n",
      "Iteration 231, loss = 0.11268310\n",
      "Iteration 232, loss = 0.11250425\n",
      "Iteration 233, loss = 0.11231349\n",
      "Iteration 234, loss = 0.11213119\n",
      "Iteration 235, loss = 0.11199906\n",
      "Iteration 236, loss = 0.11180776\n",
      "Iteration 237, loss = 0.11161774\n",
      "Iteration 238, loss = 0.11146615\n",
      "Iteration 239, loss = 0.11127980\n",
      "Iteration 240, loss = 0.11112639\n",
      "Iteration 241, loss = 0.11092099\n",
      "Iteration 242, loss = 0.11077685\n",
      "Iteration 243, loss = 0.11062786\n",
      "Iteration 244, loss = 0.11043180\n",
      "Iteration 245, loss = 0.11034360\n",
      "Iteration 246, loss = 0.11010816\n",
      "Iteration 247, loss = 0.11000426\n",
      "Iteration 248, loss = 0.10980725\n",
      "Iteration 249, loss = 0.10963499\n",
      "Iteration 250, loss = 0.10951621\n",
      "Iteration 251, loss = 0.10933722\n",
      "Iteration 252, loss = 0.10917296\n",
      "Iteration 253, loss = 0.10901925\n",
      "Iteration 254, loss = 0.10883999\n",
      "Iteration 255, loss = 0.10869275\n",
      "Iteration 256, loss = 0.10857422\n",
      "Iteration 257, loss = 0.10839489\n",
      "Iteration 258, loss = 0.10825878\n",
      "Iteration 259, loss = 0.10813298\n",
      "Iteration 260, loss = 0.10796381\n",
      "Iteration 261, loss = 0.10779804\n",
      "Iteration 262, loss = 0.10765902\n",
      "Iteration 263, loss = 0.10752728\n",
      "Iteration 264, loss = 0.10736565\n",
      "Iteration 265, loss = 0.10725102\n",
      "Iteration 266, loss = 0.10707593\n",
      "Iteration 267, loss = 0.10693959\n",
      "Iteration 268, loss = 0.10681079\n",
      "Iteration 269, loss = 0.10667822\n",
      "Iteration 270, loss = 0.10658891\n",
      "Iteration 271, loss = 0.10642095\n",
      "Iteration 272, loss = 0.10625061\n",
      "Iteration 273, loss = 0.10618594\n",
      "Iteration 274, loss = 0.10600270\n",
      "Iteration 275, loss = 0.10588166\n",
      "Iteration 276, loss = 0.10574642\n",
      "Iteration 277, loss = 0.10563065\n",
      "Iteration 278, loss = 0.10546906\n",
      "Iteration 279, loss = 0.10535141\n",
      "Iteration 280, loss = 0.10518772\n",
      "Iteration 281, loss = 0.10507614\n",
      "Iteration 282, loss = 0.10492140\n",
      "Iteration 283, loss = 0.10481839\n",
      "Iteration 284, loss = 0.10473075\n",
      "Iteration 285, loss = 0.10454251\n",
      "Iteration 286, loss = 0.10446620\n",
      "Iteration 287, loss = 0.10429357\n",
      "Iteration 288, loss = 0.10419987\n",
      "Iteration 289, loss = 0.10403383\n",
      "Iteration 290, loss = 0.10395097\n",
      "Iteration 291, loss = 0.10378683\n",
      "Iteration 292, loss = 0.10372505\n",
      "Iteration 293, loss = 0.10353949\n",
      "Iteration 294, loss = 0.10349038\n",
      "Iteration 295, loss = 0.10331676\n",
      "Iteration 296, loss = 0.10318968\n",
      "Iteration 297, loss = 0.10306568\n",
      "Iteration 298, loss = 0.10294024\n",
      "Iteration 299, loss = 0.10280755\n",
      "Iteration 300, loss = 0.10269647\n",
      "Iteration 301, loss = 0.10262106\n",
      "Iteration 302, loss = 0.10245874\n",
      "Iteration 303, loss = 0.10233485\n",
      "Iteration 304, loss = 0.10224944\n",
      "Iteration 305, loss = 0.10214143\n",
      "Iteration 306, loss = 0.10201066\n",
      "Iteration 307, loss = 0.10187363\n",
      "Iteration 308, loss = 0.10175877\n",
      "Iteration 309, loss = 0.10162444\n",
      "Iteration 310, loss = 0.10151648\n",
      "Iteration 311, loss = 0.10139999\n",
      "Iteration 312, loss = 0.10133019\n",
      "Iteration 313, loss = 0.10121839\n",
      "Iteration 314, loss = 0.10102786\n",
      "Iteration 315, loss = 0.10093491\n",
      "Iteration 316, loss = 0.10079453\n",
      "Iteration 317, loss = 0.10068618\n",
      "Iteration 318, loss = 0.10056519\n",
      "Iteration 319, loss = 0.10049582\n",
      "Iteration 320, loss = 0.10039874\n",
      "Iteration 321, loss = 0.10025767\n",
      "Iteration 322, loss = 0.10016376\n",
      "Iteration 323, loss = 0.10000487\n",
      "Iteration 324, loss = 0.09990251\n",
      "Iteration 325, loss = 0.09980332\n",
      "Iteration 326, loss = 0.09970208\n",
      "Iteration 327, loss = 0.09960051\n",
      "Iteration 328, loss = 0.09946375\n",
      "Iteration 329, loss = 0.09940106\n",
      "Iteration 330, loss = 0.09925907\n",
      "Iteration 331, loss = 0.09914474\n",
      "Iteration 332, loss = 0.09907175\n",
      "Iteration 333, loss = 0.09893866\n",
      "Iteration 334, loss = 0.09884195\n",
      "Iteration 335, loss = 0.09873078\n",
      "Iteration 336, loss = 0.09865309\n",
      "Iteration 337, loss = 0.09852928\n",
      "Iteration 338, loss = 0.09842861\n",
      "Iteration 339, loss = 0.09831279\n",
      "Iteration 340, loss = 0.09821709\n",
      "Iteration 341, loss = 0.09812282\n",
      "Iteration 342, loss = 0.09812022\n",
      "Iteration 343, loss = 0.09791157\n",
      "Iteration 344, loss = 0.09780581\n",
      "Iteration 345, loss = 0.09770376\n",
      "Iteration 346, loss = 0.09760614\n",
      "Iteration 347, loss = 0.09756681\n",
      "Iteration 348, loss = 0.09741630\n",
      "Iteration 349, loss = 0.09732749\n",
      "Iteration 350, loss = 0.09723186\n",
      "Iteration 351, loss = 0.09711401\n",
      "Iteration 352, loss = 0.09705824\n",
      "Iteration 353, loss = 0.09696962\n",
      "Iteration 354, loss = 0.09687528\n",
      "Iteration 355, loss = 0.09673246\n",
      "Iteration 356, loss = 0.09664267\n",
      "Iteration 357, loss = 0.09663757\n",
      "Iteration 358, loss = 0.09645425\n",
      "Iteration 359, loss = 0.09638129\n",
      "Iteration 360, loss = 0.09631993\n",
      "Iteration 361, loss = 0.09615706\n",
      "Iteration 362, loss = 0.09609345\n",
      "Iteration 363, loss = 0.09600357\n",
      "Iteration 364, loss = 0.09588418\n",
      "Iteration 365, loss = 0.09582914\n",
      "Iteration 366, loss = 0.09572680\n",
      "Iteration 367, loss = 0.09562582\n",
      "Iteration 368, loss = 0.09554367\n",
      "Iteration 369, loss = 0.09542947\n",
      "Iteration 370, loss = 0.09534198\n",
      "Iteration 371, loss = 0.09534513\n",
      "Iteration 372, loss = 0.09515408\n",
      "Iteration 373, loss = 0.09507451\n",
      "Iteration 374, loss = 0.09498505\n",
      "Iteration 375, loss = 0.09490718\n",
      "Iteration 376, loss = 0.09480593\n",
      "Iteration 377, loss = 0.09471470\n",
      "Iteration 378, loss = 0.09463602\n",
      "Iteration 379, loss = 0.09455004\n",
      "Iteration 380, loss = 0.09449266\n",
      "Iteration 381, loss = 0.09436886\n",
      "Iteration 382, loss = 0.09433462\n",
      "Iteration 383, loss = 0.09421689\n",
      "Iteration 384, loss = 0.09417193\n",
      "Iteration 385, loss = 0.09401995\n",
      "Iteration 386, loss = 0.09396808\n",
      "Iteration 387, loss = 0.09389448\n",
      "Iteration 388, loss = 0.09378410\n",
      "Iteration 389, loss = 0.09369583\n",
      "Iteration 390, loss = 0.09360518\n",
      "Iteration 391, loss = 0.09359158\n",
      "Iteration 392, loss = 0.09347854\n",
      "Iteration 393, loss = 0.09338862\n",
      "Iteration 394, loss = 0.09327890\n",
      "Iteration 395, loss = 0.09320426\n",
      "Iteration 396, loss = 0.09311351\n",
      "Iteration 397, loss = 0.09305936\n",
      "Iteration 398, loss = 0.09296740\n",
      "Iteration 399, loss = 0.09287635\n",
      "Iteration 400, loss = 0.09280115\n",
      "Iteration 401, loss = 0.09271793\n",
      "Iteration 402, loss = 0.09263172\n",
      "Iteration 403, loss = 0.09255478\n",
      "Iteration 404, loss = 0.09254826\n",
      "Iteration 405, loss = 0.09238776\n",
      "Iteration 406, loss = 0.09230522\n",
      "Iteration 407, loss = 0.09226888\n",
      "Iteration 408, loss = 0.09217903\n",
      "Iteration 409, loss = 0.09209865\n",
      "Iteration 410, loss = 0.09201098\n",
      "Iteration 411, loss = 0.09191547\n",
      "Iteration 412, loss = 0.09186087\n",
      "Iteration 413, loss = 0.09178739\n",
      "Iteration 414, loss = 0.09176902\n",
      "Iteration 415, loss = 0.09167717\n",
      "Iteration 416, loss = 0.09156250\n",
      "Iteration 417, loss = 0.09153173\n",
      "Iteration 418, loss = 0.09143221\n",
      "Iteration 419, loss = 0.09130791\n",
      "Iteration 420, loss = 0.09130022\n",
      "Iteration 421, loss = 0.09118324\n",
      "Iteration 422, loss = 0.09107988\n",
      "Iteration 423, loss = 0.09100841\n",
      "Iteration 424, loss = 0.09092530\n",
      "Iteration 425, loss = 0.09085846\n",
      "Iteration 426, loss = 0.09079020\n",
      "Iteration 427, loss = 0.09069818\n",
      "Iteration 428, loss = 0.09063263\n",
      "Iteration 429, loss = 0.09057916\n",
      "Iteration 430, loss = 0.09052998\n",
      "Iteration 431, loss = 0.09040578\n",
      "Iteration 432, loss = 0.09034465\n",
      "Iteration 433, loss = 0.09028415\n",
      "Iteration 434, loss = 0.09018945\n",
      "Iteration 435, loss = 0.09016524\n",
      "Iteration 436, loss = 0.09009945\n",
      "Iteration 437, loss = 0.08999097\n",
      "Iteration 438, loss = 0.08992259\n",
      "Iteration 439, loss = 0.08990733\n",
      "Iteration 440, loss = 0.08977237\n",
      "Iteration 441, loss = 0.08972336\n",
      "Iteration 442, loss = 0.08963985\n",
      "Iteration 443, loss = 0.08955429\n",
      "Iteration 444, loss = 0.08949449\n",
      "Iteration 445, loss = 0.08941140\n",
      "Iteration 446, loss = 0.08937677\n",
      "Iteration 447, loss = 0.08928448\n",
      "Iteration 448, loss = 0.08922621\n",
      "Iteration 449, loss = 0.08918037\n",
      "Iteration 450, loss = 0.08907874\n",
      "Iteration 451, loss = 0.08905647\n",
      "Iteration 452, loss = 0.08895127\n",
      "Iteration 453, loss = 0.08886503\n",
      "Iteration 454, loss = 0.08882458\n",
      "Iteration 455, loss = 0.08873578\n",
      "Iteration 456, loss = 0.08869915\n",
      "Iteration 457, loss = 0.08861487\n",
      "Iteration 458, loss = 0.08858548\n",
      "Iteration 459, loss = 0.08847729\n",
      "Iteration 460, loss = 0.08839351\n",
      "Iteration 461, loss = 0.08835193\n",
      "Iteration 462, loss = 0.08828544\n",
      "Iteration 463, loss = 0.08823943\n",
      "Iteration 464, loss = 0.08821990\n",
      "Iteration 465, loss = 0.08808222\n",
      "Iteration 466, loss = 0.08802412\n",
      "Iteration 467, loss = 0.08795732\n",
      "Iteration 468, loss = 0.08790180\n",
      "Iteration 469, loss = 0.08783416\n",
      "Iteration 470, loss = 0.08778191\n",
      "Iteration 471, loss = 0.08768492\n",
      "Iteration 472, loss = 0.08763396\n",
      "Iteration 473, loss = 0.08756660\n",
      "Iteration 474, loss = 0.08751768\n",
      "Iteration 475, loss = 0.08747770\n",
      "Iteration 476, loss = 0.08738309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 477, loss = 0.08729713\n",
      "Iteration 478, loss = 0.08728389\n",
      "Iteration 479, loss = 0.08727248\n",
      "Iteration 480, loss = 0.08726472\n",
      "Iteration 481, loss = 0.08725142\n",
      "Iteration 482, loss = 0.08723229\n",
      "Iteration 483, loss = 0.08722136\n",
      "Iteration 484, loss = 0.08721752\n",
      "Iteration 485, loss = 0.08719836\n",
      "Iteration 486, loss = 0.08718557\n",
      "Iteration 487, loss = 0.08716948\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 488, loss = 0.08715510\n",
      "Iteration 489, loss = 0.08715190\n",
      "Iteration 490, loss = 0.08714998\n",
      "Iteration 491, loss = 0.08714801\n",
      "Iteration 492, loss = 0.08714479\n",
      "Iteration 493, loss = 0.08714204\n",
      "Iteration 494, loss = 0.08714124\n",
      "Iteration 495, loss = 0.08713878\n",
      "Iteration 496, loss = 0.08713497\n",
      "Iteration 497, loss = 0.08713219\n",
      "Iteration 498, loss = 0.08713053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 499, loss = 0.08712659\n",
      "Iteration 500, loss = 0.08712576\n",
      "Iteration 501, loss = 0.08712571\n",
      "Iteration 502, loss = 0.08712507\n",
      "Iteration 503, loss = 0.08712452\n",
      "Iteration 504, loss = 0.08712412\n",
      "Iteration 505, loss = 0.08712343\n",
      "Iteration 506, loss = 0.08712296\n",
      "Iteration 507, loss = 0.08712260\n",
      "Iteration 508, loss = 0.08712253\n",
      "Iteration 509, loss = 0.08712149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 510, loss = 0.08712085\n",
      "Iteration 511, loss = 0.08712075\n",
      "Iteration 512, loss = 0.08712063\n",
      "Iteration 513, loss = 0.08712056\n",
      "Iteration 514, loss = 0.08712044\n",
      "Iteration 515, loss = 0.08712034\n",
      "Iteration 516, loss = 0.08712027\n",
      "Iteration 517, loss = 0.08712015\n",
      "Iteration 518, loss = 0.08712003\n",
      "Iteration 519, loss = 0.08711993\n",
      "Iteration 520, loss = 0.08711985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 521, loss = 0.08711971\n",
      "Iteration 522, loss = 0.08711969\n",
      "Iteration 523, loss = 0.08711968\n",
      "Iteration 524, loss = 0.08711965\n",
      "Iteration 525, loss = 0.08711963\n",
      "Iteration 526, loss = 0.08711961\n",
      "Iteration 527, loss = 0.08711959\n",
      "Iteration 528, loss = 0.08711958\n",
      "Iteration 529, loss = 0.08711956\n",
      "Iteration 530, loss = 0.08711953\n",
      "Iteration 531, loss = 0.08711952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 532, loss = 0.08711949\n",
      "Iteration 533, loss = 0.08711948\n",
      "Iteration 534, loss = 0.08711948\n",
      "Iteration 535, loss = 0.08711948\n",
      "Iteration 536, loss = 0.08711947\n",
      "Iteration 537, loss = 0.08711947\n",
      "Iteration 538, loss = 0.08711947\n",
      "Iteration 539, loss = 0.08711946\n",
      "Iteration 540, loss = 0.08711946\n",
      "Iteration 541, loss = 0.08711946\n",
      "Iteration 542, loss = 0.08711945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.html\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(30,30,)], # [(60,),(100,),(60,10),(30,30,)]\n",
    "    'mlp__activation' : ['relu'], #['logistic','tanh','relu'] \n",
    "    'mlp__solver' : ['sgd'],\n",
    "    'mlp__alpha' : [0.1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1 ,*0.1\n",
    "    'mlp__batch_size' : [99], # ~297 samples for train at inner cv -> 3 equal size batches , #[16,32,64,128,'auto']\n",
    "    'mlp__learning_rate' : ['constant','adaptive'], # [*'constant','invscaling','adaptive']\n",
    "    'mlp__learning_rate_init' : [0.01],\n",
    "    'mlp__power_t' : [0.5],\n",
    "    'mlp__momentum' : [0.2], # np.arange(0.1,1,0.1), *0.2\n",
    "    'mlp__max_iter' : [1000],\n",
    "    'mlp__verbose' : [1]\n",
    "}\n",
    "\n",
    "search_sgd = RandomizedSearchCV(clf_mlp, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=100)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.968     0.986     0.977       212\n",
      "   Malignant      0.986     0.967     0.976       212\n",
      "\n",
      "    accuracy                          0.976       424\n",
      "   macro avg      0.977     0.976     0.976       424\n",
      "weighted avg      0.977     0.976     0.976       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Solver : LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.html\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(30,30,)],  #[(60,),(100,),(60,10),(30,30,)]\n",
    "    'mlp__activation' : ['relu'],\n",
    "    'mlp__solver' : ['lbfgs'],\n",
    "    'mlp__alpha' : [1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1\n",
    "    'mlp__max_iter' : np.arange(300,500,10) , # np.arange(100,300,10) , [100,200,500,1000]\n",
    "}\n",
    "\n",
    "search_lbfgs = RandomizedSearchCV(clf_mlp, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=1000)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_lbfgs, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.972     0.981     0.977       212\n",
      "   Malignant      0.981     0.972     0.976       212\n",
      "\n",
      "    accuracy                          0.976       424\n",
      "   macro avg      0.976     0.976     0.976       424\n",
      "weighted avg      0.976     0.976     0.976       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16'></a>\n",
    "## 16) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below are the tables of the specific feature selection method.\n",
    "* The performance of the algorithms is in descending order.\n",
    "* All the results are the average values of a 10-fold cross validation.\n",
    "* The columns contain the accuracy and the average values of precision, recall and f1 score.\n",
    "* It is observed that the number of samples of Βenign and Μalignant cancer are equal (212 respectively), so the weighted average and the macro average are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"6\"> All features : Default algorithms</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>precision </th>\n",
    "        <th>recall</th>\n",
    "        <th>f1 score</th>\n",
    "        <th>accuracy</th>\n",
    "        <th>Execution Time<br>\n",
    "        (seconds)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SVC</th>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.094</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MLP</th>\n",
    "        <td>0.972</td>\n",
    "        <td>0.972</td>\n",
    "        <td>0.972</td>\n",
    "        <td>0.972</td>\n",
    "        <td>4.628</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LGBM</th>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Random Forest</th>\n",
    "        <td>0.967</td>\n",
    "        <td>0.967</td>\n",
    "        <td>0.967</td>\n",
    "        <td>0.967</td>\n",
    "        <td>2.059</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>AdaBoost</th>\n",
    "        <td>0.967</td>\n",
    "        <td>0.967</td>\n",
    "        <td>0.967</td>\n",
    "        <td>0.967</td>\n",
    "        <td>1.247</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ridge</th>\n",
    "        <td>0.966</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.120</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>XGBoost</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.967</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LDA</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.202</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>KNN</th>\n",
    "        <td>0.961</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.093</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>QDA</th>\n",
    "        <td>0.951</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.131</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SGD</th>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.082</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>GNB</th>\n",
    "        <td>0.935</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.064</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Decision Tree</th>\n",
    "        <td>0.920</td>\n",
    "        <td>0.920</td>\n",
    "        <td>0.920</td>\n",
    "        <td>0.920</td>\n",
    "        <td>0.153</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"6\"> All features : Tuned algorithms</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>precision </th>\n",
    "        <th>recall</th>\n",
    "        <th>f1 score</th>\n",
    "        <th>accuracy</th>\n",
    "        <th>Execution Time<br>\n",
    "        (seconds)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MLP</th>\n",
    "        <td>0.981</td>\n",
    "        <td>0.981</td>\n",
    "        <td>0.981</td>\n",
    "        <td>0.981</td>\n",
    "        <td>trial and error</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SGD</th>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>4.961</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SVC</th>\n",
    "        <td>0.977</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>7.133</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LGBM</th>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>157.276</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>KNN</th>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>119.102</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>QDA</th>\n",
    "        <td>0.966</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>32.733</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>XGBoost</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>352.877</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ridge</th>\n",
    "        <td>0.964</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>72.653</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LDA</th>\n",
    "        <td>0.963</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>7.975</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>AdaBoost</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>0.962</td>\n",
    "        <td>65.299</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Random Forest</th>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>234.318</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>GNB</th>\n",
    "        <td>0.931</td>\n",
    "        <td>0.929</td>\n",
    "        <td>0.929</td>\n",
    "        <td>0.929</td>\n",
    "        <td>9.786</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Decision Tree</th>\n",
    "        <td>0.929</td>\n",
    "        <td>0.929</td>\n",
    "        <td>0.929</td>\n",
    "        <td>0.929</td>\n",
    "        <td>66.677</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As it is seen, some models perform better with default parameters. This can happen for various reasons such as:\n",
    "    - Unlucky selection of hyperparameters from random search\n",
    "    - Hyperparameters selected cause overfitting\n",
    "    - Smaller training sample in the inner loop due to nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Sources for nested cross validation :\n",
    "    1. [Cross-Validation and Hyperparameter Search in scikit-learn - A Complete Guide](<https://dev.to/balapriya/cross-validation-and-hyperparameter-search-in-scikit-learn-a-complete-guide-5ed8>)\n",
    "    2. [Nested Cross Validation for Algorithm Selection](<https://vitalflux.com/python-nested-cross-validation-algorithm-selection/>)\n",
    "    3. [Nested Cross-Validation for Machine Learning with Python](<https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/>)\n",
    "    4. [Nested cross validation for model selection](<https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection/65158#65158>)\n",
    "    5. [scikit-learn GridSearchCV with multiple repetitions](<https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764>)\n",
    "    6. [Model selection done right: A gentle introduction to nested cross-validation](<https://ploomber.io/blog/nested-cv/>)\n",
    "    7. [How to obtain optimal hyperparameters after nested cross validation?](<https://stats.stackexchange.com/questions/254612/how-to-obtain-optimal-hyperparameters-after-nested-cross-validation>)\n",
    "    8. [Cross-validation for parameter tuning, model selection, and feature selection](<https://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb>)\n",
    "- Sources for Hyper Parameter-Optimization :\n",
    "    1. [Random Search for Hyper-Parameter Optimization](<https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>)\n",
    "    2. [Hyperparameter tuning for machine learning models](<https://www.jeremyjordan.me/hyperparameter-tuning/>)\n",
    "- Sources for code :\n",
    "    - All sources are in comments at each code part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
