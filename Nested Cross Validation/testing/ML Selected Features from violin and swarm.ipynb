{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Διπλωματική Εργασία\n",
    "## Ταξινόμηση του καρκίνου του μαστού με μεθόδους μηχανικής μάθησης\n",
    "### Εξαγωγή χαρακτηριστικών με PCA\n",
    "\n",
    "> Λάζαρος Πανιτσίδης<br />\n",
    "> Τμήμα Μηχανικών Παραγωγής και Διοίκησης <br />\n",
    "> Διεθνές Πανεπιστήμιο της Ελλάδος <br />\n",
    "> lazarospanitsidis@outlook.com -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diploma thesis\n",
    "## Breast cancer classification using machine learning methods\n",
    "### Selected Features\n",
    "\n",
    "> Lazaros Panitsidis<br />\n",
    "> Department of Industrial Engineering and Management <br />\n",
    "> International Hellenic University <br />\n",
    "> lazarospanitsidis@outlook.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Useful Python Libraries](#1)\n",
    "1. [Data Processing](#2)\n",
    "1. [Gaussian Naive Bayes](#3)\n",
    "1. [Linear Discriminant Analysis](#4)\n",
    "1. [Quadratic Discriminant Analysis](#5)\n",
    "1. [Ridge Classifier](#6)\n",
    "1. [Decision Tree Classifier](#7)\n",
    "1. [Random Forest Classifier](#8)\n",
    "1. [ADA Boost Classifier (Adaptive Boosting)](#9)\n",
    "1. [C-Support Vector Classification](#10)\n",
    "1. [Stochastic Gradient Descent Classifier](#11)\n",
    "1. [eXtreme Gradient Boosting](#12)\n",
    "1. [Light Gradient Boosting Machine](#13)\n",
    "1. [K-Nearest Neighbors Classifier](#14)\n",
    "1. [Multi-layer Perceptron Classifier](#15)\n",
    "1. [Summary](#16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1) Useful Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#import warnings library\n",
    "import warnings\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# some of them are not used in this file\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE, RFECV , mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score , GridSearchCV , LeaveOneOut,KFold,RandomizedSearchCV,StratifiedKFold, HalvingGridSearchCV\n",
    "from skopt import BayesSearchCV # https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV , https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score , make_scorer , classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline , Pipeline # https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder , MinMaxScaler\n",
    "from xgboost import XGBClassifier , plot_importance\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier , RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import lightgbm as lgbm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pygad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2) Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 424 cases in this dataset\n",
      "There are 30 features in this dataset\n",
      "There are 212 cases diagnosed as malignant tumor\n",
      "There are 212 cases diagnosed as benign tumor\n",
      "The percentage of malignant cases is: 50.00%\n"
     ]
    }
   ],
   "source": [
    "dataWISC = pd.read_csv('dataWisc.csv')\n",
    "dataWISC.drop([\"id\", \"Unnamed: 32\"], axis = 1, inplace = True)\n",
    "\n",
    "# Undersampling function\n",
    "def make_undersample(_df, column):\n",
    "  dfs_r = {}\n",
    "  dfs_c = {}\n",
    "  smaller = 1e1000\n",
    "  ignore = \"\"\n",
    "  for c in _df[column].unique():\n",
    "    dfs_c[c] = _df[_df[column] == c]\n",
    "    if dfs_c[c].shape[0] < smaller:\n",
    "      smaller = dfs_c[c].shape[0]\n",
    "      ignore = c\n",
    "\n",
    "  for c in dfs_c:\n",
    "    if c == ignore:\n",
    "      continue\n",
    "    dfs_r[c] = resample(dfs_c[c], \n",
    "                        replace=False, # sample without replacement\n",
    "                        n_samples=smaller,\n",
    "                        random_state=0)\n",
    "  return pd.concat([dfs_r[c] for c in dfs_r] + [dfs_c[ignore]])\n",
    "\n",
    "dataWISC = make_undersample(dataWISC,'diagnosis')\n",
    "\n",
    "#Description of the dataset\n",
    "\n",
    "#how many cases are included in the dataset\n",
    "length = len(dataWISC)\n",
    "#how many features are in the dataset\n",
    "features = dataWISC.shape[1]-1 # - diagnosis\n",
    "\n",
    "# Number of malignant cases\n",
    "malignant = len(dataWISC[dataWISC['diagnosis']=='M'])\n",
    "\n",
    "#Number of benign cases\n",
    "benign = len(dataWISC[dataWISC['diagnosis']=='B'])\n",
    "\n",
    "#Rate of malignant tumors over all cases\n",
    "rate = (float(malignant)/(length))*100\n",
    "\n",
    "print (\"There are \"+ str(len(dataWISC))+\" cases in this dataset\")\n",
    "print (\"There are {}\".format(features)+\" features in this dataset\")\n",
    "print (\"There are {}\".format(malignant)+\" cases diagnosed as malignant tumor\")\n",
    "print (\"There are {}\".format(benign)+\" cases diagnosed as benign tumor\")\n",
    "print (\"The percentage of malignant cases is: {:.2f}%\".format(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataWISC.diagnosis                          # M or B \n",
    "x = dataWISC.drop('diagnosis',axis = 1 )\n",
    "target_names=['Benign','Malignant']\n",
    "le= LabelEncoder()\n",
    "le.fit(y)\n",
    "y_le = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13.49</td>\n",
       "      <td>0.07698</td>\n",
       "      <td>0.04751</td>\n",
       "      <td>0.033840</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>698.8</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>12.58</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.2719</td>\n",
       "      <td>564.1</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.008772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14.87</td>\n",
       "      <td>0.08345</td>\n",
       "      <td>0.06824</td>\n",
       "      <td>0.049510</td>\n",
       "      <td>0.2323</td>\n",
       "      <td>783.6</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "49         13.49           0.07698         0.04751             0.033840   \n",
       "285        12.58           0.04216         0.00186             0.002924   \n",
       "495        14.87           0.08345         0.06824             0.049510   \n",
       "\n",
       "     radius_se  area_worst  concavity_worst  concave points_worst  \n",
       "49      0.2338       698.8         0.228200              0.128200  \n",
       "285     0.2719       564.1         0.005579              0.008772  \n",
       "495     0.2323       783.6         0.170000              0.101700  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = x[[ 'radius_mean',\n",
    "            'compactness_mean',\n",
    "            'concavity_mean',\n",
    "            'concave points_mean',\n",
    "            'radius_se',\n",
    "            'area_worst',\n",
    "            'concavity_worst',\n",
    "            'concave points_worst']]\n",
    "x_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/#:~:text=Given%20the%20improved%20estimate%20of,biased%20estimates%20of%20model%20performance.\n",
    "# cv = LeaveOneOut()\n",
    "rng = np.random.RandomState(13) # random number generator , use it in every random state if shuffle=True for different results.Usefull to test a specific algorithm multiple times within a for loop.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "search_cv = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "  originalclass.extend(y_true)\n",
    "  predictedclass.extend(y_pred)\n",
    "  #print(classification_report(y_true, y_pred, target_names=target_names)) \n",
    "  return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def print_best_params(search):\n",
    "    print(\"\")\n",
    "    print(\"Best hyperparameters : \", search.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best estimator : \", search.best_estimator_)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method didn't work so it will not be used (nested cross validation which returns the best parameters and their scores)\n",
    "\n",
    "# Following kf is the outer loop\n",
    "outer_kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=13)\n",
    "inner_kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=13)\n",
    "# model = SVC()\n",
    "# params = {'kernel':['rbf','linear'],'C':[1,10]}\n",
    "def nested_cv_with_gscv(model,params,x,y):\n",
    "    outer_loop_accuracy_scores = []\n",
    "    inner_loop_won_params = []\n",
    "    inner_loop_accuracy_scores = []\n",
    "\n",
    "    # Looping through the outer loop, feeding each training set into a GSCV as the inner loop\n",
    "    for train_index,test_index in outer_kf.split(x,y):\n",
    "        \n",
    "        GSCV = GridSearchCV(estimator=model,param_grid=params,cv=inner_kf)\n",
    "        \n",
    "        # GSCV is looping through the training data to find the best parameters. This is the inner loop\n",
    "        GSCV.fit(x[train_index],y[train_index])\n",
    "        \n",
    "        # The best hyper parameters from GSCV is now being tested on the unseen outer loop test data.\n",
    "        pred = GSCV.predict(x[test_index])\n",
    "        \n",
    "        # Appending the \"winning\" hyper parameters and their associated accuracy score\n",
    "        inner_loop_won_params.append(GSCV.best_params_)\n",
    "        outer_loop_accuracy_scores.append(accuracy_score(y[test_index],pred))\n",
    "        inner_loop_accuracy_scores.append(GSCV.best_score_)\n",
    "\n",
    "    for i in zip(inner_loop_won_params,outer_loop_accuracy_scores,inner_loop_accuracy_scores):\n",
    "        print (i)\n",
    "\n",
    "    print('Mean of outer loop accuracy score:',np.mean(outer_loop_accuracy_scores))\n",
    "\n",
    "# https://github.com/rosscleung/Projects/blob/b9abc20db545d9f483e90a9b046ea50c74f25718/Tutorial%20notebooks/Nested%20Cross%20Validation%20Example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The process by which the best model will be selected is as follows:\n",
    "1. Evaluation of the default algorithm with 10-fold cross validation\n",
    "2. Evaluation of the tuned hyperparameter algorithm with nested cross-validation (5-fold Grid Search/Randomized Search inside a 10-fold cross validation)\n",
    "3. Choosing the best model (from steps 1 and 2) and finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3) [Gaussian Naive Bayes](<https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.908     0.976     0.941       212\n",
      "   Malignant      0.974     0.901     0.936       212\n",
      "\n",
      "    accuracy                          0.939       424\n",
      "   macro avg      0.941     0.939     0.939       424\n",
      "weighted avg      0.941     0.939     0.939       424\n",
      "\n",
      "--- Time of execution : 0.06582403182983398 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_gnb = Pipeline([('scaler', StandardScaler()), ('gnb', GaussianNB())])\n",
    "score = cross_val_score(clf_gnb, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.908     0.981     0.943       212\n",
      "   Malignant      0.979     0.901     0.939       212\n",
      "\n",
      "    accuracy                          0.941       424\n",
      "   macro avg      0.944     0.941     0.941       424\n",
      "weighted avg      0.944     0.941     0.941       424\n",
      "\n",
      "--- Time of execution : 9.840656995773315 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = { 'gnb__var_smoothing': np.logspace(0,-10, num=100) }\n",
    "\n",
    "search = GridSearchCV(clf_gnb, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4) [Linear Discriminant Analysis](<https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.894     0.953     0.922       212\n",
      "   Malignant      0.949     0.887     0.917       212\n",
      "\n",
      "    accuracy                          0.920       424\n",
      "   macro avg      0.922     0.920     0.920       424\n",
      "weighted avg      0.922     0.920     0.920       424\n",
      "\n",
      "--- Time of execution : 0.08078479766845703 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_lda = Pipeline([('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis())])\n",
    "\n",
    "score = cross_val_score(clf_lda, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.894     0.953     0.922       212\n",
      "   Malignant      0.949     0.887     0.917       212\n",
      "\n",
      "    accuracy                          0.920       424\n",
      "   macro avg      0.922     0.920     0.920       424\n",
      "weighted avg      0.922     0.920     0.920       424\n",
      "\n",
      "--- Time of execution : 7.515499114990234 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = [\n",
    "    {\n",
    "        'lda__solver' : ['lsqr','eigen'],\n",
    "        'lda__shrinkage':[None,'auto']\n",
    "    },\n",
    "    {\n",
    "        'lda__solver' : ['svd'],\n",
    "        'lda__tol': np.linspace(0, 0.01, num=100)\n",
    "    }\n",
    "]\n",
    "\n",
    "search = RandomizedSearchCV(clf_lda, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5) [Quadratic Discriminant Analysis](<https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.957     0.943     0.950       212\n",
      "   Malignant      0.944     0.958     0.951       212\n",
      "\n",
      "    accuracy                          0.950       424\n",
      "   macro avg      0.951     0.950     0.950       424\n",
      "weighted avg      0.951     0.950     0.950       424\n",
      "\n",
      "--- Time of execution : 0.10073089599609375 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_qda = Pipeline([('scaler', StandardScaler()), ('qda', QuadraticDiscriminantAnalysis())])\n",
    "\n",
    "score = cross_val_score(clf_qda, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.957     0.943     0.950       212\n",
      "   Malignant      0.944     0.958     0.951       212\n",
      "\n",
      "    accuracy                          0.950       424\n",
      "   macro avg      0.951     0.950     0.950       424\n",
      "weighted avg      0.951     0.950     0.950       424\n",
      "\n",
      "--- Time of execution : 39.69619059562683 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'qda__reg_param': np.linspace(0, 1, num=100),\n",
    "    'qda__tol': np.linspace(0, 0.01, num=100)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_qda, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6) [Ridge Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.898     0.958     0.927       212\n",
      "   Malignant      0.955     0.892     0.922       212\n",
      "\n",
      "    accuracy                          0.925       424\n",
      "   macro avg      0.926     0.925     0.924       424\n",
      "weighted avg      0.926     0.925     0.924       424\n",
      "\n",
      "--- Time of execution : 0.13416314125061035 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_rc = Pipeline([('scaler', StandardScaler()), ('rg', RidgeClassifier())])\n",
    "\n",
    "score = cross_val_score(clf_rc, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.894     0.958     0.925       212\n",
      "   Malignant      0.954     0.887     0.919       212\n",
      "\n",
      "    accuracy                          0.922       424\n",
      "   macro avg      0.924     0.922     0.922       424\n",
      "weighted avg      0.924     0.922     0.922       424\n",
      "\n",
      "--- Time of execution : 43.70612049102783 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'rg__alpha' : np.linspace(0, 1, num=10),\n",
    "    'rg__fit_intercept' : [True,False],\n",
    "    'rg__copy_X' : [True,False],\n",
    "    'rg__max_iter' : [None],\n",
    "    'rg__tol' : [0.001],\n",
    "    'rg__class_weight' : [None,'balanced'],\n",
    "    'rg__solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'rg__positive' : [False]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_rc, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=500)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7) [Decision Tree Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.924     0.920     0.922       212\n",
      "   Malignant      0.920     0.925     0.922       212\n",
      "\n",
      "    accuracy                          0.922       424\n",
      "   macro avg      0.922     0.922     0.922       424\n",
      "weighted avg      0.922     0.922     0.922       424\n",
      "\n",
      "--- Time of execution : 0.09126710891723633 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_tree = Pipeline([('scaler', StandardScaler()), ('tree', DecisionTreeClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_tree, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.900     0.929     0.914       212\n",
      "   Malignant      0.927     0.896     0.911       212\n",
      "\n",
      "    accuracy                          0.913       424\n",
      "   macro avg      0.913     0.913     0.913       424\n",
      "weighted avg      0.913     0.913     0.913       424\n",
      "\n",
      "--- Time of execution : 51.515055418014526 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'tree__criterion' :['gini','entropy'],\n",
    "    'tree__splitter' : ['best','random'],\n",
    "    'tree__max_depth': [list(range(2, 20)),None],\n",
    "    'tree__min_samples_split': list(range(2, 6)),\n",
    "    'tree__min_samples_leaf': list(range(1, 8)),\n",
    "    'tree__min_weight_fraction_leaf' : [0.0],\n",
    "    'tree__max_features': [None, 'sqrt', 'log2'],\n",
    "    'tree__max_leaf_nodes' : [None],\n",
    "    'tree__min_impurity_decrease' : [0.0],\n",
    "    'tree__class_weight' : [None,'balanced'],\n",
    "    'tree__ccp_alpha' : [0.0],\n",
    "    'tree__random_state' : [13]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_tree, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=1000)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters :  {'tree__splitter': 'best', 'tree__random_state': 13, 'tree__min_weight_fraction_leaf': 0.0, 'tree__min_samples_split': 2, 'tree__min_samples_leaf': 2, 'tree__min_impurity_decrease': 0.0, 'tree__max_leaf_nodes': None, 'tree__max_features': None, 'tree__max_depth': None, 'tree__criterion': 'gini', 'tree__class_weight': None, 'tree__ccp_alpha': 0.0}\n",
      "\n",
      "Best estimator :  Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('tree',\n",
      "                 DecisionTreeClassifier(min_samples_leaf=2, random_state=13))])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tree__splitter</th>\n",
       "      <th>param_tree__random_state</th>\n",
       "      <th>param_tree__min_weight_fraction_leaf</th>\n",
       "      <th>param_tree__min_samples_split</th>\n",
       "      <th>param_tree__min_samples_leaf</th>\n",
       "      <th>param_tree__min_impurity_decrease</th>\n",
       "      <th>param_tree__max_leaf_nodes</th>\n",
       "      <th>param_tree__max_features</th>\n",
       "      <th>param_tree__max_depth</th>\n",
       "      <th>param_tree__criterion</th>\n",
       "      <th>param_tree__class_weight</th>\n",
       "      <th>param_tree__ccp_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.012611</td>\n",
       "      <td>0.013762</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.917601</td>\n",
       "      <td>0.976467</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.025286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.917601</td>\n",
       "      <td>0.976467</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.025286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>best</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'tree__splitter': 'best', 'tree__random_state...</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.976467</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.976177</td>\n",
       "      <td>0.945788</td>\n",
       "      <td>0.025293</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "226       0.012611      0.013762         0.002792        0.001596   \n",
       "499       0.004495      0.000452         0.002906        0.000807   \n",
       "113       0.005186      0.000747         0.001596        0.000489   \n",
       "\n",
       "    param_tree__splitter param_tree__random_state  \\\n",
       "226                 best                       13   \n",
       "499                 best                       13   \n",
       "113                 best                       13   \n",
       "\n",
       "    param_tree__min_weight_fraction_leaf param_tree__min_samples_split  \\\n",
       "226                                  0.0                             2   \n",
       "499                                  0.0                             4   \n",
       "113                                  0.0                             4   \n",
       "\n",
       "    param_tree__min_samples_leaf param_tree__min_impurity_decrease  \\\n",
       "226                            2                               0.0   \n",
       "499                            2                               0.0   \n",
       "113                            5                               0.0   \n",
       "\n",
       "    param_tree__max_leaf_nodes param_tree__max_features param_tree__max_depth  \\\n",
       "226                       None                     None                  None   \n",
       "499                       None                     None                  None   \n",
       "113                       None                     None                  None   \n",
       "\n",
       "    param_tree__criterion param_tree__class_weight param_tree__ccp_alpha  \\\n",
       "226                  gini                     None                   0.0   \n",
       "499                  gini                     None                   0.0   \n",
       "113               entropy                 balanced                   0.0   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "226  {'tree__splitter': 'best', 'tree__random_state...           0.929402   \n",
       "499  {'tree__splitter': 'best', 'tree__random_state...           0.929402   \n",
       "113  {'tree__splitter': 'best', 'tree__random_state...           0.917647   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "226           0.917601           0.976467           0.929402   \n",
       "499           0.917601           0.976467           0.929402   \n",
       "113           0.929324           0.976467           0.929324   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "226           0.976190         0.945813        0.025286                1  \n",
       "499           0.976190         0.945813        0.025286                1  \n",
       "113           0.976177         0.945788        0.025293                3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(x_new, y) # we need this for adaboost\n",
    "\n",
    "print_best_params(search)\n",
    "search_results = pd.DataFrame(search.cv_results_)\n",
    "search_results.sort_values(by='mean_test_score',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8) [Random Forest Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.962     0.943     0.952       212\n",
      "   Malignant      0.944     0.962     0.953       212\n",
      "\n",
      "    accuracy                          0.953       424\n",
      "   macro avg      0.953     0.953     0.953       424\n",
      "weighted avg      0.953     0.953     0.953       424\n",
      "\n",
      "--- Time of execution : 1.2561113834381104 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_rf = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=13))])\n",
    "                       \n",
    "score = cross_val_score(clf_rf, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'rf__bootstrap': [True,False],\n",
    "    'rf__max_depth': [list(range(5,15)), None],\n",
    "    'rf__n_estimators' :[100],\n",
    "    'rf__max_features': [None, 'sqrt', 'log2'],\n",
    "    'rf__max_leaf_nodes' : [None,list(range(5,15))],\n",
    "    'rf__min_samples_leaf': list(range(1,10)),\n",
    "    'rf__min_samples_split': list(range(2, 6)),\n",
    "    'rf__criterion' :['entropy','gini'],\n",
    "    'rf__random_state' : [13]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_rf, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.948     0.943     0.946       212\n",
      "   Malignant      0.944     0.948     0.946       212\n",
      "\n",
      "    accuracy                          0.946       424\n",
      "   macro avg      0.946     0.946     0.946       424\n",
      "weighted avg      0.946     0.946     0.946       424\n",
      "\n",
      "--- Time of execution : 168.3261158466339 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9) [ADA Boost Classifier (Adaptive Boosting)](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#:~:text=An%20AdaBoost%20%5B1%5D%20classifier%20is,focus%20more%20on%20difficult%20cases.>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.951     0.925     0.938       212\n",
      "   Malignant      0.927     0.953     0.940       212\n",
      "\n",
      "    accuracy                          0.939       424\n",
      "   macro avg      0.939     0.939     0.939       424\n",
      "weighted avg      0.939     0.939     0.939       424\n",
      "\n",
      "--- Time of execution : 0.8676400184631348 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_adaboost = Pipeline([('scaler', StandardScaler()), ('adab', AdaBoostClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_adaboost, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.943     0.939     0.941       212\n",
      "   Malignant      0.939     0.943     0.941       212\n",
      "\n",
      "    accuracy                          0.941       424\n",
      "   macro avg      0.941     0.941     0.941       424\n",
      "weighted avg      0.941     0.941     0.941       424\n",
      "\n",
      "--- Time of execution : 148.87837553024292 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'adab__base_estimator' : [DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=3,random_state=13)],\n",
    "    'adab__n_estimators' : np.arange(100,210,10),\n",
    "    'adab__learning_rate' : np.power(10, np.arange(-3, 1, dtype=float)),\n",
    "    'adab__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "    'adab__random_state' : [13],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(clf_adaboost, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10) [C-Support Vector Classification](<https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.940     0.958     0.949       212\n",
      "   Malignant      0.957     0.939     0.948       212\n",
      "\n",
      "    accuracy                          0.948       424\n",
      "   macro avg      0.948     0.948     0.948       424\n",
      "weighted avg      0.948     0.948     0.948       424\n",
      "\n",
      "--- Time of execution : 0.08186149597167969 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_svc = Pipeline([('scaler', StandardScaler()),('svc', SVC())])\n",
    "\n",
    "score = cross_val_score(clf_svc, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.927     0.958     0.942       212\n",
      "   Malignant      0.956     0.925     0.940       212\n",
      "\n",
      "    accuracy                          0.941       424\n",
      "   macro avg      0.942     0.941     0.941       424\n",
      "weighted avg      0.942     0.941     0.941       424\n",
      "\n",
      "--- Time of execution : 5.633396625518799 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = [\n",
    "    {\n",
    "        'svc__kernel': ['rbf'], \n",
    "        'svc__gamma': [1,1e-1,1e-2, 1e-3, 1e-4,'auto','scale'], \n",
    "        'svc__C': [1, 10, 100, 1000],\n",
    "        'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "        'svc__random_state' : [13]\n",
    "    },\n",
    "    {\n",
    "        'svc__kernel': ['linear'], \n",
    "        'svc__C': [1, 10, 100, 1000],\n",
    "        'svc__decision_function_shape': ['ovo', 'ovr'],\n",
    "        'svc__random_state' : [13]\n",
    "    },\n",
    "]\n",
    "\n",
    "search = GridSearchCV(clf_svc, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## 11) [Stochastic Gradient Descent Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.938     0.920     0.929       212\n",
      "   Malignant      0.921     0.939     0.930       212\n",
      "\n",
      "    accuracy                          0.929       424\n",
      "   macro avg      0.929     0.929     0.929       424\n",
      "weighted avg      0.929     0.929     0.929       424\n",
      "\n",
      "--- Time of execution : 0.06582403182983398 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_sgd = Pipeline([('scaler', StandardScaler()), ('sgd', SGDClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.948     0.939     0.943       212\n",
      "   Malignant      0.939     0.948     0.944       212\n",
      "\n",
      "    accuracy                          0.943       424\n",
      "   macro avg      0.943     0.943     0.943       424\n",
      "weighted avg      0.943     0.943     0.943       424\n",
      "\n",
      "--- Time of execution : 3.517984390258789 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'sgd__average': [True, False],\n",
    "    'sgd__l1_ratio': np.linspace(0, 1, num=10),\n",
    "    'sgd__alpha': np.power(10, np.arange(-2, 1, dtype=float)),\n",
    "    'sgd__random_state' : [13]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_sgd, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## 12) [eXtreme Gradient Boosting](<https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.957     0.953     0.955       212\n",
      "   Malignant      0.953     0.958     0.955       212\n",
      "\n",
      "    accuracy                          0.955       424\n",
      "   macro avg      0.955     0.955     0.955       424\n",
      "weighted avg      0.955     0.955     0.955       424\n",
      "\n",
      "--- Time of execution : 0.6044058799743652 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_xgb = Pipeline([('scaler', StandardScaler()), ('xgb', XGBClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_xgb, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018sp/lectures/lecturenote19.html\n",
    "# https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6\n",
    "\n",
    "start = time.time()\n",
    "param_grid = {\n",
    "        'xgb__booster' : ['gbtree'],\n",
    "        'xgb__validate_parameters' : [True],\n",
    "        'xgb__learning_rate' : np.arange(0.01,1.1,0.05),\n",
    "        'xgb__gamma' : np.arange(0,1.05,0.05),\n",
    "        'xgb__max_depth' : np.arange(2,11,1),\n",
    "        'xgb__min_child_weight' : np.arange(1,6,1),\n",
    "        'xgb__max_delta_step' : np.arange(0,5,1),\n",
    "        'xgb__subsample' : [0.5],\n",
    "        'xgb__colsample_bylevel' : [1],\n",
    "        'xgb__colsample_bynode' : [1],\n",
    "        'xgb__colsample_bytree' : [1],\n",
    "        'xgb__reg_lambda' : [0,1],\n",
    "        'xgb__reg_alpha' : [0],\n",
    "        'xgb__tree_method' : ['exact'],\n",
    "        'xgb__scale_pos_weight' : [1],\n",
    "        'xgb__objective' : ['binary:logistic'], # 'multi:softmax' -> same scores as 'binary:logistic' with grid search\n",
    "        #'num_class' : [2],\n",
    "        'xgb__n_estimators' : np.arange(100,210,10),\n",
    "        'xgb__random_state' : [13]\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf_xgb, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.939     0.948     0.944       212\n",
      "   Malignant      0.948     0.939     0.943       212\n",
      "\n",
      "    accuracy                          0.943       424\n",
      "   macro avg      0.943     0.943     0.943       424\n",
      "weighted avg      0.943     0.943     0.943       424\n",
      "\n",
      "--- Time of execution : 240.4299190044403 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## 13) [Light Gradient Boosting Machine](<https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.945     0.967     0.956       212\n",
      "   Malignant      0.966     0.943     0.955       212\n",
      "\n",
      "    accuracy                          0.955       424\n",
      "   macro avg      0.955     0.955     0.955       424\n",
      "weighted avg      0.955     0.955     0.955       424\n",
      "\n",
      "--- Time of execution : 0.40244412422180176 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_lgbm = Pipeline([('scaler', StandardScaler()), ('lgbm', lgbm.LGBMClassifier(random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_lgbm, x_new, y_le, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "# https://www.youtube.com/watch?v=5CWwwtEM2TA&ab_channel=PyData & https://github.com/MSusik/newgradientboosting/blob/master/pydata.pdf\n",
    "\n",
    "start = time.time()\n",
    "param_grid = {\n",
    "        'lgbm__boosting_type' : ['gbdt','dart'],\n",
    "        'lgbm__num_leaves' : np.arange(5,55,1),\n",
    "        'lgbm__max_depth' : np.arange(2,11,1),\n",
    "        'lgbm__learning_rate' : np.arange(0.01,1.1,0.05),\n",
    "        'lgbm__n_estimators' : np.arange(100,210,10),\n",
    "        'lgbm__objective' : ['binary'],\n",
    "        'lgbm__min_child_samples' : np.arange(10,35,5),\n",
    "        'lgbm__subsample' : [0.5],\n",
    "        'lgbm__reg_lambda' : [0,1],\n",
    "        'lgbm__reg_alpha' : [0],\n",
    "        'lgbm__subsample' : [0.5],\n",
    "        'lgbm__colsample_bytree' : [1],\n",
    "        'lgbm__scale_pos_weight' : [1],\n",
    "        'lgbm__random_state' : [13]\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf_lgbm, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=300)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.939     0.948     0.944       212\n",
      "   Malignant      0.948     0.939     0.943       212\n",
      "\n",
      "    accuracy                          0.943       424\n",
      "   macro avg      0.943     0.943     0.943       424\n",
      "weighted avg      0.943     0.943     0.943       424\n",
      "\n",
      "--- Time of execution : 89.87398099899292 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## 14) [K-Nearest Neighbors Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.922     0.953     0.937       212\n",
      "   Malignant      0.951     0.920     0.935       212\n",
      "\n",
      "    accuracy                          0.936       424\n",
      "   macro avg      0.937     0.936     0.936       424\n",
      "weighted avg      0.937     0.936     0.936       424\n",
      "\n",
      "--- Time of execution : 0.09297537803649902 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "score = cross_val_score(clf_knn, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Grid Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.931     0.953     0.942       212\n",
      "   Malignant      0.952     0.929     0.940       212\n",
      "\n",
      "    accuracy                          0.941       424\n",
      "   macro avg      0.941     0.941     0.941       424\n",
      "weighted avg      0.941     0.941     0.941       424\n",
      "\n",
      "--- Time of execution : 82.81771111488342 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': list(range(3,10)),\n",
    "    'knn__weights': ['uniform','distance'],\n",
    "    'knn__algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__leaf_size': [10,20,30,40,50],\n",
    "    'knn__p': [1,2],\n",
    "    'knn__metric': ['minkowski','manhattan','chebyshev']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_knn, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=0,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(grid_search, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='15'></a>\n",
    "## 15) [Multi-layer Perceptron Classifier](<https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.957     0.948     0.953       212\n",
      "   Malignant      0.949     0.958     0.953       212\n",
      "\n",
      "    accuracy                          0.953       424\n",
      "   macro avg      0.953     0.953     0.953       424\n",
      "weighted avg      0.953     0.953     0.953       424\n",
      "\n",
      "--- Time of execution : 3.4507648944854736 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_mlp =  Pipeline([('scaler', StandardScaler()),('mlp', MLPClassifier(shuffle=True,random_state=13))])\n",
    "\n",
    "score = cross_val_score(clf_mlp, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))\n",
    "print(\"--- Time of execution : %s seconds ---\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Νested Cross Validation with Randomized Search for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tried a wider range of hyperparameters in nested cross validation at first , but over testing, worst attempts were removed (those in comments). Finally, when few hyperparameters remained, they were tested separately with a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solver : ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 1.05076302\n",
      "Iteration 2, loss = 1.01209895\n",
      "Iteration 3, loss = 0.97559823\n",
      "Iteration 4, loss = 0.93972659\n",
      "Iteration 5, loss = 0.90595721\n",
      "Iteration 6, loss = 0.87350846\n",
      "Iteration 7, loss = 0.84191166\n",
      "Iteration 8, loss = 0.81245992\n",
      "Iteration 9, loss = 0.78384408\n",
      "Iteration 10, loss = 0.75713442\n",
      "Iteration 11, loss = 0.73171578\n",
      "Iteration 12, loss = 0.70843870\n",
      "Iteration 13, loss = 0.68524010\n",
      "Iteration 14, loss = 0.66363359\n",
      "Iteration 15, loss = 0.64287727\n",
      "Iteration 16, loss = 0.62364296\n",
      "Iteration 17, loss = 0.60494026\n",
      "Iteration 18, loss = 0.58700077\n",
      "Iteration 19, loss = 0.56993349\n",
      "Iteration 20, loss = 0.55385830\n",
      "Iteration 21, loss = 0.53884568\n",
      "Iteration 22, loss = 0.52428039\n",
      "Iteration 23, loss = 0.51127268\n",
      "Iteration 24, loss = 0.49889330\n",
      "Iteration 25, loss = 0.48687519\n",
      "Iteration 26, loss = 0.47587526\n",
      "Iteration 27, loss = 0.46541722\n",
      "Iteration 28, loss = 0.45585337\n",
      "Iteration 29, loss = 0.44656915\n",
      "Iteration 30, loss = 0.43791178\n",
      "Iteration 31, loss = 0.42985022\n",
      "Iteration 32, loss = 0.42216135\n",
      "Iteration 33, loss = 0.41479229\n",
      "Iteration 34, loss = 0.40818360\n",
      "Iteration 35, loss = 0.40137040\n",
      "Iteration 36, loss = 0.39494830\n",
      "Iteration 37, loss = 0.38909068\n",
      "Iteration 38, loss = 0.38344012\n",
      "Iteration 39, loss = 0.37781408\n",
      "Iteration 40, loss = 0.37277960\n",
      "Iteration 41, loss = 0.36781029\n",
      "Iteration 42, loss = 0.36309260\n",
      "Iteration 43, loss = 0.35833729\n",
      "Iteration 44, loss = 0.35409972\n",
      "Iteration 45, loss = 0.34999610\n",
      "Iteration 46, loss = 0.34585920\n",
      "Iteration 47, loss = 0.34207589\n",
      "Iteration 48, loss = 0.33855214\n",
      "Iteration 49, loss = 0.33512766\n",
      "Iteration 50, loss = 0.33170450\n",
      "Iteration 51, loss = 0.32860916\n",
      "Iteration 52, loss = 0.32541534\n",
      "Iteration 53, loss = 0.32238079\n",
      "Iteration 54, loss = 0.31963313\n",
      "Iteration 55, loss = 0.31669174\n",
      "Iteration 56, loss = 0.31415374\n",
      "Iteration 57, loss = 0.31167329\n",
      "Iteration 58, loss = 0.30920840\n",
      "Iteration 59, loss = 0.30675502\n",
      "Iteration 60, loss = 0.30446343\n",
      "Iteration 61, loss = 0.30239652\n",
      "Iteration 62, loss = 0.30036721\n",
      "Iteration 63, loss = 0.29826621\n",
      "Iteration 64, loss = 0.29634053\n",
      "Iteration 65, loss = 0.29444035\n",
      "Iteration 66, loss = 0.29276226\n",
      "Iteration 67, loss = 0.29092555\n",
      "Iteration 68, loss = 0.28920363\n",
      "Iteration 69, loss = 0.28759059\n",
      "Iteration 70, loss = 0.28613312\n",
      "Iteration 71, loss = 0.28451617\n",
      "Iteration 72, loss = 0.28316124\n",
      "Iteration 73, loss = 0.28168041\n",
      "Iteration 74, loss = 0.28023862\n",
      "Iteration 75, loss = 0.27896305\n",
      "Iteration 76, loss = 0.27759437\n",
      "Iteration 77, loss = 0.27635510\n",
      "Iteration 78, loss = 0.27507675\n",
      "Iteration 79, loss = 0.27380124\n",
      "Iteration 80, loss = 0.27258822\n",
      "Iteration 81, loss = 0.27139992\n",
      "Iteration 82, loss = 0.27026767\n",
      "Iteration 83, loss = 0.26911758\n",
      "Iteration 84, loss = 0.26809318\n",
      "Iteration 85, loss = 0.26696229\n",
      "Iteration 86, loss = 0.26593255\n",
      "Iteration 87, loss = 0.26485909\n",
      "Iteration 88, loss = 0.26382270\n",
      "Iteration 89, loss = 0.26286185\n",
      "Iteration 90, loss = 0.26187239\n",
      "Iteration 91, loss = 0.26089437\n",
      "Iteration 92, loss = 0.25994215\n",
      "Iteration 93, loss = 0.25906471\n",
      "Iteration 94, loss = 0.25814308\n",
      "Iteration 95, loss = 0.25722174\n",
      "Iteration 96, loss = 0.25629731\n",
      "Iteration 97, loss = 0.25545536\n",
      "Iteration 98, loss = 0.25458918\n",
      "Iteration 99, loss = 0.25369074\n",
      "Iteration 100, loss = 0.25288873\n",
      "Iteration 101, loss = 0.25207777\n",
      "Iteration 102, loss = 0.25126433\n",
      "Iteration 103, loss = 0.25049477\n",
      "Iteration 104, loss = 0.24974489\n",
      "Iteration 105, loss = 0.24897622\n",
      "Iteration 106, loss = 0.24828607\n",
      "Iteration 107, loss = 0.24757976\n",
      "Iteration 108, loss = 0.24686616\n",
      "Iteration 109, loss = 0.24620742\n",
      "Iteration 110, loss = 0.24551676\n",
      "Iteration 111, loss = 0.24482526\n",
      "Iteration 112, loss = 0.24415785\n",
      "Iteration 113, loss = 0.24352302\n",
      "Iteration 114, loss = 0.24286383\n",
      "Iteration 115, loss = 0.24224003\n",
      "Iteration 116, loss = 0.24158654\n",
      "Iteration 117, loss = 0.24097763\n",
      "Iteration 118, loss = 0.24034041\n",
      "Iteration 119, loss = 0.23973156\n",
      "Iteration 120, loss = 0.23913077\n",
      "Iteration 121, loss = 0.23851259\n",
      "Iteration 122, loss = 0.23790577\n",
      "Iteration 123, loss = 0.23731229\n",
      "Iteration 124, loss = 0.23674263\n",
      "Iteration 125, loss = 0.23612390\n",
      "Iteration 126, loss = 0.23554330\n",
      "Iteration 127, loss = 0.23499445\n",
      "Iteration 128, loss = 0.23443950\n",
      "Iteration 129, loss = 0.23388466\n",
      "Iteration 130, loss = 0.23333302\n",
      "Iteration 131, loss = 0.23282345\n",
      "Iteration 132, loss = 0.23226366\n",
      "Iteration 133, loss = 0.23178132\n",
      "Iteration 134, loss = 0.23121941\n",
      "Iteration 135, loss = 0.23070551\n",
      "Iteration 136, loss = 0.23021308\n",
      "Iteration 137, loss = 0.22972655\n",
      "Iteration 138, loss = 0.22920958\n",
      "Iteration 139, loss = 0.22872981\n",
      "Iteration 140, loss = 0.22824632\n",
      "Iteration 141, loss = 0.22772296\n",
      "Iteration 142, loss = 0.22724347\n",
      "Iteration 143, loss = 0.22680100\n",
      "Iteration 144, loss = 0.22632303\n",
      "Iteration 145, loss = 0.22581271\n",
      "Iteration 146, loss = 0.22539492\n",
      "Iteration 147, loss = 0.22493316\n",
      "Iteration 148, loss = 0.22449192\n",
      "Iteration 149, loss = 0.22404709\n",
      "Iteration 150, loss = 0.22358106\n",
      "Iteration 151, loss = 0.22321891\n",
      "Iteration 152, loss = 0.22274261\n",
      "Iteration 153, loss = 0.22231816\n",
      "Iteration 154, loss = 0.22191990\n",
      "Iteration 155, loss = 0.22152731\n",
      "Iteration 156, loss = 0.22107987\n",
      "Iteration 157, loss = 0.22071453\n",
      "Iteration 158, loss = 0.22032577\n",
      "Iteration 159, loss = 0.21991836\n",
      "Iteration 160, loss = 0.21953981\n",
      "Iteration 161, loss = 0.21915663\n",
      "Iteration 162, loss = 0.21875431\n",
      "Iteration 163, loss = 0.21839491\n",
      "Iteration 164, loss = 0.21803136\n",
      "Iteration 165, loss = 0.21766537\n",
      "Iteration 166, loss = 0.21727428\n",
      "Iteration 167, loss = 0.21691703\n",
      "Iteration 168, loss = 0.21658783\n",
      "Iteration 169, loss = 0.21617949\n",
      "Iteration 170, loss = 0.21585620\n",
      "Iteration 171, loss = 0.21549345\n",
      "Iteration 172, loss = 0.21515659\n",
      "Iteration 173, loss = 0.21482307\n",
      "Iteration 174, loss = 0.21448723\n",
      "Iteration 175, loss = 0.21415934\n",
      "Iteration 176, loss = 0.21385129\n",
      "Iteration 177, loss = 0.21350463\n",
      "Iteration 178, loss = 0.21319351\n",
      "Iteration 179, loss = 0.21287332\n",
      "Iteration 180, loss = 0.21253172\n",
      "Iteration 181, loss = 0.21227453\n",
      "Iteration 182, loss = 0.21188063\n",
      "Iteration 183, loss = 0.21158376\n",
      "Iteration 184, loss = 0.21125319\n",
      "Iteration 185, loss = 0.21094160\n",
      "Iteration 186, loss = 0.21066875\n",
      "Iteration 187, loss = 0.21032607\n",
      "Iteration 188, loss = 0.21001790\n",
      "Iteration 189, loss = 0.20969742\n",
      "Iteration 190, loss = 0.20938632\n",
      "Iteration 191, loss = 0.20907343\n",
      "Iteration 192, loss = 0.20878220\n",
      "Iteration 193, loss = 0.20847987\n",
      "Iteration 194, loss = 0.20817002\n",
      "Iteration 195, loss = 0.20787192\n",
      "Iteration 196, loss = 0.20756091\n",
      "Iteration 197, loss = 0.20727528\n",
      "Iteration 198, loss = 0.20696800\n",
      "Iteration 199, loss = 0.20665814\n",
      "Iteration 200, loss = 0.20642729\n",
      "Iteration 201, loss = 0.20606944\n",
      "Iteration 202, loss = 0.20579911\n",
      "Iteration 203, loss = 0.20550610\n",
      "Iteration 204, loss = 0.20522881\n",
      "Iteration 205, loss = 0.20494395\n",
      "Iteration 206, loss = 0.20466935\n",
      "Iteration 207, loss = 0.20438223\n",
      "Iteration 208, loss = 0.20413504\n",
      "Iteration 209, loss = 0.20384406\n",
      "Iteration 210, loss = 0.20361028\n",
      "Iteration 211, loss = 0.20330778\n",
      "Iteration 212, loss = 0.20308316\n",
      "Iteration 213, loss = 0.20280029\n",
      "Iteration 214, loss = 0.20253543\n",
      "Iteration 215, loss = 0.20228094\n",
      "Iteration 216, loss = 0.20205069\n",
      "Iteration 217, loss = 0.20179720\n",
      "Iteration 218, loss = 0.20156086\n",
      "Iteration 219, loss = 0.20128159\n",
      "Iteration 220, loss = 0.20104973\n",
      "Iteration 221, loss = 0.20081115\n",
      "Iteration 222, loss = 0.20056866\n",
      "Iteration 223, loss = 0.20033039\n",
      "Iteration 224, loss = 0.20008536\n",
      "Iteration 225, loss = 0.19985113\n",
      "Iteration 226, loss = 0.19963366\n",
      "Iteration 227, loss = 0.19936555\n",
      "Iteration 228, loss = 0.19914340\n",
      "Iteration 229, loss = 0.19895854\n",
      "Iteration 230, loss = 0.19870134\n",
      "Iteration 231, loss = 0.19845447\n",
      "Iteration 232, loss = 0.19823958\n",
      "Iteration 233, loss = 0.19799719\n",
      "Iteration 234, loss = 0.19777407\n",
      "Iteration 235, loss = 0.19757265\n",
      "Iteration 236, loss = 0.19733178\n",
      "Iteration 237, loss = 0.19716756\n",
      "Iteration 238, loss = 0.19691774\n",
      "Iteration 239, loss = 0.19670867\n",
      "Iteration 240, loss = 0.19648659\n",
      "Iteration 241, loss = 0.19627264\n",
      "Iteration 242, loss = 0.19609495\n",
      "Iteration 243, loss = 0.19584554\n",
      "Iteration 244, loss = 0.19564512\n",
      "Iteration 245, loss = 0.19542019\n",
      "Iteration 246, loss = 0.19521235\n",
      "Iteration 247, loss = 0.19501845\n",
      "Iteration 248, loss = 0.19479700\n",
      "Iteration 249, loss = 0.19460078\n",
      "Iteration 250, loss = 0.19440173\n",
      "Iteration 251, loss = 0.19419877\n",
      "Iteration 252, loss = 0.19403388\n",
      "Iteration 253, loss = 0.19382155\n",
      "Iteration 254, loss = 0.19362669\n",
      "Iteration 255, loss = 0.19340889\n",
      "Iteration 256, loss = 0.19321297\n",
      "Iteration 257, loss = 0.19301488\n",
      "Iteration 258, loss = 0.19281127\n",
      "Iteration 259, loss = 0.19261373\n",
      "Iteration 260, loss = 0.19245478\n",
      "Iteration 261, loss = 0.19221886\n",
      "Iteration 262, loss = 0.19204601\n",
      "Iteration 263, loss = 0.19188004\n",
      "Iteration 264, loss = 0.19165653\n",
      "Iteration 265, loss = 0.19151191\n",
      "Iteration 266, loss = 0.19129142\n",
      "Iteration 267, loss = 0.19115669\n",
      "Iteration 268, loss = 0.19094871\n",
      "Iteration 269, loss = 0.19077042\n",
      "Iteration 270, loss = 0.19060841\n",
      "Iteration 271, loss = 0.19041432\n",
      "Iteration 272, loss = 0.19026178\n",
      "Iteration 273, loss = 0.19008077\n",
      "Iteration 274, loss = 0.18995195\n",
      "Iteration 275, loss = 0.18974459\n",
      "Iteration 276, loss = 0.18960677\n",
      "Iteration 277, loss = 0.18941225\n",
      "Iteration 278, loss = 0.18925905\n",
      "Iteration 279, loss = 0.18911686\n",
      "Iteration 280, loss = 0.18895158\n",
      "Iteration 281, loss = 0.18877993\n",
      "Iteration 282, loss = 0.18863964\n",
      "Iteration 283, loss = 0.18848141\n",
      "Iteration 284, loss = 0.18830055\n",
      "Iteration 285, loss = 0.18816041\n",
      "Iteration 286, loss = 0.18798579\n",
      "Iteration 287, loss = 0.18784504\n",
      "Iteration 288, loss = 0.18769682\n",
      "Iteration 289, loss = 0.18754049\n",
      "Iteration 290, loss = 0.18738070\n",
      "Iteration 291, loss = 0.18722037\n",
      "Iteration 292, loss = 0.18711717\n",
      "Iteration 293, loss = 0.18701587\n",
      "Iteration 294, loss = 0.18678202\n",
      "Iteration 295, loss = 0.18661362\n",
      "Iteration 296, loss = 0.18647930\n",
      "Iteration 297, loss = 0.18632568\n",
      "Iteration 298, loss = 0.18616941\n",
      "Iteration 299, loss = 0.18605093\n",
      "Iteration 300, loss = 0.18588306\n",
      "Iteration 301, loss = 0.18573495\n",
      "Iteration 302, loss = 0.18558671\n",
      "Iteration 303, loss = 0.18542208\n",
      "Iteration 304, loss = 0.18530240\n",
      "Iteration 305, loss = 0.18514700\n",
      "Iteration 306, loss = 0.18498908\n",
      "Iteration 307, loss = 0.18486280\n",
      "Iteration 308, loss = 0.18471534\n",
      "Iteration 309, loss = 0.18460110\n",
      "Iteration 310, loss = 0.18444292\n",
      "Iteration 311, loss = 0.18430507\n",
      "Iteration 312, loss = 0.18419575\n",
      "Iteration 313, loss = 0.18406794\n",
      "Iteration 314, loss = 0.18391361\n",
      "Iteration 315, loss = 0.18383025\n",
      "Iteration 316, loss = 0.18365172\n",
      "Iteration 317, loss = 0.18351357\n",
      "Iteration 318, loss = 0.18343760\n",
      "Iteration 319, loss = 0.18325957\n",
      "Iteration 320, loss = 0.18313223\n",
      "Iteration 321, loss = 0.18300764\n",
      "Iteration 322, loss = 0.18286739\n",
      "Iteration 323, loss = 0.18275844\n",
      "Iteration 324, loss = 0.18262140\n",
      "Iteration 325, loss = 0.18248157\n",
      "Iteration 326, loss = 0.18237276\n",
      "Iteration 327, loss = 0.18227017\n",
      "Iteration 328, loss = 0.18215562\n",
      "Iteration 329, loss = 0.18198952\n",
      "Iteration 330, loss = 0.18188390\n",
      "Iteration 331, loss = 0.18175348\n",
      "Iteration 332, loss = 0.18164091\n",
      "Iteration 333, loss = 0.18152997\n",
      "Iteration 334, loss = 0.18140242\n",
      "Iteration 335, loss = 0.18127651\n",
      "Iteration 336, loss = 0.18115483\n",
      "Iteration 337, loss = 0.18102167\n",
      "Iteration 338, loss = 0.18092835\n",
      "Iteration 339, loss = 0.18081132\n",
      "Iteration 340, loss = 0.18070473\n",
      "Iteration 341, loss = 0.18057838\n",
      "Iteration 342, loss = 0.18048167\n",
      "Iteration 343, loss = 0.18034299\n",
      "Iteration 344, loss = 0.18023466\n",
      "Iteration 345, loss = 0.18014620\n",
      "Iteration 346, loss = 0.18004136\n",
      "Iteration 347, loss = 0.17991836\n",
      "Iteration 348, loss = 0.17983756\n",
      "Iteration 349, loss = 0.17970451\n",
      "Iteration 350, loss = 0.17958682\n",
      "Iteration 351, loss = 0.17948182\n",
      "Iteration 352, loss = 0.17938393\n",
      "Iteration 353, loss = 0.17932071\n",
      "Iteration 354, loss = 0.17918899\n",
      "Iteration 355, loss = 0.17910808\n",
      "Iteration 356, loss = 0.17900171\n",
      "Iteration 357, loss = 0.17890559\n",
      "Iteration 358, loss = 0.17880706\n",
      "Iteration 359, loss = 0.17870153\n",
      "Iteration 360, loss = 0.17857141\n",
      "Iteration 361, loss = 0.17846398\n",
      "Iteration 362, loss = 0.17836122\n",
      "Iteration 363, loss = 0.17825257\n",
      "Iteration 364, loss = 0.17821410\n",
      "Iteration 365, loss = 0.17804249\n",
      "Iteration 366, loss = 0.17793963\n",
      "Iteration 367, loss = 0.17782129\n",
      "Iteration 368, loss = 0.17772899\n",
      "Iteration 369, loss = 0.17763190\n",
      "Iteration 370, loss = 0.17752197\n",
      "Iteration 371, loss = 0.17743394\n",
      "Iteration 372, loss = 0.17731874\n",
      "Iteration 373, loss = 0.17720755\n",
      "Iteration 374, loss = 0.17712390\n",
      "Iteration 375, loss = 0.17706325\n",
      "Iteration 376, loss = 0.17693354\n",
      "Iteration 377, loss = 0.17682949\n",
      "Iteration 378, loss = 0.17669092\n",
      "Iteration 379, loss = 0.17658458\n",
      "Iteration 380, loss = 0.17647992\n",
      "Iteration 381, loss = 0.17641023\n",
      "Iteration 382, loss = 0.17630398\n",
      "Iteration 383, loss = 0.17619006\n",
      "Iteration 384, loss = 0.17612089\n",
      "Iteration 385, loss = 0.17599874\n",
      "Iteration 386, loss = 0.17586236\n",
      "Iteration 387, loss = 0.17577952\n",
      "Iteration 388, loss = 0.17567755\n",
      "Iteration 389, loss = 0.17559072\n",
      "Iteration 390, loss = 0.17549513\n",
      "Iteration 391, loss = 0.17550018\n",
      "Iteration 392, loss = 0.17531678\n",
      "Iteration 393, loss = 0.17518110\n",
      "Iteration 394, loss = 0.17511855\n",
      "Iteration 395, loss = 0.17501326\n",
      "Iteration 396, loss = 0.17498311\n",
      "Iteration 397, loss = 0.17490013\n",
      "Iteration 398, loss = 0.17473338\n",
      "Iteration 399, loss = 0.17464053\n",
      "Iteration 400, loss = 0.17455581\n",
      "Iteration 401, loss = 0.17450236\n",
      "Iteration 402, loss = 0.17439262\n",
      "Iteration 403, loss = 0.17428644\n",
      "Iteration 404, loss = 0.17422233\n",
      "Iteration 405, loss = 0.17413255\n",
      "Iteration 406, loss = 0.17404810\n",
      "Iteration 407, loss = 0.17397329\n",
      "Iteration 408, loss = 0.17388836\n",
      "Iteration 409, loss = 0.17378928\n",
      "Iteration 410, loss = 0.17373981\n",
      "Iteration 411, loss = 0.17363811\n",
      "Iteration 412, loss = 0.17358891\n",
      "Iteration 413, loss = 0.17348777\n",
      "Iteration 414, loss = 0.17340939\n",
      "Iteration 415, loss = 0.17333188\n",
      "Iteration 416, loss = 0.17326616\n",
      "Iteration 417, loss = 0.17320209\n",
      "Iteration 418, loss = 0.17309475\n",
      "Iteration 419, loss = 0.17301302\n",
      "Iteration 420, loss = 0.17296910\n",
      "Iteration 421, loss = 0.17286636\n",
      "Iteration 422, loss = 0.17278754\n",
      "Iteration 423, loss = 0.17270949\n",
      "Iteration 424, loss = 0.17267239\n",
      "Iteration 425, loss = 0.17260281\n",
      "Iteration 426, loss = 0.17249279\n",
      "Iteration 427, loss = 0.17241378\n",
      "Iteration 428, loss = 0.17236805\n",
      "Iteration 429, loss = 0.17229249\n",
      "Iteration 430, loss = 0.17222612\n",
      "Iteration 431, loss = 0.17214218\n",
      "Iteration 432, loss = 0.17210334\n",
      "Iteration 433, loss = 0.17199616\n",
      "Iteration 434, loss = 0.17194406\n",
      "Iteration 435, loss = 0.17185052\n",
      "Iteration 436, loss = 0.17176217\n",
      "Iteration 437, loss = 0.17167354\n",
      "Iteration 438, loss = 0.17163692\n",
      "Iteration 439, loss = 0.17152736\n",
      "Iteration 440, loss = 0.17144494\n",
      "Iteration 441, loss = 0.17139226\n",
      "Iteration 442, loss = 0.17128755\n",
      "Iteration 443, loss = 0.17124586\n",
      "Iteration 444, loss = 0.17115019\n",
      "Iteration 445, loss = 0.17109347\n",
      "Iteration 446, loss = 0.17101537\n",
      "Iteration 447, loss = 0.17096378\n",
      "Iteration 448, loss = 0.17086749\n",
      "Iteration 449, loss = 0.17081005\n",
      "Iteration 450, loss = 0.17073219\n",
      "Iteration 451, loss = 0.17073003\n",
      "Iteration 452, loss = 0.17060465\n",
      "Iteration 453, loss = 0.17052124\n",
      "Iteration 454, loss = 0.17055328\n",
      "Iteration 455, loss = 0.17039704\n",
      "Iteration 456, loss = 0.17034006\n",
      "Iteration 457, loss = 0.17027508\n",
      "Iteration 458, loss = 0.17019954\n",
      "Iteration 459, loss = 0.17012975\n",
      "Iteration 460, loss = 0.17006535\n",
      "Iteration 461, loss = 0.17007752\n",
      "Iteration 462, loss = 0.16995280\n",
      "Iteration 463, loss = 0.16993786\n",
      "Iteration 464, loss = 0.16984847\n",
      "Iteration 465, loss = 0.16978413\n",
      "Iteration 466, loss = 0.16972308\n",
      "Iteration 467, loss = 0.16965239\n",
      "Iteration 468, loss = 0.16959647\n",
      "Iteration 469, loss = 0.16957422\n",
      "Iteration 470, loss = 0.16950392\n",
      "Iteration 471, loss = 0.16944796\n",
      "Iteration 472, loss = 0.16937012\n",
      "Iteration 473, loss = 0.16931117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.55599885\n",
      "Iteration 2, loss = 0.29475830\n",
      "Iteration 3, loss = 0.27833651\n",
      "Iteration 4, loss = 0.25651647\n",
      "Iteration 5, loss = 0.22728584\n",
      "Iteration 6, loss = 0.20517373\n",
      "Iteration 7, loss = 0.18578033\n",
      "Iteration 8, loss = 0.17934987\n",
      "Iteration 9, loss = 0.17421723\n",
      "Iteration 10, loss = 0.16484270\n",
      "Iteration 11, loss = 0.16601364\n",
      "Iteration 12, loss = 0.15960158\n",
      "Iteration 13, loss = 0.16008588\n",
      "Iteration 14, loss = 0.15763594\n",
      "Iteration 15, loss = 0.15703578\n",
      "Iteration 16, loss = 0.16010052\n",
      "Iteration 17, loss = 0.15885665\n",
      "Iteration 18, loss = 0.16168221\n",
      "Iteration 19, loss = 0.15745913\n",
      "Iteration 20, loss = 0.15659507\n",
      "Iteration 21, loss = 0.15590259\n",
      "Iteration 22, loss = 0.15686590\n",
      "Iteration 23, loss = 0.15540954\n",
      "Iteration 24, loss = 0.15583298\n",
      "Iteration 25, loss = 0.15530172\n",
      "Iteration 26, loss = 0.15625861\n",
      "Iteration 27, loss = 0.15385946\n",
      "Iteration 28, loss = 0.16211196\n",
      "Iteration 29, loss = 0.15393627\n",
      "Iteration 30, loss = 0.16412850\n",
      "Iteration 31, loss = 0.15920153\n",
      "Iteration 32, loss = 0.15647087\n",
      "Iteration 33, loss = 0.15634315\n",
      "Iteration 34, loss = 0.15953178\n",
      "Iteration 35, loss = 0.15874095\n",
      "Iteration 36, loss = 0.15317791\n",
      "Iteration 37, loss = 0.15758313\n",
      "Iteration 38, loss = 0.15354948\n",
      "Iteration 39, loss = 0.16128889\n",
      "Iteration 40, loss = 0.16190198\n",
      "Iteration 41, loss = 0.15394415\n",
      "Iteration 42, loss = 0.16026752\n",
      "Iteration 43, loss = 0.16661512\n",
      "Iteration 44, loss = 0.15814332\n",
      "Iteration 45, loss = 0.16151969\n",
      "Iteration 46, loss = 0.15944907\n",
      "Iteration 47, loss = 0.15901564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 1.05242629\n",
      "Iteration 2, loss = 1.01441360\n",
      "Iteration 3, loss = 0.97735792\n",
      "Iteration 4, loss = 0.94213326\n",
      "Iteration 5, loss = 0.90804383\n",
      "Iteration 6, loss = 0.87565401\n",
      "Iteration 7, loss = 0.84400781\n",
      "Iteration 8, loss = 0.81493756\n",
      "Iteration 9, loss = 0.78643569\n",
      "Iteration 10, loss = 0.76015473\n",
      "Iteration 11, loss = 0.73469333\n",
      "Iteration 12, loss = 0.71122547\n",
      "Iteration 13, loss = 0.68794864\n",
      "Iteration 14, loss = 0.66676079\n",
      "Iteration 15, loss = 0.64663448\n",
      "Iteration 16, loss = 0.62770330\n",
      "Iteration 17, loss = 0.60924923\n",
      "Iteration 18, loss = 0.59153930\n",
      "Iteration 19, loss = 0.57480171\n",
      "Iteration 20, loss = 0.55868281\n",
      "Iteration 21, loss = 0.54385720\n",
      "Iteration 22, loss = 0.52934751\n",
      "Iteration 23, loss = 0.51616831\n",
      "Iteration 24, loss = 0.50332282\n",
      "Iteration 25, loss = 0.49125601\n",
      "Iteration 26, loss = 0.48010532\n",
      "Iteration 27, loss = 0.46931826\n",
      "Iteration 28, loss = 0.45973240\n",
      "Iteration 29, loss = 0.45028373\n",
      "Iteration 30, loss = 0.44125381\n",
      "Iteration 31, loss = 0.43285091\n",
      "Iteration 32, loss = 0.42511255\n",
      "Iteration 33, loss = 0.41765881\n",
      "Iteration 34, loss = 0.41084041\n",
      "Iteration 35, loss = 0.40382075\n",
      "Iteration 36, loss = 0.39732314\n",
      "Iteration 37, loss = 0.39120427\n",
      "Iteration 38, loss = 0.38552802\n",
      "Iteration 39, loss = 0.37951740\n",
      "Iteration 40, loss = 0.37432287\n",
      "Iteration 41, loss = 0.36903167\n",
      "Iteration 42, loss = 0.36424188\n",
      "Iteration 43, loss = 0.35933895\n",
      "Iteration 44, loss = 0.35490459\n",
      "Iteration 45, loss = 0.35042165\n",
      "Iteration 46, loss = 0.34615621\n",
      "Iteration 47, loss = 0.34216138\n",
      "Iteration 48, loss = 0.33833314\n",
      "Iteration 49, loss = 0.33464311\n",
      "Iteration 50, loss = 0.33102248\n",
      "Iteration 51, loss = 0.32758336\n",
      "Iteration 52, loss = 0.32422322\n",
      "Iteration 53, loss = 0.32105285\n",
      "Iteration 54, loss = 0.31801680\n",
      "Iteration 55, loss = 0.31479842\n",
      "Iteration 56, loss = 0.31215102\n",
      "Iteration 57, loss = 0.30941300\n",
      "Iteration 58, loss = 0.30695731\n",
      "Iteration 59, loss = 0.30451002\n",
      "Iteration 60, loss = 0.30210582\n",
      "Iteration 61, loss = 0.29986531\n",
      "Iteration 62, loss = 0.29777042\n",
      "Iteration 63, loss = 0.29571463\n",
      "Iteration 64, loss = 0.29379605\n",
      "Iteration 65, loss = 0.29191649\n",
      "Iteration 66, loss = 0.29020232\n",
      "Iteration 67, loss = 0.28848072\n",
      "Iteration 68, loss = 0.28675741\n",
      "Iteration 69, loss = 0.28523948\n",
      "Iteration 70, loss = 0.28378068\n",
      "Iteration 71, loss = 0.28227509\n",
      "Iteration 72, loss = 0.28091343\n",
      "Iteration 73, loss = 0.27950281\n",
      "Iteration 74, loss = 0.27815627\n",
      "Iteration 75, loss = 0.27694546\n",
      "Iteration 76, loss = 0.27567430\n",
      "Iteration 77, loss = 0.27438436\n",
      "Iteration 78, loss = 0.27322348\n",
      "Iteration 79, loss = 0.27201178\n",
      "Iteration 80, loss = 0.27091753\n",
      "Iteration 81, loss = 0.26984264\n",
      "Iteration 82, loss = 0.26874044\n",
      "Iteration 83, loss = 0.26763869\n",
      "Iteration 84, loss = 0.26669159\n",
      "Iteration 85, loss = 0.26572038\n",
      "Iteration 86, loss = 0.26472870\n",
      "Iteration 87, loss = 0.26376786\n",
      "Iteration 88, loss = 0.26279516\n",
      "Iteration 89, loss = 0.26189771\n",
      "Iteration 90, loss = 0.26101893\n",
      "Iteration 91, loss = 0.26005475\n",
      "Iteration 92, loss = 0.25922001\n",
      "Iteration 93, loss = 0.25838329\n",
      "Iteration 94, loss = 0.25754045\n",
      "Iteration 95, loss = 0.25666489\n",
      "Iteration 96, loss = 0.25585549\n",
      "Iteration 97, loss = 0.25506440\n",
      "Iteration 98, loss = 0.25428067\n",
      "Iteration 99, loss = 0.25351079\n",
      "Iteration 100, loss = 0.25273741\n",
      "Iteration 101, loss = 0.25207013\n",
      "Iteration 102, loss = 0.25132707\n",
      "Iteration 103, loss = 0.25064444\n",
      "Iteration 104, loss = 0.24993753\n",
      "Iteration 105, loss = 0.24924426\n",
      "Iteration 106, loss = 0.24858531\n",
      "Iteration 107, loss = 0.24789555\n",
      "Iteration 108, loss = 0.24725909\n",
      "Iteration 109, loss = 0.24655588\n",
      "Iteration 110, loss = 0.24593068\n",
      "Iteration 111, loss = 0.24526667\n",
      "Iteration 112, loss = 0.24466568\n",
      "Iteration 113, loss = 0.24402920\n",
      "Iteration 114, loss = 0.24340344\n",
      "Iteration 115, loss = 0.24280782\n",
      "Iteration 116, loss = 0.24218407\n",
      "Iteration 117, loss = 0.24157327\n",
      "Iteration 118, loss = 0.24101135\n",
      "Iteration 119, loss = 0.24039189\n",
      "Iteration 120, loss = 0.23984978\n",
      "Iteration 121, loss = 0.23922909\n",
      "Iteration 122, loss = 0.23866722\n",
      "Iteration 123, loss = 0.23808527\n",
      "Iteration 124, loss = 0.23756652\n",
      "Iteration 125, loss = 0.23693689\n",
      "Iteration 126, loss = 0.23641167\n",
      "Iteration 127, loss = 0.23584977\n",
      "Iteration 128, loss = 0.23530488\n",
      "Iteration 129, loss = 0.23477014\n",
      "Iteration 130, loss = 0.23422746\n",
      "Iteration 131, loss = 0.23369781\n",
      "Iteration 132, loss = 0.23318478\n",
      "Iteration 133, loss = 0.23269625\n",
      "Iteration 134, loss = 0.23216181\n",
      "Iteration 135, loss = 0.23166581\n",
      "Iteration 136, loss = 0.23120065\n",
      "Iteration 137, loss = 0.23075374\n",
      "Iteration 138, loss = 0.23024462\n",
      "Iteration 139, loss = 0.22977467\n",
      "Iteration 140, loss = 0.22932580\n",
      "Iteration 141, loss = 0.22884444\n",
      "Iteration 142, loss = 0.22841323\n",
      "Iteration 143, loss = 0.22792153\n",
      "Iteration 144, loss = 0.22748865\n",
      "Iteration 145, loss = 0.22705656\n",
      "Iteration 146, loss = 0.22666013\n",
      "Iteration 147, loss = 0.22619282\n",
      "Iteration 148, loss = 0.22577315\n",
      "Iteration 149, loss = 0.22536459\n",
      "Iteration 150, loss = 0.22493137\n",
      "Iteration 151, loss = 0.22451969\n",
      "Iteration 152, loss = 0.22412398\n",
      "Iteration 153, loss = 0.22372067\n",
      "Iteration 154, loss = 0.22329335\n",
      "Iteration 155, loss = 0.22292490\n",
      "Iteration 156, loss = 0.22251732\n",
      "Iteration 157, loss = 0.22213220\n",
      "Iteration 158, loss = 0.22172648\n",
      "Iteration 159, loss = 0.22136991\n",
      "Iteration 160, loss = 0.22098940\n",
      "Iteration 161, loss = 0.22059617\n",
      "Iteration 162, loss = 0.22026589\n",
      "Iteration 163, loss = 0.21988148\n",
      "Iteration 164, loss = 0.21951661\n",
      "Iteration 165, loss = 0.21918109\n",
      "Iteration 166, loss = 0.21884443\n",
      "Iteration 167, loss = 0.21848251\n",
      "Iteration 168, loss = 0.21815569\n",
      "Iteration 169, loss = 0.21781177\n",
      "Iteration 170, loss = 0.21750473\n",
      "Iteration 171, loss = 0.21719615\n",
      "Iteration 172, loss = 0.21683844\n",
      "Iteration 173, loss = 0.21652717\n",
      "Iteration 174, loss = 0.21621341\n",
      "Iteration 175, loss = 0.21589074\n",
      "Iteration 176, loss = 0.21558827\n",
      "Iteration 177, loss = 0.21529721\n",
      "Iteration 178, loss = 0.21497133\n",
      "Iteration 179, loss = 0.21468307\n",
      "Iteration 180, loss = 0.21438316\n",
      "Iteration 181, loss = 0.21406641\n",
      "Iteration 182, loss = 0.21375009\n",
      "Iteration 183, loss = 0.21345783\n",
      "Iteration 184, loss = 0.21315418\n",
      "Iteration 185, loss = 0.21284241\n",
      "Iteration 186, loss = 0.21256499\n",
      "Iteration 187, loss = 0.21226146\n",
      "Iteration 188, loss = 0.21201349\n",
      "Iteration 189, loss = 0.21170976\n",
      "Iteration 190, loss = 0.21141215\n",
      "Iteration 191, loss = 0.21111800\n",
      "Iteration 192, loss = 0.21083764\n",
      "Iteration 193, loss = 0.21055850\n",
      "Iteration 194, loss = 0.21027506\n",
      "Iteration 195, loss = 0.20999847\n",
      "Iteration 196, loss = 0.20972647\n",
      "Iteration 197, loss = 0.20946790\n",
      "Iteration 198, loss = 0.20918659\n",
      "Iteration 199, loss = 0.20891992\n",
      "Iteration 200, loss = 0.20871162\n",
      "Iteration 201, loss = 0.20840873\n",
      "Iteration 202, loss = 0.20816178\n",
      "Iteration 203, loss = 0.20788863\n",
      "Iteration 204, loss = 0.20762921\n",
      "Iteration 205, loss = 0.20740202\n",
      "Iteration 206, loss = 0.20714733\n",
      "Iteration 207, loss = 0.20687411\n",
      "Iteration 208, loss = 0.20663857\n",
      "Iteration 209, loss = 0.20638823\n",
      "Iteration 210, loss = 0.20614807\n",
      "Iteration 211, loss = 0.20587530\n",
      "Iteration 212, loss = 0.20565739\n",
      "Iteration 213, loss = 0.20539738\n",
      "Iteration 214, loss = 0.20517840\n",
      "Iteration 215, loss = 0.20491674\n",
      "Iteration 216, loss = 0.20465297\n",
      "Iteration 217, loss = 0.20443076\n",
      "Iteration 218, loss = 0.20421383\n",
      "Iteration 219, loss = 0.20398229\n",
      "Iteration 220, loss = 0.20374184\n",
      "Iteration 221, loss = 0.20351412\n",
      "Iteration 222, loss = 0.20331417\n",
      "Iteration 223, loss = 0.20307463\n",
      "Iteration 224, loss = 0.20286554\n",
      "Iteration 225, loss = 0.20263965\n",
      "Iteration 226, loss = 0.20245309\n",
      "Iteration 227, loss = 0.20221176\n",
      "Iteration 228, loss = 0.20199775\n",
      "Iteration 229, loss = 0.20179460\n",
      "Iteration 230, loss = 0.20158886\n",
      "Iteration 231, loss = 0.20137628\n",
      "Iteration 232, loss = 0.20116693\n",
      "Iteration 233, loss = 0.20095336\n",
      "Iteration 234, loss = 0.20075295\n",
      "Iteration 235, loss = 0.20055085\n",
      "Iteration 236, loss = 0.20035576\n",
      "Iteration 237, loss = 0.20015652\n",
      "Iteration 238, loss = 0.19995257\n",
      "Iteration 239, loss = 0.19976288\n",
      "Iteration 240, loss = 0.19957722\n",
      "Iteration 241, loss = 0.19938208\n",
      "Iteration 242, loss = 0.19920745\n",
      "Iteration 243, loss = 0.19902359\n",
      "Iteration 244, loss = 0.19881367\n",
      "Iteration 245, loss = 0.19865161\n",
      "Iteration 246, loss = 0.19845561\n",
      "Iteration 247, loss = 0.19827724\n",
      "Iteration 248, loss = 0.19809451\n",
      "Iteration 249, loss = 0.19795846\n",
      "Iteration 250, loss = 0.19774494\n",
      "Iteration 251, loss = 0.19758272\n",
      "Iteration 252, loss = 0.19739434\n",
      "Iteration 253, loss = 0.19722273\n",
      "Iteration 254, loss = 0.19709933\n",
      "Iteration 255, loss = 0.19691173\n",
      "Iteration 256, loss = 0.19671689\n",
      "Iteration 257, loss = 0.19652957\n",
      "Iteration 258, loss = 0.19637092\n",
      "Iteration 259, loss = 0.19619363\n",
      "Iteration 260, loss = 0.19604422\n",
      "Iteration 261, loss = 0.19585576\n",
      "Iteration 262, loss = 0.19569038\n",
      "Iteration 263, loss = 0.19553113\n",
      "Iteration 264, loss = 0.19536068\n",
      "Iteration 265, loss = 0.19518811\n",
      "Iteration 266, loss = 0.19502356\n",
      "Iteration 267, loss = 0.19485806\n",
      "Iteration 268, loss = 0.19469780\n",
      "Iteration 269, loss = 0.19455689\n",
      "Iteration 270, loss = 0.19438352\n",
      "Iteration 271, loss = 0.19423466\n",
      "Iteration 272, loss = 0.19403970\n",
      "Iteration 273, loss = 0.19385482\n",
      "Iteration 274, loss = 0.19371842\n",
      "Iteration 275, loss = 0.19353981\n",
      "Iteration 276, loss = 0.19333818\n",
      "Iteration 277, loss = 0.19318360\n",
      "Iteration 278, loss = 0.19300881\n",
      "Iteration 279, loss = 0.19287128\n",
      "Iteration 280, loss = 0.19269987\n",
      "Iteration 281, loss = 0.19253343\n",
      "Iteration 282, loss = 0.19238406\n",
      "Iteration 283, loss = 0.19221653\n",
      "Iteration 284, loss = 0.19204602\n",
      "Iteration 285, loss = 0.19187701\n",
      "Iteration 286, loss = 0.19173148\n",
      "Iteration 287, loss = 0.19158925\n",
      "Iteration 288, loss = 0.19144553\n",
      "Iteration 289, loss = 0.19125693\n",
      "Iteration 290, loss = 0.19115554\n",
      "Iteration 291, loss = 0.19097793\n",
      "Iteration 292, loss = 0.19082208\n",
      "Iteration 293, loss = 0.19071385\n",
      "Iteration 294, loss = 0.19055979\n",
      "Iteration 295, loss = 0.19039415\n",
      "Iteration 296, loss = 0.19027639\n",
      "Iteration 297, loss = 0.19015496\n",
      "Iteration 298, loss = 0.18997545\n",
      "Iteration 299, loss = 0.18983088\n",
      "Iteration 300, loss = 0.18971280\n",
      "Iteration 301, loss = 0.18957036\n",
      "Iteration 302, loss = 0.18943222\n",
      "Iteration 303, loss = 0.18929362\n",
      "Iteration 304, loss = 0.18916230\n",
      "Iteration 305, loss = 0.18903723\n",
      "Iteration 306, loss = 0.18888714\n",
      "Iteration 307, loss = 0.18882023\n",
      "Iteration 308, loss = 0.18863939\n",
      "Iteration 309, loss = 0.18849018\n",
      "Iteration 310, loss = 0.18835775\n",
      "Iteration 311, loss = 0.18821564\n",
      "Iteration 312, loss = 0.18810417\n",
      "Iteration 313, loss = 0.18798002\n",
      "Iteration 314, loss = 0.18784581\n",
      "Iteration 315, loss = 0.18775024\n",
      "Iteration 316, loss = 0.18758959\n",
      "Iteration 317, loss = 0.18744619\n",
      "Iteration 318, loss = 0.18736077\n",
      "Iteration 319, loss = 0.18718606\n",
      "Iteration 320, loss = 0.18706688\n",
      "Iteration 321, loss = 0.18697203\n",
      "Iteration 322, loss = 0.18685500\n",
      "Iteration 323, loss = 0.18668692\n",
      "Iteration 324, loss = 0.18657938\n",
      "Iteration 325, loss = 0.18642636\n",
      "Iteration 326, loss = 0.18630052\n",
      "Iteration 327, loss = 0.18620519\n",
      "Iteration 328, loss = 0.18606575\n",
      "Iteration 329, loss = 0.18594792\n",
      "Iteration 330, loss = 0.18581325\n",
      "Iteration 331, loss = 0.18573409\n",
      "Iteration 332, loss = 0.18555424\n",
      "Iteration 333, loss = 0.18544519\n",
      "Iteration 334, loss = 0.18532164\n",
      "Iteration 335, loss = 0.18520782\n",
      "Iteration 336, loss = 0.18509548\n",
      "Iteration 337, loss = 0.18497457\n",
      "Iteration 338, loss = 0.18487581\n",
      "Iteration 339, loss = 0.18474997\n",
      "Iteration 340, loss = 0.18464232\n",
      "Iteration 341, loss = 0.18451127\n",
      "Iteration 342, loss = 0.18439356\n",
      "Iteration 343, loss = 0.18432443\n",
      "Iteration 344, loss = 0.18417138\n",
      "Iteration 345, loss = 0.18403890\n",
      "Iteration 346, loss = 0.18393711\n",
      "Iteration 347, loss = 0.18381822\n",
      "Iteration 348, loss = 0.18372571\n",
      "Iteration 349, loss = 0.18360418\n",
      "Iteration 350, loss = 0.18347026\n",
      "Iteration 351, loss = 0.18335353\n",
      "Iteration 352, loss = 0.18324110\n",
      "Iteration 353, loss = 0.18314671\n",
      "Iteration 354, loss = 0.18309537\n",
      "Iteration 355, loss = 0.18293794\n",
      "Iteration 356, loss = 0.18282385\n",
      "Iteration 357, loss = 0.18270085\n",
      "Iteration 358, loss = 0.18262833\n",
      "Iteration 359, loss = 0.18249153\n",
      "Iteration 360, loss = 0.18238400\n",
      "Iteration 361, loss = 0.18227272\n",
      "Iteration 362, loss = 0.18216601\n",
      "Iteration 363, loss = 0.18205543\n",
      "Iteration 364, loss = 0.18197937\n",
      "Iteration 365, loss = 0.18187257\n",
      "Iteration 366, loss = 0.18177140\n",
      "Iteration 367, loss = 0.18164378\n",
      "Iteration 368, loss = 0.18154878\n",
      "Iteration 369, loss = 0.18148530\n",
      "Iteration 370, loss = 0.18135275\n",
      "Iteration 371, loss = 0.18126111\n",
      "Iteration 372, loss = 0.18116964\n",
      "Iteration 373, loss = 0.18104454\n",
      "Iteration 374, loss = 0.18098131\n",
      "Iteration 375, loss = 0.18091268\n",
      "Iteration 376, loss = 0.18078395\n",
      "Iteration 377, loss = 0.18068163\n",
      "Iteration 378, loss = 0.18056611\n",
      "Iteration 379, loss = 0.18047047\n",
      "Iteration 380, loss = 0.18039799\n",
      "Iteration 381, loss = 0.18029915\n",
      "Iteration 382, loss = 0.18018316\n",
      "Iteration 383, loss = 0.18011044\n",
      "Iteration 384, loss = 0.18002859\n",
      "Iteration 385, loss = 0.17992130\n",
      "Iteration 386, loss = 0.17981875\n",
      "Iteration 387, loss = 0.17972200\n",
      "Iteration 388, loss = 0.17961545\n",
      "Iteration 389, loss = 0.17954367\n",
      "Iteration 390, loss = 0.17947246\n",
      "Iteration 391, loss = 0.17940107\n",
      "Iteration 392, loss = 0.17926303\n",
      "Iteration 393, loss = 0.17918815\n",
      "Iteration 394, loss = 0.17906837\n",
      "Iteration 395, loss = 0.17903021\n",
      "Iteration 396, loss = 0.17896019\n",
      "Iteration 397, loss = 0.17881415\n",
      "Iteration 398, loss = 0.17871452\n",
      "Iteration 399, loss = 0.17867365\n",
      "Iteration 400, loss = 0.17856363\n",
      "Iteration 401, loss = 0.17845233\n",
      "Iteration 402, loss = 0.17841078\n",
      "Iteration 403, loss = 0.17827853\n",
      "Iteration 404, loss = 0.17819924\n",
      "Iteration 405, loss = 0.17810043\n",
      "Iteration 406, loss = 0.17803311\n",
      "Iteration 407, loss = 0.17792748\n",
      "Iteration 408, loss = 0.17790946\n",
      "Iteration 409, loss = 0.17775074\n",
      "Iteration 410, loss = 0.17770598\n",
      "Iteration 411, loss = 0.17759919\n",
      "Iteration 412, loss = 0.17754837\n",
      "Iteration 413, loss = 0.17743107\n",
      "Iteration 414, loss = 0.17735515\n",
      "Iteration 415, loss = 0.17727421\n",
      "Iteration 416, loss = 0.17720226\n",
      "Iteration 417, loss = 0.17710102\n",
      "Iteration 418, loss = 0.17701926\n",
      "Iteration 419, loss = 0.17693148\n",
      "Iteration 420, loss = 0.17685235\n",
      "Iteration 421, loss = 0.17683780\n",
      "Iteration 422, loss = 0.17668143\n",
      "Iteration 423, loss = 0.17662574\n",
      "Iteration 424, loss = 0.17651234\n",
      "Iteration 425, loss = 0.17645569\n",
      "Iteration 426, loss = 0.17639581\n",
      "Iteration 427, loss = 0.17628465\n",
      "Iteration 428, loss = 0.17620523\n",
      "Iteration 429, loss = 0.17613783\n",
      "Iteration 430, loss = 0.17607624\n",
      "Iteration 431, loss = 0.17598898\n",
      "Iteration 432, loss = 0.17591650\n",
      "Iteration 433, loss = 0.17590729\n",
      "Iteration 434, loss = 0.17577538\n",
      "Iteration 435, loss = 0.17571985\n",
      "Iteration 436, loss = 0.17561591\n",
      "Iteration 437, loss = 0.17555603\n",
      "Iteration 438, loss = 0.17550106\n",
      "Iteration 439, loss = 0.17541878\n",
      "Iteration 440, loss = 0.17535507\n",
      "Iteration 441, loss = 0.17528268\n",
      "Iteration 442, loss = 0.17525883\n",
      "Iteration 443, loss = 0.17516329\n",
      "Iteration 444, loss = 0.17507222\n",
      "Iteration 445, loss = 0.17500812\n",
      "Iteration 446, loss = 0.17495830\n",
      "Iteration 447, loss = 0.17487672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 1.05508639\n",
      "Iteration 2, loss = 1.01655892\n",
      "Iteration 3, loss = 0.97901013\n",
      "Iteration 4, loss = 0.94295856\n",
      "Iteration 5, loss = 0.90867990\n",
      "Iteration 6, loss = 0.87607286\n",
      "Iteration 7, loss = 0.84398043\n",
      "Iteration 8, loss = 0.81459908\n",
      "Iteration 9, loss = 0.78603677\n",
      "Iteration 10, loss = 0.75941741\n",
      "Iteration 11, loss = 0.73364580\n",
      "Iteration 12, loss = 0.70980399\n",
      "Iteration 13, loss = 0.68643254\n",
      "Iteration 14, loss = 0.66481386\n",
      "Iteration 15, loss = 0.64435444\n",
      "Iteration 16, loss = 0.62517765\n",
      "Iteration 17, loss = 0.60666481\n",
      "Iteration 18, loss = 0.58866803\n",
      "Iteration 19, loss = 0.57177989\n",
      "Iteration 20, loss = 0.55552496\n",
      "Iteration 21, loss = 0.54045208\n",
      "Iteration 22, loss = 0.52582567\n",
      "Iteration 23, loss = 0.51246801\n",
      "Iteration 24, loss = 0.49953270\n",
      "Iteration 25, loss = 0.48744746\n",
      "Iteration 26, loss = 0.47608974\n",
      "Iteration 27, loss = 0.46503144\n",
      "Iteration 28, loss = 0.45518614\n",
      "Iteration 29, loss = 0.44570695\n",
      "Iteration 30, loss = 0.43645702\n",
      "Iteration 31, loss = 0.42780222\n",
      "Iteration 32, loss = 0.41987769\n",
      "Iteration 33, loss = 0.41237216\n",
      "Iteration 34, loss = 0.40540062\n",
      "Iteration 35, loss = 0.39835989\n",
      "Iteration 36, loss = 0.39188617\n",
      "Iteration 37, loss = 0.38567324\n",
      "Iteration 38, loss = 0.37993847\n",
      "Iteration 39, loss = 0.37395741\n",
      "Iteration 40, loss = 0.36862348\n",
      "Iteration 41, loss = 0.36336461\n",
      "Iteration 42, loss = 0.35848244\n",
      "Iteration 43, loss = 0.35358779\n",
      "Iteration 44, loss = 0.34911481\n",
      "Iteration 45, loss = 0.34459884\n",
      "Iteration 46, loss = 0.34028113\n",
      "Iteration 47, loss = 0.33609204\n",
      "Iteration 48, loss = 0.33213805\n",
      "Iteration 49, loss = 0.32845142\n",
      "Iteration 50, loss = 0.32473004\n",
      "Iteration 51, loss = 0.32114720\n",
      "Iteration 52, loss = 0.31777595\n",
      "Iteration 53, loss = 0.31451351\n",
      "Iteration 54, loss = 0.31143962\n",
      "Iteration 55, loss = 0.30811917\n",
      "Iteration 56, loss = 0.30540974\n",
      "Iteration 57, loss = 0.30254835\n",
      "Iteration 58, loss = 0.30000321\n",
      "Iteration 59, loss = 0.29751195\n",
      "Iteration 60, loss = 0.29503114\n",
      "Iteration 61, loss = 0.29269759\n",
      "Iteration 62, loss = 0.29054916\n",
      "Iteration 63, loss = 0.28842375\n",
      "Iteration 64, loss = 0.28643425\n",
      "Iteration 65, loss = 0.28448350\n",
      "Iteration 66, loss = 0.28266153\n",
      "Iteration 67, loss = 0.28082877\n",
      "Iteration 68, loss = 0.27905770\n",
      "Iteration 69, loss = 0.27747416\n",
      "Iteration 70, loss = 0.27593333\n",
      "Iteration 71, loss = 0.27436743\n",
      "Iteration 72, loss = 0.27288579\n",
      "Iteration 73, loss = 0.27139200\n",
      "Iteration 74, loss = 0.26995910\n",
      "Iteration 75, loss = 0.26863299\n",
      "Iteration 76, loss = 0.26730339\n",
      "Iteration 77, loss = 0.26595653\n",
      "Iteration 78, loss = 0.26473297\n",
      "Iteration 79, loss = 0.26346903\n",
      "Iteration 80, loss = 0.26229905\n",
      "Iteration 81, loss = 0.26119369\n",
      "Iteration 82, loss = 0.26003420\n",
      "Iteration 83, loss = 0.25887545\n",
      "Iteration 84, loss = 0.25785657\n",
      "Iteration 85, loss = 0.25682937\n",
      "Iteration 86, loss = 0.25580501\n",
      "Iteration 87, loss = 0.25480006\n",
      "Iteration 88, loss = 0.25377674\n",
      "Iteration 89, loss = 0.25282818\n",
      "Iteration 90, loss = 0.25186809\n",
      "Iteration 91, loss = 0.25089552\n",
      "Iteration 92, loss = 0.24999842\n",
      "Iteration 93, loss = 0.24912273\n",
      "Iteration 94, loss = 0.24825225\n",
      "Iteration 95, loss = 0.24733541\n",
      "Iteration 96, loss = 0.24649628\n",
      "Iteration 97, loss = 0.24568179\n",
      "Iteration 98, loss = 0.24487324\n",
      "Iteration 99, loss = 0.24407069\n",
      "Iteration 100, loss = 0.24324658\n",
      "Iteration 101, loss = 0.24252958\n",
      "Iteration 102, loss = 0.24174839\n",
      "Iteration 103, loss = 0.24100813\n",
      "Iteration 104, loss = 0.24025116\n",
      "Iteration 105, loss = 0.23951107\n",
      "Iteration 106, loss = 0.23879365\n",
      "Iteration 107, loss = 0.23809757\n",
      "Iteration 108, loss = 0.23744402\n",
      "Iteration 109, loss = 0.23669754\n",
      "Iteration 110, loss = 0.23603637\n",
      "Iteration 111, loss = 0.23532840\n",
      "Iteration 112, loss = 0.23469355\n",
      "Iteration 113, loss = 0.23401943\n",
      "Iteration 114, loss = 0.23337098\n",
      "Iteration 115, loss = 0.23271205\n",
      "Iteration 116, loss = 0.23204162\n",
      "Iteration 117, loss = 0.23138612\n",
      "Iteration 118, loss = 0.23077936\n",
      "Iteration 119, loss = 0.23010623\n",
      "Iteration 120, loss = 0.22950294\n",
      "Iteration 121, loss = 0.22886253\n",
      "Iteration 122, loss = 0.22825544\n",
      "Iteration 123, loss = 0.22763643\n",
      "Iteration 124, loss = 0.22709226\n",
      "Iteration 125, loss = 0.22643626\n",
      "Iteration 126, loss = 0.22589254\n",
      "Iteration 127, loss = 0.22530858\n",
      "Iteration 128, loss = 0.22473875\n",
      "Iteration 129, loss = 0.22417860\n",
      "Iteration 130, loss = 0.22360832\n",
      "Iteration 131, loss = 0.22304634\n",
      "Iteration 132, loss = 0.22249268\n",
      "Iteration 133, loss = 0.22197352\n",
      "Iteration 134, loss = 0.22140280\n",
      "Iteration 135, loss = 0.22085616\n",
      "Iteration 136, loss = 0.22035901\n",
      "Iteration 137, loss = 0.21984796\n",
      "Iteration 138, loss = 0.21930086\n",
      "Iteration 139, loss = 0.21876675\n",
      "Iteration 140, loss = 0.21825614\n",
      "Iteration 141, loss = 0.21774629\n",
      "Iteration 142, loss = 0.21726481\n",
      "Iteration 143, loss = 0.21672416\n",
      "Iteration 144, loss = 0.21622849\n",
      "Iteration 145, loss = 0.21576481\n",
      "Iteration 146, loss = 0.21531292\n",
      "Iteration 147, loss = 0.21481991\n",
      "Iteration 148, loss = 0.21437874\n",
      "Iteration 149, loss = 0.21391661\n",
      "Iteration 150, loss = 0.21345883\n",
      "Iteration 151, loss = 0.21302382\n",
      "Iteration 152, loss = 0.21260869\n",
      "Iteration 153, loss = 0.21217924\n",
      "Iteration 154, loss = 0.21171460\n",
      "Iteration 155, loss = 0.21132586\n",
      "Iteration 156, loss = 0.21090642\n",
      "Iteration 157, loss = 0.21048911\n",
      "Iteration 158, loss = 0.21005118\n",
      "Iteration 159, loss = 0.20967045\n",
      "Iteration 160, loss = 0.20926748\n",
      "Iteration 161, loss = 0.20884292\n",
      "Iteration 162, loss = 0.20848124\n",
      "Iteration 163, loss = 0.20807303\n",
      "Iteration 164, loss = 0.20767541\n",
      "Iteration 165, loss = 0.20731237\n",
      "Iteration 166, loss = 0.20693379\n",
      "Iteration 167, loss = 0.20656790\n",
      "Iteration 168, loss = 0.20619742\n",
      "Iteration 169, loss = 0.20582132\n",
      "Iteration 170, loss = 0.20548513\n",
      "Iteration 171, loss = 0.20514947\n",
      "Iteration 172, loss = 0.20475888\n",
      "Iteration 173, loss = 0.20440949\n",
      "Iteration 174, loss = 0.20405761\n",
      "Iteration 175, loss = 0.20370392\n",
      "Iteration 176, loss = 0.20336381\n",
      "Iteration 177, loss = 0.20303473\n",
      "Iteration 178, loss = 0.20268747\n",
      "Iteration 179, loss = 0.20237435\n",
      "Iteration 180, loss = 0.20203420\n",
      "Iteration 181, loss = 0.20168245\n",
      "Iteration 182, loss = 0.20134804\n",
      "Iteration 183, loss = 0.20100879\n",
      "Iteration 184, loss = 0.20067970\n",
      "Iteration 185, loss = 0.20032807\n",
      "Iteration 186, loss = 0.20000767\n",
      "Iteration 187, loss = 0.19968271\n",
      "Iteration 188, loss = 0.19939934\n",
      "Iteration 189, loss = 0.19904958\n",
      "Iteration 190, loss = 0.19874822\n",
      "Iteration 191, loss = 0.19841138\n",
      "Iteration 192, loss = 0.19810742\n",
      "Iteration 193, loss = 0.19778465\n",
      "Iteration 194, loss = 0.19747571\n",
      "Iteration 195, loss = 0.19716310\n",
      "Iteration 196, loss = 0.19685579\n",
      "Iteration 197, loss = 0.19656888\n",
      "Iteration 198, loss = 0.19626263\n",
      "Iteration 199, loss = 0.19595072\n",
      "Iteration 200, loss = 0.19567746\n",
      "Iteration 201, loss = 0.19535631\n",
      "Iteration 202, loss = 0.19507303\n",
      "Iteration 203, loss = 0.19477326\n",
      "Iteration 204, loss = 0.19446891\n",
      "Iteration 205, loss = 0.19419352\n",
      "Iteration 206, loss = 0.19390570\n",
      "Iteration 207, loss = 0.19360868\n",
      "Iteration 208, loss = 0.19332292\n",
      "Iteration 209, loss = 0.19304724\n",
      "Iteration 210, loss = 0.19278229\n",
      "Iteration 211, loss = 0.19248417\n",
      "Iteration 212, loss = 0.19221343\n",
      "Iteration 213, loss = 0.19196273\n",
      "Iteration 214, loss = 0.19171679\n",
      "Iteration 215, loss = 0.19144638\n",
      "Iteration 216, loss = 0.19116732\n",
      "Iteration 217, loss = 0.19093766\n",
      "Iteration 218, loss = 0.19068487\n",
      "Iteration 219, loss = 0.19045087\n",
      "Iteration 220, loss = 0.19019144\n",
      "Iteration 221, loss = 0.18994109\n",
      "Iteration 222, loss = 0.18971846\n",
      "Iteration 223, loss = 0.18945911\n",
      "Iteration 224, loss = 0.18924877\n",
      "Iteration 225, loss = 0.18898806\n",
      "Iteration 226, loss = 0.18877925\n",
      "Iteration 227, loss = 0.18854157\n",
      "Iteration 228, loss = 0.18829667\n",
      "Iteration 229, loss = 0.18807897\n",
      "Iteration 230, loss = 0.18784281\n",
      "Iteration 231, loss = 0.18762518\n",
      "Iteration 232, loss = 0.18741748\n",
      "Iteration 233, loss = 0.18717724\n",
      "Iteration 234, loss = 0.18697180\n",
      "Iteration 235, loss = 0.18675151\n",
      "Iteration 236, loss = 0.18654207\n",
      "Iteration 237, loss = 0.18631620\n",
      "Iteration 238, loss = 0.18611215\n",
      "Iteration 239, loss = 0.18589099\n",
      "Iteration 240, loss = 0.18566196\n",
      "Iteration 241, loss = 0.18544031\n",
      "Iteration 242, loss = 0.18524041\n",
      "Iteration 243, loss = 0.18503718\n",
      "Iteration 244, loss = 0.18478544\n",
      "Iteration 245, loss = 0.18459126\n",
      "Iteration 246, loss = 0.18436474\n",
      "Iteration 247, loss = 0.18416269\n",
      "Iteration 248, loss = 0.18395207\n",
      "Iteration 249, loss = 0.18378380\n",
      "Iteration 250, loss = 0.18354780\n",
      "Iteration 251, loss = 0.18336928\n",
      "Iteration 252, loss = 0.18316478\n",
      "Iteration 253, loss = 0.18297196\n",
      "Iteration 254, loss = 0.18280560\n",
      "Iteration 255, loss = 0.18261213\n",
      "Iteration 256, loss = 0.18240400\n",
      "Iteration 257, loss = 0.18221265\n",
      "Iteration 258, loss = 0.18203573\n",
      "Iteration 259, loss = 0.18184157\n",
      "Iteration 260, loss = 0.18168191\n",
      "Iteration 261, loss = 0.18147942\n",
      "Iteration 262, loss = 0.18130155\n",
      "Iteration 263, loss = 0.18112952\n",
      "Iteration 264, loss = 0.18095810\n",
      "Iteration 265, loss = 0.18077176\n",
      "Iteration 266, loss = 0.18059629\n",
      "Iteration 267, loss = 0.18042616\n",
      "Iteration 268, loss = 0.18025004\n",
      "Iteration 269, loss = 0.18009484\n",
      "Iteration 270, loss = 0.17990982\n",
      "Iteration 271, loss = 0.17975624\n",
      "Iteration 272, loss = 0.17957565\n",
      "Iteration 273, loss = 0.17939725\n",
      "Iteration 274, loss = 0.17924095\n",
      "Iteration 275, loss = 0.17908220\n",
      "Iteration 276, loss = 0.17889915\n",
      "Iteration 277, loss = 0.17875543\n",
      "Iteration 278, loss = 0.17858118\n",
      "Iteration 279, loss = 0.17843263\n",
      "Iteration 280, loss = 0.17826805\n",
      "Iteration 281, loss = 0.17813389\n",
      "Iteration 282, loss = 0.17796086\n",
      "Iteration 283, loss = 0.17779812\n",
      "Iteration 284, loss = 0.17763440\n",
      "Iteration 285, loss = 0.17748257\n",
      "Iteration 286, loss = 0.17731720\n",
      "Iteration 287, loss = 0.17717089\n",
      "Iteration 288, loss = 0.17703764\n",
      "Iteration 289, loss = 0.17685151\n",
      "Iteration 290, loss = 0.17671213\n",
      "Iteration 291, loss = 0.17657496\n",
      "Iteration 292, loss = 0.17641605\n",
      "Iteration 293, loss = 0.17629353\n",
      "Iteration 294, loss = 0.17616419\n",
      "Iteration 295, loss = 0.17600709\n",
      "Iteration 296, loss = 0.17588828\n",
      "Iteration 297, loss = 0.17574796\n",
      "Iteration 298, loss = 0.17559262\n",
      "Iteration 299, loss = 0.17545535\n",
      "Iteration 300, loss = 0.17532618\n",
      "Iteration 301, loss = 0.17518133\n",
      "Iteration 302, loss = 0.17503837\n",
      "Iteration 303, loss = 0.17491429\n",
      "Iteration 304, loss = 0.17477691\n",
      "Iteration 305, loss = 0.17464344\n",
      "Iteration 306, loss = 0.17450698\n",
      "Iteration 307, loss = 0.17442250\n",
      "Iteration 308, loss = 0.17426841\n",
      "Iteration 309, loss = 0.17413173\n",
      "Iteration 310, loss = 0.17401125\n",
      "Iteration 311, loss = 0.17386675\n",
      "Iteration 312, loss = 0.17375630\n",
      "Iteration 313, loss = 0.17363332\n",
      "Iteration 314, loss = 0.17351510\n",
      "Iteration 315, loss = 0.17340590\n",
      "Iteration 316, loss = 0.17327511\n",
      "Iteration 317, loss = 0.17314715\n",
      "Iteration 318, loss = 0.17305027\n",
      "Iteration 319, loss = 0.17288619\n",
      "Iteration 320, loss = 0.17276750\n",
      "Iteration 321, loss = 0.17267499\n",
      "Iteration 322, loss = 0.17256056\n",
      "Iteration 323, loss = 0.17241218\n",
      "Iteration 324, loss = 0.17229346\n",
      "Iteration 325, loss = 0.17214968\n",
      "Iteration 326, loss = 0.17204190\n",
      "Iteration 327, loss = 0.17193327\n",
      "Iteration 328, loss = 0.17181390\n",
      "Iteration 329, loss = 0.17170071\n",
      "Iteration 330, loss = 0.17158474\n",
      "Iteration 331, loss = 0.17148280\n",
      "Iteration 332, loss = 0.17132429\n",
      "Iteration 333, loss = 0.17122673\n",
      "Iteration 334, loss = 0.17109487\n",
      "Iteration 335, loss = 0.17098488\n",
      "Iteration 336, loss = 0.17085440\n",
      "Iteration 337, loss = 0.17074699\n",
      "Iteration 338, loss = 0.17062379\n",
      "Iteration 339, loss = 0.17049381\n",
      "Iteration 340, loss = 0.17039059\n",
      "Iteration 341, loss = 0.17025003\n",
      "Iteration 342, loss = 0.17015115\n",
      "Iteration 343, loss = 0.17008187\n",
      "Iteration 344, loss = 0.16991953\n",
      "Iteration 345, loss = 0.16976808\n",
      "Iteration 346, loss = 0.16967017\n",
      "Iteration 347, loss = 0.16955266\n",
      "Iteration 348, loss = 0.16945820\n",
      "Iteration 349, loss = 0.16932468\n",
      "Iteration 350, loss = 0.16920365\n",
      "Iteration 351, loss = 0.16912154\n",
      "Iteration 352, loss = 0.16897415\n",
      "Iteration 353, loss = 0.16890044\n",
      "Iteration 354, loss = 0.16880605\n",
      "Iteration 355, loss = 0.16871651\n",
      "Iteration 356, loss = 0.16856606\n",
      "Iteration 357, loss = 0.16845139\n",
      "Iteration 358, loss = 0.16836787\n",
      "Iteration 359, loss = 0.16823449\n",
      "Iteration 360, loss = 0.16812372\n",
      "Iteration 361, loss = 0.16803569\n",
      "Iteration 362, loss = 0.16791289\n",
      "Iteration 363, loss = 0.16780421\n",
      "Iteration 364, loss = 0.16770767\n",
      "Iteration 365, loss = 0.16761891\n",
      "Iteration 366, loss = 0.16751864\n",
      "Iteration 367, loss = 0.16739307\n",
      "Iteration 368, loss = 0.16729260\n",
      "Iteration 369, loss = 0.16719480\n",
      "Iteration 370, loss = 0.16707920\n",
      "Iteration 371, loss = 0.16697616\n",
      "Iteration 372, loss = 0.16688959\n",
      "Iteration 373, loss = 0.16676668\n",
      "Iteration 374, loss = 0.16666869\n",
      "Iteration 375, loss = 0.16658064\n",
      "Iteration 376, loss = 0.16646705\n",
      "Iteration 377, loss = 0.16636312\n",
      "Iteration 378, loss = 0.16625325\n",
      "Iteration 379, loss = 0.16616359\n",
      "Iteration 380, loss = 0.16608377\n",
      "Iteration 381, loss = 0.16597145\n",
      "Iteration 382, loss = 0.16587559\n",
      "Iteration 383, loss = 0.16582556\n",
      "Iteration 384, loss = 0.16571064\n",
      "Iteration 385, loss = 0.16561353\n",
      "Iteration 386, loss = 0.16551213\n",
      "Iteration 387, loss = 0.16542069\n",
      "Iteration 388, loss = 0.16530426\n",
      "Iteration 389, loss = 0.16523178\n",
      "Iteration 390, loss = 0.16510560\n",
      "Iteration 391, loss = 0.16508075\n",
      "Iteration 392, loss = 0.16493204\n",
      "Iteration 393, loss = 0.16484242\n",
      "Iteration 394, loss = 0.16476279\n",
      "Iteration 395, loss = 0.16472360\n",
      "Iteration 396, loss = 0.16461477\n",
      "Iteration 397, loss = 0.16448013\n",
      "Iteration 398, loss = 0.16440124\n",
      "Iteration 399, loss = 0.16436401\n",
      "Iteration 400, loss = 0.16423477\n",
      "Iteration 401, loss = 0.16415022\n",
      "Iteration 402, loss = 0.16409326\n",
      "Iteration 403, loss = 0.16398702\n",
      "Iteration 404, loss = 0.16390384\n",
      "Iteration 405, loss = 0.16382668\n",
      "Iteration 406, loss = 0.16376437\n",
      "Iteration 407, loss = 0.16364931\n",
      "Iteration 408, loss = 0.16361281\n",
      "Iteration 409, loss = 0.16348464\n",
      "Iteration 410, loss = 0.16344653\n",
      "Iteration 411, loss = 0.16333749\n",
      "Iteration 412, loss = 0.16328229\n",
      "Iteration 413, loss = 0.16317635\n",
      "Iteration 414, loss = 0.16312415\n",
      "Iteration 415, loss = 0.16304101\n",
      "Iteration 416, loss = 0.16296843\n",
      "Iteration 417, loss = 0.16288188\n",
      "Iteration 418, loss = 0.16283015\n",
      "Iteration 419, loss = 0.16273018\n",
      "Iteration 420, loss = 0.16266199\n",
      "Iteration 421, loss = 0.16267713\n",
      "Iteration 422, loss = 0.16250731\n",
      "Iteration 423, loss = 0.16244653\n",
      "Iteration 424, loss = 0.16236798\n",
      "Iteration 425, loss = 0.16231053\n",
      "Iteration 426, loss = 0.16224813\n",
      "Iteration 427, loss = 0.16217202\n",
      "Iteration 428, loss = 0.16207838\n",
      "Iteration 429, loss = 0.16202010\n",
      "Iteration 430, loss = 0.16194231\n",
      "Iteration 431, loss = 0.16186857\n",
      "Iteration 432, loss = 0.16180969\n",
      "Iteration 433, loss = 0.16179988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.93604063\n",
      "Iteration 2, loss = 0.65208141\n",
      "Iteration 3, loss = 0.48620367\n",
      "Iteration 4, loss = 0.39384608\n",
      "Iteration 5, loss = 0.34172867\n",
      "Iteration 6, loss = 0.31045996\n",
      "Iteration 7, loss = 0.29306318\n",
      "Iteration 8, loss = 0.27972041\n",
      "Iteration 9, loss = 0.27030690\n",
      "Iteration 10, loss = 0.26230975\n",
      "Iteration 11, loss = 0.25623412\n",
      "Iteration 12, loss = 0.24976429\n",
      "Iteration 13, loss = 0.24426620\n",
      "Iteration 14, loss = 0.23939637\n",
      "Iteration 15, loss = 0.23503905\n",
      "Iteration 16, loss = 0.23003093\n",
      "Iteration 17, loss = 0.22575251\n",
      "Iteration 18, loss = 0.22206956\n",
      "Iteration 19, loss = 0.21814374\n",
      "Iteration 20, loss = 0.21513953\n",
      "Iteration 21, loss = 0.21181810\n",
      "Iteration 22, loss = 0.20922052\n",
      "Iteration 23, loss = 0.20666969\n",
      "Iteration 24, loss = 0.20478969\n",
      "Iteration 25, loss = 0.20216900\n",
      "Iteration 26, loss = 0.19994978\n",
      "Iteration 27, loss = 0.19775420\n",
      "Iteration 28, loss = 0.19609273\n",
      "Iteration 29, loss = 0.19427978\n",
      "Iteration 30, loss = 0.19285045\n",
      "Iteration 31, loss = 0.19150724\n",
      "Iteration 32, loss = 0.19019507\n",
      "Iteration 33, loss = 0.18880392\n",
      "Iteration 34, loss = 0.18761513\n",
      "Iteration 35, loss = 0.18646737\n",
      "Iteration 36, loss = 0.18523590\n",
      "Iteration 37, loss = 0.18411115\n",
      "Iteration 38, loss = 0.18316185\n",
      "Iteration 39, loss = 0.18236262\n",
      "Iteration 40, loss = 0.18128213\n",
      "Iteration 41, loss = 0.18067478\n",
      "Iteration 42, loss = 0.17979216\n",
      "Iteration 43, loss = 0.17901894\n",
      "Iteration 44, loss = 0.17829731\n",
      "Iteration 45, loss = 0.17741993\n",
      "Iteration 46, loss = 0.17669272\n",
      "Iteration 47, loss = 0.17617964\n",
      "Iteration 48, loss = 0.17555209\n",
      "Iteration 49, loss = 0.17487575\n",
      "Iteration 50, loss = 0.17447550\n",
      "Iteration 51, loss = 0.17430843\n",
      "Iteration 52, loss = 0.17351189\n",
      "Iteration 53, loss = 0.17303201\n",
      "Iteration 54, loss = 0.17270790\n",
      "Iteration 55, loss = 0.17237860\n",
      "Iteration 56, loss = 0.17181342\n",
      "Iteration 57, loss = 0.17132245\n",
      "Iteration 58, loss = 0.17141903\n",
      "Iteration 59, loss = 0.17052713\n",
      "Iteration 60, loss = 0.17029058\n",
      "Iteration 61, loss = 0.17001369\n",
      "Iteration 62, loss = 0.17002575\n",
      "Iteration 63, loss = 0.16962074\n",
      "Iteration 64, loss = 0.16918653\n",
      "Iteration 65, loss = 0.16888756\n",
      "Iteration 66, loss = 0.16869347\n",
      "Iteration 67, loss = 0.16837580\n",
      "Iteration 68, loss = 0.16821862\n",
      "Iteration 69, loss = 0.16816445\n",
      "Iteration 70, loss = 0.16804022\n",
      "Iteration 71, loss = 0.16776757\n",
      "Iteration 72, loss = 0.16823657\n",
      "Iteration 73, loss = 0.16748246\n",
      "Iteration 74, loss = 0.16729310\n",
      "Iteration 75, loss = 0.16704453\n",
      "Iteration 76, loss = 0.16724777\n",
      "Iteration 77, loss = 0.16713106\n",
      "Iteration 78, loss = 0.16698449\n",
      "Iteration 79, loss = 0.16664661\n",
      "Iteration 80, loss = 0.16652142\n",
      "Iteration 81, loss = 0.16651081\n",
      "Iteration 82, loss = 0.16672291\n",
      "Iteration 83, loss = 0.16630040\n",
      "Iteration 84, loss = 0.16657037\n",
      "Iteration 85, loss = 0.16629584\n",
      "Iteration 86, loss = 0.16636935\n",
      "Iteration 87, loss = 0.16632923\n",
      "Iteration 88, loss = 0.16607538\n",
      "Iteration 89, loss = 0.16622297\n",
      "Iteration 90, loss = 0.16607729\n",
      "Iteration 91, loss = 0.16582271\n",
      "Iteration 92, loss = 0.16571668\n",
      "Iteration 93, loss = 0.16595007\n",
      "Iteration 94, loss = 0.16571244\n",
      "Iteration 95, loss = 0.16559176\n",
      "Iteration 96, loss = 0.16585671\n",
      "Iteration 97, loss = 0.16603148\n",
      "Iteration 98, loss = 0.16614667\n",
      "Iteration 99, loss = 0.16552678\n",
      "Iteration 100, loss = 0.16589304\n",
      "Iteration 101, loss = 0.16560862\n",
      "Iteration 102, loss = 0.16574713\n",
      "Iteration 103, loss = 0.16550959\n",
      "Iteration 104, loss = 0.16554908\n",
      "Iteration 105, loss = 0.16544560\n",
      "Iteration 106, loss = 0.16536349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 1.04963013\n",
      "Iteration 2, loss = 1.01170243\n",
      "Iteration 3, loss = 0.97437761\n",
      "Iteration 4, loss = 0.93858779\n",
      "Iteration 5, loss = 0.90362355\n",
      "Iteration 6, loss = 0.87116716\n",
      "Iteration 7, loss = 0.83995444\n",
      "Iteration 8, loss = 0.81021730\n",
      "Iteration 9, loss = 0.78221873\n",
      "Iteration 10, loss = 0.75523391\n",
      "Iteration 11, loss = 0.72993513\n",
      "Iteration 12, loss = 0.70631489\n",
      "Iteration 13, loss = 0.68383533\n",
      "Iteration 14, loss = 0.66251522\n",
      "Iteration 15, loss = 0.64190333\n",
      "Iteration 16, loss = 0.62247482\n",
      "Iteration 17, loss = 0.60460203\n",
      "Iteration 18, loss = 0.58658853\n",
      "Iteration 19, loss = 0.57020149\n",
      "Iteration 20, loss = 0.55474609\n",
      "Iteration 21, loss = 0.53980296\n",
      "Iteration 22, loss = 0.52591945\n",
      "Iteration 23, loss = 0.51312771\n",
      "Iteration 24, loss = 0.50060534\n",
      "Iteration 25, loss = 0.48904555\n",
      "Iteration 26, loss = 0.47807136\n",
      "Iteration 27, loss = 0.46795853\n",
      "Iteration 28, loss = 0.45800452\n",
      "Iteration 29, loss = 0.44901702\n",
      "Iteration 30, loss = 0.44019518\n",
      "Iteration 31, loss = 0.43199414\n",
      "Iteration 32, loss = 0.42425914\n",
      "Iteration 33, loss = 0.41690299\n",
      "Iteration 34, loss = 0.41009540\n",
      "Iteration 35, loss = 0.40334608\n",
      "Iteration 36, loss = 0.39695392\n",
      "Iteration 37, loss = 0.39093858\n",
      "Iteration 38, loss = 0.38504032\n",
      "Iteration 39, loss = 0.37971878\n",
      "Iteration 40, loss = 0.37403365\n",
      "Iteration 41, loss = 0.36898131\n",
      "Iteration 42, loss = 0.36435350\n",
      "Iteration 43, loss = 0.35960151\n",
      "Iteration 44, loss = 0.35497782\n",
      "Iteration 45, loss = 0.35034454\n",
      "Iteration 46, loss = 0.34624262\n",
      "Iteration 47, loss = 0.34220302\n",
      "Iteration 48, loss = 0.33828569\n",
      "Iteration 49, loss = 0.33448236\n",
      "Iteration 50, loss = 0.33103833\n",
      "Iteration 51, loss = 0.32759277\n",
      "Iteration 52, loss = 0.32428356\n",
      "Iteration 53, loss = 0.32102295\n",
      "Iteration 54, loss = 0.31800112\n",
      "Iteration 55, loss = 0.31516373\n",
      "Iteration 56, loss = 0.31251006\n",
      "Iteration 57, loss = 0.30973247\n",
      "Iteration 58, loss = 0.30735362\n",
      "Iteration 59, loss = 0.30488611\n",
      "Iteration 60, loss = 0.30259031\n",
      "Iteration 61, loss = 0.30045585\n",
      "Iteration 62, loss = 0.29836679\n",
      "Iteration 63, loss = 0.29637841\n",
      "Iteration 64, loss = 0.29439500\n",
      "Iteration 65, loss = 0.29264569\n",
      "Iteration 66, loss = 0.29082663\n",
      "Iteration 67, loss = 0.28914315\n",
      "Iteration 68, loss = 0.28750963\n",
      "Iteration 69, loss = 0.28592661\n",
      "Iteration 70, loss = 0.28441706\n",
      "Iteration 71, loss = 0.28292267\n",
      "Iteration 72, loss = 0.28162830\n",
      "Iteration 73, loss = 0.28016549\n",
      "Iteration 74, loss = 0.27883599\n",
      "Iteration 75, loss = 0.27752757\n",
      "Iteration 76, loss = 0.27626598\n",
      "Iteration 77, loss = 0.27499727\n",
      "Iteration 78, loss = 0.27373296\n",
      "Iteration 79, loss = 0.27255171\n",
      "Iteration 80, loss = 0.27139194\n",
      "Iteration 81, loss = 0.27022263\n",
      "Iteration 82, loss = 0.26907960\n",
      "Iteration 83, loss = 0.26803472\n",
      "Iteration 84, loss = 0.26698261\n",
      "Iteration 85, loss = 0.26590669\n",
      "Iteration 86, loss = 0.26488284\n",
      "Iteration 87, loss = 0.26392201\n",
      "Iteration 88, loss = 0.26288182\n",
      "Iteration 89, loss = 0.26194644\n",
      "Iteration 90, loss = 0.26105059\n",
      "Iteration 91, loss = 0.26011903\n",
      "Iteration 92, loss = 0.25921605\n",
      "Iteration 93, loss = 0.25830325\n",
      "Iteration 94, loss = 0.25742006\n",
      "Iteration 95, loss = 0.25656589\n",
      "Iteration 96, loss = 0.25573453\n",
      "Iteration 97, loss = 0.25496439\n",
      "Iteration 98, loss = 0.25412676\n",
      "Iteration 99, loss = 0.25332277\n",
      "Iteration 100, loss = 0.25252837\n",
      "Iteration 101, loss = 0.25181603\n",
      "Iteration 102, loss = 0.25107034\n",
      "Iteration 103, loss = 0.25033686\n",
      "Iteration 104, loss = 0.24966412\n",
      "Iteration 105, loss = 0.24889303\n",
      "Iteration 106, loss = 0.24821163\n",
      "Iteration 107, loss = 0.24751500\n",
      "Iteration 108, loss = 0.24688539\n",
      "Iteration 109, loss = 0.24618844\n",
      "Iteration 110, loss = 0.24553453\n",
      "Iteration 111, loss = 0.24487911\n",
      "Iteration 112, loss = 0.24419927\n",
      "Iteration 113, loss = 0.24354295\n",
      "Iteration 114, loss = 0.24290113\n",
      "Iteration 115, loss = 0.24224664\n",
      "Iteration 116, loss = 0.24160934\n",
      "Iteration 117, loss = 0.24092367\n",
      "Iteration 118, loss = 0.24033836\n",
      "Iteration 119, loss = 0.23970501\n",
      "Iteration 120, loss = 0.23905179\n",
      "Iteration 121, loss = 0.23844704\n",
      "Iteration 122, loss = 0.23784386\n",
      "Iteration 123, loss = 0.23723422\n",
      "Iteration 124, loss = 0.23663611\n",
      "Iteration 125, loss = 0.23605174\n",
      "Iteration 126, loss = 0.23547666\n",
      "Iteration 127, loss = 0.23486351\n",
      "Iteration 128, loss = 0.23431684\n",
      "Iteration 129, loss = 0.23380745\n",
      "Iteration 130, loss = 0.23325238\n",
      "Iteration 131, loss = 0.23269966\n",
      "Iteration 132, loss = 0.23218031\n",
      "Iteration 133, loss = 0.23164935\n",
      "Iteration 134, loss = 0.23112469\n",
      "Iteration 135, loss = 0.23062192\n",
      "Iteration 136, loss = 0.23014744\n",
      "Iteration 137, loss = 0.22963515\n",
      "Iteration 138, loss = 0.22916994\n",
      "Iteration 139, loss = 0.22865254\n",
      "Iteration 140, loss = 0.22818698\n",
      "Iteration 141, loss = 0.22772283\n",
      "Iteration 142, loss = 0.22725025\n",
      "Iteration 143, loss = 0.22678882\n",
      "Iteration 144, loss = 0.22633675\n",
      "Iteration 145, loss = 0.22587511\n",
      "Iteration 146, loss = 0.22541883\n",
      "Iteration 147, loss = 0.22496305\n",
      "Iteration 148, loss = 0.22456948\n",
      "Iteration 149, loss = 0.22415846\n",
      "Iteration 150, loss = 0.22371173\n",
      "Iteration 151, loss = 0.22328932\n",
      "Iteration 152, loss = 0.22286836\n",
      "Iteration 153, loss = 0.22247647\n",
      "Iteration 154, loss = 0.22208574\n",
      "Iteration 155, loss = 0.22165207\n",
      "Iteration 156, loss = 0.22127212\n",
      "Iteration 157, loss = 0.22087586\n",
      "Iteration 158, loss = 0.22046906\n",
      "Iteration 159, loss = 0.22011626\n",
      "Iteration 160, loss = 0.21971084\n",
      "Iteration 161, loss = 0.21933800\n",
      "Iteration 162, loss = 0.21895146\n",
      "Iteration 163, loss = 0.21860188\n",
      "Iteration 164, loss = 0.21824000\n",
      "Iteration 165, loss = 0.21789117\n",
      "Iteration 166, loss = 0.21752868\n",
      "Iteration 167, loss = 0.21716794\n",
      "Iteration 168, loss = 0.21682295\n",
      "Iteration 169, loss = 0.21649850\n",
      "Iteration 170, loss = 0.21612475\n",
      "Iteration 171, loss = 0.21580950\n",
      "Iteration 172, loss = 0.21543574\n",
      "Iteration 173, loss = 0.21511785\n",
      "Iteration 174, loss = 0.21476484\n",
      "Iteration 175, loss = 0.21443532\n",
      "Iteration 176, loss = 0.21407927\n",
      "Iteration 177, loss = 0.21376011\n",
      "Iteration 178, loss = 0.21343200\n",
      "Iteration 179, loss = 0.21308631\n",
      "Iteration 180, loss = 0.21278202\n",
      "Iteration 181, loss = 0.21242748\n",
      "Iteration 182, loss = 0.21213373\n",
      "Iteration 183, loss = 0.21181004\n",
      "Iteration 184, loss = 0.21147703\n",
      "Iteration 185, loss = 0.21120406\n",
      "Iteration 186, loss = 0.21088362\n",
      "Iteration 187, loss = 0.21054540\n",
      "Iteration 188, loss = 0.21023252\n",
      "Iteration 189, loss = 0.20995367\n",
      "Iteration 190, loss = 0.20961606\n",
      "Iteration 191, loss = 0.20931476\n",
      "Iteration 192, loss = 0.20905085\n",
      "Iteration 193, loss = 0.20872583\n",
      "Iteration 194, loss = 0.20844630\n",
      "Iteration 195, loss = 0.20813524\n",
      "Iteration 196, loss = 0.20785698\n",
      "Iteration 197, loss = 0.20757789\n",
      "Iteration 198, loss = 0.20729253\n",
      "Iteration 199, loss = 0.20701798\n",
      "Iteration 200, loss = 0.20673713\n",
      "Iteration 201, loss = 0.20647308\n",
      "Iteration 202, loss = 0.20619737\n",
      "Iteration 203, loss = 0.20591482\n",
      "Iteration 204, loss = 0.20564134\n",
      "Iteration 205, loss = 0.20536548\n",
      "Iteration 206, loss = 0.20510686\n",
      "Iteration 207, loss = 0.20484174\n",
      "Iteration 208, loss = 0.20458690\n",
      "Iteration 209, loss = 0.20434052\n",
      "Iteration 210, loss = 0.20405389\n",
      "Iteration 211, loss = 0.20380319\n",
      "Iteration 212, loss = 0.20360307\n",
      "Iteration 213, loss = 0.20331519\n",
      "Iteration 214, loss = 0.20309941\n",
      "Iteration 215, loss = 0.20283048\n",
      "Iteration 216, loss = 0.20257851\n",
      "Iteration 217, loss = 0.20235012\n",
      "Iteration 218, loss = 0.20211903\n",
      "Iteration 219, loss = 0.20192245\n",
      "Iteration 220, loss = 0.20166423\n",
      "Iteration 221, loss = 0.20143417\n",
      "Iteration 222, loss = 0.20119021\n",
      "Iteration 223, loss = 0.20095308\n",
      "Iteration 224, loss = 0.20072864\n",
      "Iteration 225, loss = 0.20050724\n",
      "Iteration 226, loss = 0.20027996\n",
      "Iteration 227, loss = 0.20006389\n",
      "Iteration 228, loss = 0.19986829\n",
      "Iteration 229, loss = 0.19962946\n",
      "Iteration 230, loss = 0.19942129\n",
      "Iteration 231, loss = 0.19919253\n",
      "Iteration 232, loss = 0.19897240\n",
      "Iteration 233, loss = 0.19875268\n",
      "Iteration 234, loss = 0.19856363\n",
      "Iteration 235, loss = 0.19834080\n",
      "Iteration 236, loss = 0.19814304\n",
      "Iteration 237, loss = 0.19792597\n",
      "Iteration 238, loss = 0.19773834\n",
      "Iteration 239, loss = 0.19751543\n",
      "Iteration 240, loss = 0.19731094\n",
      "Iteration 241, loss = 0.19713200\n",
      "Iteration 242, loss = 0.19693970\n",
      "Iteration 243, loss = 0.19672450\n",
      "Iteration 244, loss = 0.19653872\n",
      "Iteration 245, loss = 0.19632529\n",
      "Iteration 246, loss = 0.19613540\n",
      "Iteration 247, loss = 0.19592817\n",
      "Iteration 248, loss = 0.19575524\n",
      "Iteration 249, loss = 0.19556896\n",
      "Iteration 250, loss = 0.19535921\n",
      "Iteration 251, loss = 0.19516091\n",
      "Iteration 252, loss = 0.19497934\n",
      "Iteration 253, loss = 0.19480466\n",
      "Iteration 254, loss = 0.19458362\n",
      "Iteration 255, loss = 0.19439387\n",
      "Iteration 256, loss = 0.19420713\n",
      "Iteration 257, loss = 0.19407213\n",
      "Iteration 258, loss = 0.19386387\n",
      "Iteration 259, loss = 0.19366554\n",
      "Iteration 260, loss = 0.19347753\n",
      "Iteration 261, loss = 0.19329262\n",
      "Iteration 262, loss = 0.19310765\n",
      "Iteration 263, loss = 0.19294424\n",
      "Iteration 264, loss = 0.19276977\n",
      "Iteration 265, loss = 0.19258348\n",
      "Iteration 266, loss = 0.19243911\n",
      "Iteration 267, loss = 0.19225798\n",
      "Iteration 268, loss = 0.19204150\n",
      "Iteration 269, loss = 0.19187007\n",
      "Iteration 270, loss = 0.19170525\n",
      "Iteration 271, loss = 0.19152181\n",
      "Iteration 272, loss = 0.19136744\n",
      "Iteration 273, loss = 0.19116917\n",
      "Iteration 274, loss = 0.19099217\n",
      "Iteration 275, loss = 0.19080154\n",
      "Iteration 276, loss = 0.19062813\n",
      "Iteration 277, loss = 0.19046859\n",
      "Iteration 278, loss = 0.19030118\n",
      "Iteration 279, loss = 0.19011975\n",
      "Iteration 280, loss = 0.18994383\n",
      "Iteration 281, loss = 0.18976829\n",
      "Iteration 282, loss = 0.18962064\n",
      "Iteration 283, loss = 0.18943986\n",
      "Iteration 284, loss = 0.18925507\n",
      "Iteration 285, loss = 0.18908323\n",
      "Iteration 286, loss = 0.18891502\n",
      "Iteration 287, loss = 0.18874967\n",
      "Iteration 288, loss = 0.18859084\n",
      "Iteration 289, loss = 0.18840540\n",
      "Iteration 290, loss = 0.18824872\n",
      "Iteration 291, loss = 0.18808659\n",
      "Iteration 292, loss = 0.18792160\n",
      "Iteration 293, loss = 0.18777356\n",
      "Iteration 294, loss = 0.18760949\n",
      "Iteration 295, loss = 0.18742584\n",
      "Iteration 296, loss = 0.18726365\n",
      "Iteration 297, loss = 0.18710240\n",
      "Iteration 298, loss = 0.18695167\n",
      "Iteration 299, loss = 0.18680599\n",
      "Iteration 300, loss = 0.18664731\n",
      "Iteration 301, loss = 0.18648731\n",
      "Iteration 302, loss = 0.18633821\n",
      "Iteration 303, loss = 0.18620609\n",
      "Iteration 304, loss = 0.18606464\n",
      "Iteration 305, loss = 0.18590540\n",
      "Iteration 306, loss = 0.18574853\n",
      "Iteration 307, loss = 0.18562036\n",
      "Iteration 308, loss = 0.18546582\n",
      "Iteration 309, loss = 0.18531960\n",
      "Iteration 310, loss = 0.18519142\n",
      "Iteration 311, loss = 0.18503279\n",
      "Iteration 312, loss = 0.18490033\n",
      "Iteration 313, loss = 0.18477137\n",
      "Iteration 314, loss = 0.18460642\n",
      "Iteration 315, loss = 0.18447916\n",
      "Iteration 316, loss = 0.18435424\n",
      "Iteration 317, loss = 0.18420694\n",
      "Iteration 318, loss = 0.18404777\n",
      "Iteration 319, loss = 0.18392717\n",
      "Iteration 320, loss = 0.18376965\n",
      "Iteration 321, loss = 0.18363906\n",
      "Iteration 322, loss = 0.18349170\n",
      "Iteration 323, loss = 0.18338002\n",
      "Iteration 324, loss = 0.18324833\n",
      "Iteration 325, loss = 0.18308333\n",
      "Iteration 326, loss = 0.18292825\n",
      "Iteration 327, loss = 0.18277960\n",
      "Iteration 328, loss = 0.18264212\n",
      "Iteration 329, loss = 0.18251925\n",
      "Iteration 330, loss = 0.18239291\n",
      "Iteration 331, loss = 0.18225564\n",
      "Iteration 332, loss = 0.18209880\n",
      "Iteration 333, loss = 0.18197826\n",
      "Iteration 334, loss = 0.18184542\n",
      "Iteration 335, loss = 0.18169965\n",
      "Iteration 336, loss = 0.18159585\n",
      "Iteration 337, loss = 0.18142975\n",
      "Iteration 338, loss = 0.18128889\n",
      "Iteration 339, loss = 0.18117795\n",
      "Iteration 340, loss = 0.18104571\n",
      "Iteration 341, loss = 0.18093631\n",
      "Iteration 342, loss = 0.18077988\n",
      "Iteration 343, loss = 0.18064617\n",
      "Iteration 344, loss = 0.18052657\n",
      "Iteration 345, loss = 0.18045376\n",
      "Iteration 346, loss = 0.18028064\n",
      "Iteration 347, loss = 0.18014913\n",
      "Iteration 348, loss = 0.18003530\n",
      "Iteration 349, loss = 0.17990952\n",
      "Iteration 350, loss = 0.17978818\n",
      "Iteration 351, loss = 0.17964573\n",
      "Iteration 352, loss = 0.17952080\n",
      "Iteration 353, loss = 0.17939068\n",
      "Iteration 354, loss = 0.17928969\n",
      "Iteration 355, loss = 0.17918733\n",
      "Iteration 356, loss = 0.17904895\n",
      "Iteration 357, loss = 0.17891784\n",
      "Iteration 358, loss = 0.17879743\n",
      "Iteration 359, loss = 0.17870106\n",
      "Iteration 360, loss = 0.17857197\n",
      "Iteration 361, loss = 0.17844585\n",
      "Iteration 362, loss = 0.17834899\n",
      "Iteration 363, loss = 0.17822969\n",
      "Iteration 364, loss = 0.17813641\n",
      "Iteration 365, loss = 0.17800819\n",
      "Iteration 366, loss = 0.17788230\n",
      "Iteration 367, loss = 0.17778064\n",
      "Iteration 368, loss = 0.17766854\n",
      "Iteration 369, loss = 0.17755967\n",
      "Iteration 370, loss = 0.17753103\n",
      "Iteration 371, loss = 0.17734964\n",
      "Iteration 372, loss = 0.17724962\n",
      "Iteration 373, loss = 0.17714877\n",
      "Iteration 374, loss = 0.17702933\n",
      "Iteration 375, loss = 0.17692523\n",
      "Iteration 376, loss = 0.17683615\n",
      "Iteration 377, loss = 0.17671581\n",
      "Iteration 378, loss = 0.17663800\n",
      "Iteration 379, loss = 0.17650510\n",
      "Iteration 380, loss = 0.17641488\n",
      "Iteration 381, loss = 0.17634686\n",
      "Iteration 382, loss = 0.17622670\n",
      "Iteration 383, loss = 0.17613530\n",
      "Iteration 384, loss = 0.17603235\n",
      "Iteration 385, loss = 0.17592580\n",
      "Iteration 386, loss = 0.17585595\n",
      "Iteration 387, loss = 0.17576852\n",
      "Iteration 388, loss = 0.17565573\n",
      "Iteration 389, loss = 0.17554741\n",
      "Iteration 390, loss = 0.17546980\n",
      "Iteration 391, loss = 0.17537346\n",
      "Iteration 392, loss = 0.17530843\n",
      "Iteration 393, loss = 0.17519012\n",
      "Iteration 394, loss = 0.17510972\n",
      "Iteration 395, loss = 0.17502060\n",
      "Iteration 396, loss = 0.17493159\n",
      "Iteration 397, loss = 0.17482799\n",
      "Iteration 398, loss = 0.17475911\n",
      "Iteration 399, loss = 0.17466747\n",
      "Iteration 400, loss = 0.17458279\n",
      "Iteration 401, loss = 0.17453086\n",
      "Iteration 402, loss = 0.17444645\n",
      "Iteration 403, loss = 0.17433136\n",
      "Iteration 404, loss = 0.17423352\n",
      "Iteration 405, loss = 0.17415303\n",
      "Iteration 406, loss = 0.17405899\n",
      "Iteration 407, loss = 0.17399595\n",
      "Iteration 408, loss = 0.17389192\n",
      "Iteration 409, loss = 0.17379505\n",
      "Iteration 410, loss = 0.17373183\n",
      "Iteration 411, loss = 0.17366461\n",
      "Iteration 412, loss = 0.17354426\n",
      "Iteration 413, loss = 0.17344598\n",
      "Iteration 414, loss = 0.17338322\n",
      "Iteration 415, loss = 0.17329584\n",
      "Iteration 416, loss = 0.17320474\n",
      "Iteration 417, loss = 0.17311926\n",
      "Iteration 418, loss = 0.17305614\n",
      "Iteration 419, loss = 0.17298870\n",
      "Iteration 420, loss = 0.17288805\n",
      "Iteration 421, loss = 0.17282077\n",
      "Iteration 422, loss = 0.17273316\n",
      "Iteration 423, loss = 0.17264690\n",
      "Iteration 424, loss = 0.17258527\n",
      "Iteration 425, loss = 0.17247234\n",
      "Iteration 426, loss = 0.17239985\n",
      "Iteration 427, loss = 0.17231286\n",
      "Iteration 428, loss = 0.17223758\n",
      "Iteration 429, loss = 0.17221310\n",
      "Iteration 430, loss = 0.17208759\n",
      "Iteration 431, loss = 0.17199966\n",
      "Iteration 432, loss = 0.17194151\n",
      "Iteration 433, loss = 0.17183579\n",
      "Iteration 434, loss = 0.17181304\n",
      "Iteration 435, loss = 0.17168927\n",
      "Iteration 436, loss = 0.17167488\n",
      "Iteration 437, loss = 0.17155930\n",
      "Iteration 438, loss = 0.17149316\n",
      "Iteration 439, loss = 0.17139910\n",
      "Iteration 440, loss = 0.17131238\n",
      "Iteration 441, loss = 0.17138370\n",
      "Iteration 442, loss = 0.17117037\n",
      "Iteration 443, loss = 0.17113973\n",
      "Iteration 444, loss = 0.17101516\n",
      "Iteration 445, loss = 0.17096275\n",
      "Iteration 446, loss = 0.17088505\n",
      "Iteration 447, loss = 0.17084895\n",
      "Iteration 448, loss = 0.17075474\n",
      "Iteration 449, loss = 0.17067717\n",
      "Iteration 450, loss = 0.17061778\n",
      "Iteration 451, loss = 0.17053792\n",
      "Iteration 452, loss = 0.17048524\n",
      "Iteration 453, loss = 0.17042437\n",
      "Iteration 454, loss = 0.17035949\n",
      "Iteration 455, loss = 0.17028548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 1.05211169\n",
      "Iteration 2, loss = 1.01387468\n",
      "Iteration 3, loss = 0.97636085\n",
      "Iteration 4, loss = 0.94054452\n",
      "Iteration 5, loss = 0.90559641\n",
      "Iteration 6, loss = 0.87319524\n",
      "Iteration 7, loss = 0.84176371\n",
      "Iteration 8, loss = 0.81193981\n",
      "Iteration 9, loss = 0.78344154\n",
      "Iteration 10, loss = 0.75634445\n",
      "Iteration 11, loss = 0.73127369\n",
      "Iteration 12, loss = 0.70717294\n",
      "Iteration 13, loss = 0.68454622\n",
      "Iteration 14, loss = 0.66257665\n",
      "Iteration 15, loss = 0.64221716\n",
      "Iteration 16, loss = 0.62239230\n",
      "Iteration 17, loss = 0.60437033\n",
      "Iteration 18, loss = 0.58620580\n",
      "Iteration 19, loss = 0.56956960\n",
      "Iteration 20, loss = 0.55380353\n",
      "Iteration 21, loss = 0.53854927\n",
      "Iteration 22, loss = 0.52425559\n",
      "Iteration 23, loss = 0.51103279\n",
      "Iteration 24, loss = 0.49820496\n",
      "Iteration 25, loss = 0.48636151\n",
      "Iteration 26, loss = 0.47500856\n",
      "Iteration 27, loss = 0.46464969\n",
      "Iteration 28, loss = 0.45447893\n",
      "Iteration 29, loss = 0.44509579\n",
      "Iteration 30, loss = 0.43605170\n",
      "Iteration 31, loss = 0.42764183\n",
      "Iteration 32, loss = 0.41971692\n",
      "Iteration 33, loss = 0.41225631\n",
      "Iteration 34, loss = 0.40511847\n",
      "Iteration 35, loss = 0.39833762\n",
      "Iteration 36, loss = 0.39176228\n",
      "Iteration 37, loss = 0.38565999\n",
      "Iteration 38, loss = 0.37954269\n",
      "Iteration 39, loss = 0.37413175\n",
      "Iteration 40, loss = 0.36831771\n",
      "Iteration 41, loss = 0.36307848\n",
      "Iteration 42, loss = 0.35836575\n",
      "Iteration 43, loss = 0.35339806\n",
      "Iteration 44, loss = 0.34863497\n",
      "Iteration 45, loss = 0.34393384\n",
      "Iteration 46, loss = 0.33974389\n",
      "Iteration 47, loss = 0.33562919\n",
      "Iteration 48, loss = 0.33162783\n",
      "Iteration 49, loss = 0.32773773\n",
      "Iteration 50, loss = 0.32415840\n",
      "Iteration 51, loss = 0.32051528\n",
      "Iteration 52, loss = 0.31704624\n",
      "Iteration 53, loss = 0.31361573\n",
      "Iteration 54, loss = 0.31050480\n",
      "Iteration 55, loss = 0.30753781\n",
      "Iteration 56, loss = 0.30470472\n",
      "Iteration 57, loss = 0.30186066\n",
      "Iteration 58, loss = 0.29936301\n",
      "Iteration 59, loss = 0.29669383\n",
      "Iteration 60, loss = 0.29433374\n",
      "Iteration 61, loss = 0.29212369\n",
      "Iteration 62, loss = 0.28994003\n",
      "Iteration 63, loss = 0.28777781\n",
      "Iteration 64, loss = 0.28575448\n",
      "Iteration 65, loss = 0.28389207\n",
      "Iteration 66, loss = 0.28202950\n",
      "Iteration 67, loss = 0.28029018\n",
      "Iteration 68, loss = 0.27860366\n",
      "Iteration 69, loss = 0.27693010\n",
      "Iteration 70, loss = 0.27536615\n",
      "Iteration 71, loss = 0.27377821\n",
      "Iteration 72, loss = 0.27242150\n",
      "Iteration 73, loss = 0.27091149\n",
      "Iteration 74, loss = 0.26953694\n",
      "Iteration 75, loss = 0.26811455\n",
      "Iteration 76, loss = 0.26676159\n",
      "Iteration 77, loss = 0.26541809\n",
      "Iteration 78, loss = 0.26414435\n",
      "Iteration 79, loss = 0.26290471\n",
      "Iteration 80, loss = 0.26173525\n",
      "Iteration 81, loss = 0.26053523\n",
      "Iteration 82, loss = 0.25939307\n",
      "Iteration 83, loss = 0.25832255\n",
      "Iteration 84, loss = 0.25725983\n",
      "Iteration 85, loss = 0.25616913\n",
      "Iteration 86, loss = 0.25512867\n",
      "Iteration 87, loss = 0.25417061\n",
      "Iteration 88, loss = 0.25308959\n",
      "Iteration 89, loss = 0.25214124\n",
      "Iteration 90, loss = 0.25122182\n",
      "Iteration 91, loss = 0.25029734\n",
      "Iteration 92, loss = 0.24938015\n",
      "Iteration 93, loss = 0.24844611\n",
      "Iteration 94, loss = 0.24755582\n",
      "Iteration 95, loss = 0.24670791\n",
      "Iteration 96, loss = 0.24588976\n",
      "Iteration 97, loss = 0.24514467\n",
      "Iteration 98, loss = 0.24432209\n",
      "Iteration 99, loss = 0.24353168\n",
      "Iteration 100, loss = 0.24274758\n",
      "Iteration 101, loss = 0.24201941\n",
      "Iteration 102, loss = 0.24126820\n",
      "Iteration 103, loss = 0.24051802\n",
      "Iteration 104, loss = 0.23983227\n",
      "Iteration 105, loss = 0.23903749\n",
      "Iteration 106, loss = 0.23834612\n",
      "Iteration 107, loss = 0.23765616\n",
      "Iteration 108, loss = 0.23699308\n",
      "Iteration 109, loss = 0.23630390\n",
      "Iteration 110, loss = 0.23563728\n",
      "Iteration 111, loss = 0.23497340\n",
      "Iteration 112, loss = 0.23430456\n",
      "Iteration 113, loss = 0.23369316\n",
      "Iteration 114, loss = 0.23305096\n",
      "Iteration 115, loss = 0.23244333\n",
      "Iteration 116, loss = 0.23182214\n",
      "Iteration 117, loss = 0.23115779\n",
      "Iteration 118, loss = 0.23059807\n",
      "Iteration 119, loss = 0.22997074\n",
      "Iteration 120, loss = 0.22936198\n",
      "Iteration 121, loss = 0.22877116\n",
      "Iteration 122, loss = 0.22817734\n",
      "Iteration 123, loss = 0.22760172\n",
      "Iteration 124, loss = 0.22703410\n",
      "Iteration 125, loss = 0.22647127\n",
      "Iteration 126, loss = 0.22592938\n",
      "Iteration 127, loss = 0.22534846\n",
      "Iteration 128, loss = 0.22482044\n",
      "Iteration 129, loss = 0.22433963\n",
      "Iteration 130, loss = 0.22379633\n",
      "Iteration 131, loss = 0.22324831\n",
      "Iteration 132, loss = 0.22275770\n",
      "Iteration 133, loss = 0.22225517\n",
      "Iteration 134, loss = 0.22175394\n",
      "Iteration 135, loss = 0.22124350\n",
      "Iteration 136, loss = 0.22080016\n",
      "Iteration 137, loss = 0.22030680\n",
      "Iteration 138, loss = 0.21985280\n",
      "Iteration 139, loss = 0.21933687\n",
      "Iteration 140, loss = 0.21889185\n",
      "Iteration 141, loss = 0.21843529\n",
      "Iteration 142, loss = 0.21795190\n",
      "Iteration 143, loss = 0.21751457\n",
      "Iteration 144, loss = 0.21708246\n",
      "Iteration 145, loss = 0.21663234\n",
      "Iteration 146, loss = 0.21618778\n",
      "Iteration 147, loss = 0.21573739\n",
      "Iteration 148, loss = 0.21532108\n",
      "Iteration 149, loss = 0.21487793\n",
      "Iteration 150, loss = 0.21441396\n",
      "Iteration 151, loss = 0.21397812\n",
      "Iteration 152, loss = 0.21353493\n",
      "Iteration 153, loss = 0.21310963\n",
      "Iteration 154, loss = 0.21270966\n",
      "Iteration 155, loss = 0.21224811\n",
      "Iteration 156, loss = 0.21186000\n",
      "Iteration 157, loss = 0.21142123\n",
      "Iteration 158, loss = 0.21100169\n",
      "Iteration 159, loss = 0.21064095\n",
      "Iteration 160, loss = 0.21021089\n",
      "Iteration 161, loss = 0.20981218\n",
      "Iteration 162, loss = 0.20940504\n",
      "Iteration 163, loss = 0.20903405\n",
      "Iteration 164, loss = 0.20865135\n",
      "Iteration 165, loss = 0.20828119\n",
      "Iteration 166, loss = 0.20790919\n",
      "Iteration 167, loss = 0.20752758\n",
      "Iteration 168, loss = 0.20717279\n",
      "Iteration 169, loss = 0.20682886\n",
      "Iteration 170, loss = 0.20643731\n",
      "Iteration 171, loss = 0.20609607\n",
      "Iteration 172, loss = 0.20571042\n",
      "Iteration 173, loss = 0.20537876\n",
      "Iteration 174, loss = 0.20499489\n",
      "Iteration 175, loss = 0.20467701\n",
      "Iteration 176, loss = 0.20427926\n",
      "Iteration 177, loss = 0.20394203\n",
      "Iteration 178, loss = 0.20360686\n",
      "Iteration 179, loss = 0.20324532\n",
      "Iteration 180, loss = 0.20293447\n",
      "Iteration 181, loss = 0.20256246\n",
      "Iteration 182, loss = 0.20225812\n",
      "Iteration 183, loss = 0.20190070\n",
      "Iteration 184, loss = 0.20156447\n",
      "Iteration 185, loss = 0.20126772\n",
      "Iteration 186, loss = 0.20096092\n",
      "Iteration 187, loss = 0.20061372\n",
      "Iteration 188, loss = 0.20030680\n",
      "Iteration 189, loss = 0.20001122\n",
      "Iteration 190, loss = 0.19969134\n",
      "Iteration 191, loss = 0.19939326\n",
      "Iteration 192, loss = 0.19914631\n",
      "Iteration 193, loss = 0.19879628\n",
      "Iteration 194, loss = 0.19851915\n",
      "Iteration 195, loss = 0.19820808\n",
      "Iteration 196, loss = 0.19792637\n",
      "Iteration 197, loss = 0.19764576\n",
      "Iteration 198, loss = 0.19735446\n",
      "Iteration 199, loss = 0.19706398\n",
      "Iteration 200, loss = 0.19677787\n",
      "Iteration 201, loss = 0.19650317\n",
      "Iteration 202, loss = 0.19620389\n",
      "Iteration 203, loss = 0.19592412\n",
      "Iteration 204, loss = 0.19563482\n",
      "Iteration 205, loss = 0.19534912\n",
      "Iteration 206, loss = 0.19508242\n",
      "Iteration 207, loss = 0.19481493\n",
      "Iteration 208, loss = 0.19455121\n",
      "Iteration 209, loss = 0.19428370\n",
      "Iteration 210, loss = 0.19401012\n",
      "Iteration 211, loss = 0.19374903\n",
      "Iteration 212, loss = 0.19351271\n",
      "Iteration 213, loss = 0.19321132\n",
      "Iteration 214, loss = 0.19295425\n",
      "Iteration 215, loss = 0.19269781\n",
      "Iteration 216, loss = 0.19240167\n",
      "Iteration 217, loss = 0.19215907\n",
      "Iteration 218, loss = 0.19191171\n",
      "Iteration 219, loss = 0.19168874\n",
      "Iteration 220, loss = 0.19141066\n",
      "Iteration 221, loss = 0.19116141\n",
      "Iteration 222, loss = 0.19089445\n",
      "Iteration 223, loss = 0.19065312\n",
      "Iteration 224, loss = 0.19040892\n",
      "Iteration 225, loss = 0.19017790\n",
      "Iteration 226, loss = 0.18992476\n",
      "Iteration 227, loss = 0.18969040\n",
      "Iteration 228, loss = 0.18947924\n",
      "Iteration 229, loss = 0.18923977\n",
      "Iteration 230, loss = 0.18901571\n",
      "Iteration 231, loss = 0.18877499\n",
      "Iteration 232, loss = 0.18854485\n",
      "Iteration 233, loss = 0.18832445\n",
      "Iteration 234, loss = 0.18811420\n",
      "Iteration 235, loss = 0.18787958\n",
      "Iteration 236, loss = 0.18767704\n",
      "Iteration 237, loss = 0.18743813\n",
      "Iteration 238, loss = 0.18724137\n",
      "Iteration 239, loss = 0.18699991\n",
      "Iteration 240, loss = 0.18678330\n",
      "Iteration 241, loss = 0.18659432\n",
      "Iteration 242, loss = 0.18640506\n",
      "Iteration 243, loss = 0.18617105\n",
      "Iteration 244, loss = 0.18597985\n",
      "Iteration 245, loss = 0.18574624\n",
      "Iteration 246, loss = 0.18555429\n",
      "Iteration 247, loss = 0.18533130\n",
      "Iteration 248, loss = 0.18513700\n",
      "Iteration 249, loss = 0.18494154\n",
      "Iteration 250, loss = 0.18472881\n",
      "Iteration 251, loss = 0.18452627\n",
      "Iteration 252, loss = 0.18434160\n",
      "Iteration 253, loss = 0.18414535\n",
      "Iteration 254, loss = 0.18393691\n",
      "Iteration 255, loss = 0.18373881\n",
      "Iteration 256, loss = 0.18356476\n",
      "Iteration 257, loss = 0.18341047\n",
      "Iteration 258, loss = 0.18317631\n",
      "Iteration 259, loss = 0.18297658\n",
      "Iteration 260, loss = 0.18278767\n",
      "Iteration 261, loss = 0.18258920\n",
      "Iteration 262, loss = 0.18238383\n",
      "Iteration 263, loss = 0.18221292\n",
      "Iteration 264, loss = 0.18200757\n",
      "Iteration 265, loss = 0.18178557\n",
      "Iteration 266, loss = 0.18163452\n",
      "Iteration 267, loss = 0.18142702\n",
      "Iteration 268, loss = 0.18120610\n",
      "Iteration 269, loss = 0.18102645\n",
      "Iteration 270, loss = 0.18086242\n",
      "Iteration 271, loss = 0.18064679\n",
      "Iteration 272, loss = 0.18047804\n",
      "Iteration 273, loss = 0.18028194\n",
      "Iteration 274, loss = 0.18010016\n",
      "Iteration 275, loss = 0.17991583\n",
      "Iteration 276, loss = 0.17973311\n",
      "Iteration 277, loss = 0.17955054\n",
      "Iteration 278, loss = 0.17939339\n",
      "Iteration 279, loss = 0.17921329\n",
      "Iteration 280, loss = 0.17903265\n",
      "Iteration 281, loss = 0.17887058\n",
      "Iteration 282, loss = 0.17868925\n",
      "Iteration 283, loss = 0.17854579\n",
      "Iteration 284, loss = 0.17834984\n",
      "Iteration 285, loss = 0.17818512\n",
      "Iteration 286, loss = 0.17800291\n",
      "Iteration 287, loss = 0.17785744\n",
      "Iteration 288, loss = 0.17769856\n",
      "Iteration 289, loss = 0.17751381\n",
      "Iteration 290, loss = 0.17734740\n",
      "Iteration 291, loss = 0.17719971\n",
      "Iteration 292, loss = 0.17704171\n",
      "Iteration 293, loss = 0.17689273\n",
      "Iteration 294, loss = 0.17672592\n",
      "Iteration 295, loss = 0.17656592\n",
      "Iteration 296, loss = 0.17639432\n",
      "Iteration 297, loss = 0.17623234\n",
      "Iteration 298, loss = 0.17608863\n",
      "Iteration 299, loss = 0.17594193\n",
      "Iteration 300, loss = 0.17580243\n",
      "Iteration 301, loss = 0.17562260\n",
      "Iteration 302, loss = 0.17547805\n",
      "Iteration 303, loss = 0.17534808\n",
      "Iteration 304, loss = 0.17519606\n",
      "Iteration 305, loss = 0.17503060\n",
      "Iteration 306, loss = 0.17487249\n",
      "Iteration 307, loss = 0.17473692\n",
      "Iteration 308, loss = 0.17459204\n",
      "Iteration 309, loss = 0.17445020\n",
      "Iteration 310, loss = 0.17432694\n",
      "Iteration 311, loss = 0.17417165\n",
      "Iteration 312, loss = 0.17404702\n",
      "Iteration 313, loss = 0.17391258\n",
      "Iteration 314, loss = 0.17376056\n",
      "Iteration 315, loss = 0.17363534\n",
      "Iteration 316, loss = 0.17352018\n",
      "Iteration 317, loss = 0.17337994\n",
      "Iteration 318, loss = 0.17323674\n",
      "Iteration 319, loss = 0.17311965\n",
      "Iteration 320, loss = 0.17298569\n",
      "Iteration 321, loss = 0.17285626\n",
      "Iteration 322, loss = 0.17273261\n",
      "Iteration 323, loss = 0.17263402\n",
      "Iteration 324, loss = 0.17250210\n",
      "Iteration 325, loss = 0.17238164\n",
      "Iteration 326, loss = 0.17223072\n",
      "Iteration 327, loss = 0.17210764\n",
      "Iteration 328, loss = 0.17199222\n",
      "Iteration 329, loss = 0.17188725\n",
      "Iteration 330, loss = 0.17175566\n",
      "Iteration 331, loss = 0.17163411\n",
      "Iteration 332, loss = 0.17149793\n",
      "Iteration 333, loss = 0.17139038\n",
      "Iteration 334, loss = 0.17126470\n",
      "Iteration 335, loss = 0.17113818\n",
      "Iteration 336, loss = 0.17105635\n",
      "Iteration 337, loss = 0.17088759\n",
      "Iteration 338, loss = 0.17077895\n",
      "Iteration 339, loss = 0.17066507\n",
      "Iteration 340, loss = 0.17053378\n",
      "Iteration 341, loss = 0.17043228\n",
      "Iteration 342, loss = 0.17030803\n",
      "Iteration 343, loss = 0.17019996\n",
      "Iteration 344, loss = 0.17008980\n",
      "Iteration 345, loss = 0.16998490\n",
      "Iteration 346, loss = 0.16986760\n",
      "Iteration 347, loss = 0.16975460\n",
      "Iteration 348, loss = 0.16965231\n",
      "Iteration 349, loss = 0.16954131\n",
      "Iteration 350, loss = 0.16943068\n",
      "Iteration 351, loss = 0.16932050\n",
      "Iteration 352, loss = 0.16921165\n",
      "Iteration 353, loss = 0.16909486\n",
      "Iteration 354, loss = 0.16899692\n",
      "Iteration 355, loss = 0.16892010\n",
      "Iteration 356, loss = 0.16878396\n",
      "Iteration 357, loss = 0.16867672\n",
      "Iteration 358, loss = 0.16856587\n",
      "Iteration 359, loss = 0.16847817\n",
      "Iteration 360, loss = 0.16835599\n",
      "Iteration 361, loss = 0.16824807\n",
      "Iteration 362, loss = 0.16815210\n",
      "Iteration 363, loss = 0.16805044\n",
      "Iteration 364, loss = 0.16796492\n",
      "Iteration 365, loss = 0.16785403\n",
      "Iteration 366, loss = 0.16773490\n",
      "Iteration 367, loss = 0.16764314\n",
      "Iteration 368, loss = 0.16753058\n",
      "Iteration 369, loss = 0.16743139\n",
      "Iteration 370, loss = 0.16735144\n",
      "Iteration 371, loss = 0.16724109\n",
      "Iteration 372, loss = 0.16713353\n",
      "Iteration 373, loss = 0.16704219\n",
      "Iteration 374, loss = 0.16693012\n",
      "Iteration 375, loss = 0.16682979\n",
      "Iteration 376, loss = 0.16674711\n",
      "Iteration 377, loss = 0.16664591\n",
      "Iteration 378, loss = 0.16656802\n",
      "Iteration 379, loss = 0.16644564\n",
      "Iteration 380, loss = 0.16634971\n",
      "Iteration 381, loss = 0.16629284\n",
      "Iteration 382, loss = 0.16617550\n",
      "Iteration 383, loss = 0.16609560\n",
      "Iteration 384, loss = 0.16598858\n",
      "Iteration 385, loss = 0.16588782\n",
      "Iteration 386, loss = 0.16581640\n",
      "Iteration 387, loss = 0.16571075\n",
      "Iteration 388, loss = 0.16561531\n",
      "Iteration 389, loss = 0.16552586\n",
      "Iteration 390, loss = 0.16543840\n",
      "Iteration 391, loss = 0.16536263\n",
      "Iteration 392, loss = 0.16526630\n",
      "Iteration 393, loss = 0.16516532\n",
      "Iteration 394, loss = 0.16509383\n",
      "Iteration 395, loss = 0.16499728\n",
      "Iteration 396, loss = 0.16491516\n",
      "Iteration 397, loss = 0.16482329\n",
      "Iteration 398, loss = 0.16474046\n",
      "Iteration 399, loss = 0.16467082\n",
      "Iteration 400, loss = 0.16459090\n",
      "Iteration 401, loss = 0.16453384\n",
      "Iteration 402, loss = 0.16443499\n",
      "Iteration 403, loss = 0.16434589\n",
      "Iteration 404, loss = 0.16424848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.51235966\n",
      "Iteration 2, loss = 0.30734451\n",
      "Iteration 3, loss = 0.28828085\n",
      "Iteration 4, loss = 0.27827808\n",
      "Iteration 5, loss = 0.23197072\n",
      "Iteration 6, loss = 0.21995211\n",
      "Iteration 7, loss = 0.19813238\n",
      "Iteration 8, loss = 0.18214051\n",
      "Iteration 9, loss = 0.17425419\n",
      "Iteration 10, loss = 0.17091639\n",
      "Iteration 11, loss = 0.16526138\n",
      "Iteration 12, loss = 0.16484102\n",
      "Iteration 13, loss = 0.16303087\n",
      "Iteration 14, loss = 0.16285057\n",
      "Iteration 15, loss = 0.16429825\n",
      "Iteration 16, loss = 0.16414167\n",
      "Iteration 17, loss = 0.16303072\n",
      "Iteration 18, loss = 0.16580501\n",
      "Iteration 19, loss = 0.16084168\n",
      "Iteration 20, loss = 0.16537693\n",
      "Iteration 21, loss = 0.16032357\n",
      "Iteration 22, loss = 0.16189939\n",
      "Iteration 23, loss = 0.16325672\n",
      "Iteration 24, loss = 0.17769800\n",
      "Iteration 25, loss = 0.17314003\n",
      "Iteration 26, loss = 0.17199476\n",
      "Iteration 27, loss = 0.16951635\n",
      "Iteration 28, loss = 0.16171786\n",
      "Iteration 29, loss = 0.16578881\n",
      "Iteration 30, loss = 0.16699330\n",
      "Iteration 31, loss = 0.16268569\n",
      "Iteration 32, loss = 0.16145513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.51335401\n",
      "Iteration 2, loss = 0.32554364\n",
      "Iteration 3, loss = 0.29996423\n",
      "Iteration 4, loss = 0.28034941\n",
      "Iteration 5, loss = 0.23677602\n",
      "Iteration 6, loss = 0.23148408\n",
      "Iteration 7, loss = 0.20586732\n",
      "Iteration 8, loss = 0.18830894\n",
      "Iteration 9, loss = 0.17808973\n",
      "Iteration 10, loss = 0.17133591\n",
      "Iteration 11, loss = 0.16710894\n",
      "Iteration 12, loss = 0.16378576\n",
      "Iteration 13, loss = 0.16281034\n",
      "Iteration 14, loss = 0.16236305\n",
      "Iteration 15, loss = 0.16103934\n",
      "Iteration 16, loss = 0.16165559\n",
      "Iteration 17, loss = 0.16161951\n",
      "Iteration 18, loss = 0.16353768\n",
      "Iteration 19, loss = 0.15968177\n",
      "Iteration 20, loss = 0.16433337\n",
      "Iteration 21, loss = 0.15974362\n",
      "Iteration 22, loss = 0.16412819\n",
      "Iteration 23, loss = 0.16412681\n",
      "Iteration 24, loss = 0.18611487\n",
      "Iteration 25, loss = 0.17429968\n",
      "Iteration 26, loss = 0.17696293\n",
      "Iteration 27, loss = 0.17216597\n",
      "Iteration 28, loss = 0.16352310\n",
      "Iteration 29, loss = 0.16964920\n",
      "Iteration 30, loss = 0.16670249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.51785258\n",
      "Iteration 2, loss = 0.33849043\n",
      "Iteration 3, loss = 0.30637650\n",
      "Iteration 4, loss = 0.28056466\n",
      "Iteration 5, loss = 0.23670742\n",
      "Iteration 6, loss = 0.22761852\n",
      "Iteration 7, loss = 0.20167954\n",
      "Iteration 8, loss = 0.18884188\n",
      "Iteration 9, loss = 0.17970775\n",
      "Iteration 10, loss = 0.17294116\n",
      "Iteration 11, loss = 0.16870106\n",
      "Iteration 12, loss = 0.16604410\n",
      "Iteration 13, loss = 0.16625366\n",
      "Iteration 14, loss = 0.16398497\n",
      "Iteration 15, loss = 0.16536730\n",
      "Iteration 16, loss = 0.16766604\n",
      "Iteration 17, loss = 0.16261383\n",
      "Iteration 18, loss = 0.16825876\n",
      "Iteration 19, loss = 0.16390483\n",
      "Iteration 20, loss = 0.16770863\n",
      "Iteration 21, loss = 0.16120178\n",
      "Iteration 22, loss = 0.16384996\n",
      "Iteration 23, loss = 0.16202220\n",
      "Iteration 24, loss = 0.17279693\n",
      "Iteration 25, loss = 0.17298118\n",
      "Iteration 26, loss = 0.16950918\n",
      "Iteration 27, loss = 0.16738398\n",
      "Iteration 28, loss = 0.16445854\n",
      "Iteration 29, loss = 0.16587361\n",
      "Iteration 30, loss = 0.16420381\n",
      "Iteration 31, loss = 0.17034208\n",
      "Iteration 32, loss = 0.16297320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.pdf\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "# https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(14,)] , #[(14,),(100,),(14,4),(7,7,)] \n",
    "    'mlp__activation' : ['relu'], #['logistic','tanh','relu']\n",
    "    'mlp__solver' : ['adam'],\n",
    "    'mlp__alpha' : [1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , *1\n",
    "    'mlp__batch_size' : [99], # ~297 samples for train at inner cv -> 3 equal size batches , #[16,32,64,128,'auto']\n",
    "    'mlp__learning_rate' : ['constant'],\n",
    "    'mlp__learning_rate_init' : [0.1,0.01,0.001],\n",
    "    'mlp__max_iter' : [1000],\n",
    "    'mlp__verbose' : [1]\n",
    "}\n",
    "\n",
    "search_adam = GridSearchCV(clf_mlp, param_grid=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro')\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_adam, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.947     0.929     0.938       212\n",
      "   Malignant      0.931     0.948     0.939       212\n",
      "\n",
      "    accuracy                          0.939       424\n",
      "   macro avg      0.939     0.939     0.939       424\n",
      "weighted avg      0.939     0.939     0.939       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Solver : SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96504635\n",
      "Iteration 2, loss = 0.83082053\n",
      "Iteration 3, loss = 0.69977523\n",
      "Iteration 4, loss = 0.59486374\n",
      "Iteration 5, loss = 0.51999206\n",
      "Iteration 6, loss = 0.46357253\n",
      "Iteration 7, loss = 0.42174088\n",
      "Iteration 8, loss = 0.38875232\n",
      "Iteration 9, loss = 0.36269612\n",
      "Iteration 10, loss = 0.34138982\n",
      "Iteration 11, loss = 0.32357936\n",
      "Iteration 12, loss = 0.30865194\n",
      "Iteration 13, loss = 0.29560771\n",
      "Iteration 14, loss = 0.28453366\n",
      "Iteration 15, loss = 0.27454853\n",
      "Iteration 16, loss = 0.26589968\n",
      "Iteration 17, loss = 0.25846545\n",
      "Iteration 18, loss = 0.25158269\n",
      "Iteration 19, loss = 0.24571987\n",
      "Iteration 20, loss = 0.24016692\n",
      "Iteration 21, loss = 0.23536054\n",
      "Iteration 22, loss = 0.23094719\n",
      "Iteration 23, loss = 0.22693350\n",
      "Iteration 24, loss = 0.22322746\n",
      "Iteration 25, loss = 0.21982446\n",
      "Iteration 26, loss = 0.21670075\n",
      "Iteration 27, loss = 0.21376624\n",
      "Iteration 28, loss = 0.21123841\n",
      "Iteration 29, loss = 0.20864644\n",
      "Iteration 30, loss = 0.20625379\n",
      "Iteration 31, loss = 0.20414602\n",
      "Iteration 32, loss = 0.20200335\n",
      "Iteration 33, loss = 0.20008378\n",
      "Iteration 34, loss = 0.19829312\n",
      "Iteration 35, loss = 0.19651847\n",
      "Iteration 36, loss = 0.19487652\n",
      "Iteration 37, loss = 0.19335023\n",
      "Iteration 38, loss = 0.19184136\n",
      "Iteration 39, loss = 0.19038179\n",
      "Iteration 40, loss = 0.18908023\n",
      "Iteration 41, loss = 0.18786731\n",
      "Iteration 42, loss = 0.18657311\n",
      "Iteration 43, loss = 0.18534487\n",
      "Iteration 44, loss = 0.18413478\n",
      "Iteration 45, loss = 0.18304065\n",
      "Iteration 46, loss = 0.18195202\n",
      "Iteration 47, loss = 0.18092310\n",
      "Iteration 48, loss = 0.17992135\n",
      "Iteration 49, loss = 0.17899660\n",
      "Iteration 50, loss = 0.17792115\n",
      "Iteration 51, loss = 0.17706431\n",
      "Iteration 52, loss = 0.17616991\n",
      "Iteration 53, loss = 0.17524971\n",
      "Iteration 54, loss = 0.17441228\n",
      "Iteration 55, loss = 0.17361397\n",
      "Iteration 56, loss = 0.17281935\n",
      "Iteration 57, loss = 0.17208628\n",
      "Iteration 58, loss = 0.17133898\n",
      "Iteration 59, loss = 0.17072427\n",
      "Iteration 60, loss = 0.16997791\n",
      "Iteration 61, loss = 0.16922945\n",
      "Iteration 62, loss = 0.16860904\n",
      "Iteration 63, loss = 0.16793461\n",
      "Iteration 64, loss = 0.16736487\n",
      "Iteration 65, loss = 0.16668419\n",
      "Iteration 66, loss = 0.16612757\n",
      "Iteration 67, loss = 0.16551498\n",
      "Iteration 68, loss = 0.16489536\n",
      "Iteration 69, loss = 0.16431977\n",
      "Iteration 70, loss = 0.16380080\n",
      "Iteration 71, loss = 0.16327502\n",
      "Iteration 72, loss = 0.16269966\n",
      "Iteration 73, loss = 0.16215006\n",
      "Iteration 74, loss = 0.16160674\n",
      "Iteration 75, loss = 0.16111178\n",
      "Iteration 76, loss = 0.16064412\n",
      "Iteration 77, loss = 0.16012974\n",
      "Iteration 78, loss = 0.15968110\n",
      "Iteration 79, loss = 0.15919030\n",
      "Iteration 80, loss = 0.15880171\n",
      "Iteration 81, loss = 0.15826565\n",
      "Iteration 82, loss = 0.15782846\n",
      "Iteration 83, loss = 0.15748626\n",
      "Iteration 84, loss = 0.15704402\n",
      "Iteration 85, loss = 0.15657813\n",
      "Iteration 86, loss = 0.15616902\n",
      "Iteration 87, loss = 0.15574666\n",
      "Iteration 88, loss = 0.15537563\n",
      "Iteration 89, loss = 0.15497780\n",
      "Iteration 90, loss = 0.15462317\n",
      "Iteration 91, loss = 0.15422653\n",
      "Iteration 92, loss = 0.15385835\n",
      "Iteration 93, loss = 0.15348160\n",
      "Iteration 94, loss = 0.15314621\n",
      "Iteration 95, loss = 0.15276092\n",
      "Iteration 96, loss = 0.15242175\n",
      "Iteration 97, loss = 0.15205464\n",
      "Iteration 98, loss = 0.15172036\n",
      "Iteration 99, loss = 0.15139458\n",
      "Iteration 100, loss = 0.15112450\n",
      "Iteration 101, loss = 0.15076126\n",
      "Iteration 102, loss = 0.15045517\n",
      "Iteration 103, loss = 0.15015171\n",
      "Iteration 104, loss = 0.14993827\n",
      "Iteration 105, loss = 0.14963377\n",
      "Iteration 106, loss = 0.14936972\n",
      "Iteration 107, loss = 0.14903006\n",
      "Iteration 108, loss = 0.14879840\n",
      "Iteration 109, loss = 0.14853895\n",
      "Iteration 110, loss = 0.14827662\n",
      "Iteration 111, loss = 0.14800387\n",
      "Iteration 112, loss = 0.14776928\n",
      "Iteration 113, loss = 0.14754239\n",
      "Iteration 114, loss = 0.14725391\n",
      "Iteration 115, loss = 0.14702023\n",
      "Iteration 116, loss = 0.14677925\n",
      "Iteration 117, loss = 0.14651226\n",
      "Iteration 118, loss = 0.14624764\n",
      "Iteration 119, loss = 0.14598277\n",
      "Iteration 120, loss = 0.14575841\n",
      "Iteration 121, loss = 0.14552471\n",
      "Iteration 122, loss = 0.14530814\n",
      "Iteration 123, loss = 0.14509553\n",
      "Iteration 124, loss = 0.14492258\n",
      "Iteration 125, loss = 0.14463359\n",
      "Iteration 126, loss = 0.14441043\n",
      "Iteration 127, loss = 0.14422174\n",
      "Iteration 128, loss = 0.14404538\n",
      "Iteration 129, loss = 0.14381693\n",
      "Iteration 130, loss = 0.14352668\n",
      "Iteration 131, loss = 0.14336098\n",
      "Iteration 132, loss = 0.14315582\n",
      "Iteration 133, loss = 0.14293857\n",
      "Iteration 134, loss = 0.14275243\n",
      "Iteration 135, loss = 0.14250429\n",
      "Iteration 136, loss = 0.14230087\n",
      "Iteration 137, loss = 0.14216301\n",
      "Iteration 138, loss = 0.14198351\n",
      "Iteration 139, loss = 0.14175256\n",
      "Iteration 140, loss = 0.14161988\n",
      "Iteration 141, loss = 0.14139884\n",
      "Iteration 142, loss = 0.14115296\n",
      "Iteration 143, loss = 0.14098028\n",
      "Iteration 144, loss = 0.14083195\n",
      "Iteration 145, loss = 0.14060665\n",
      "Iteration 146, loss = 0.14044959\n",
      "Iteration 147, loss = 0.14025743\n",
      "Iteration 148, loss = 0.14011420\n",
      "Iteration 149, loss = 0.13994111\n",
      "Iteration 150, loss = 0.13972403\n",
      "Iteration 151, loss = 0.13964465\n",
      "Iteration 152, loss = 0.13938847\n",
      "Iteration 153, loss = 0.13923269\n",
      "Iteration 154, loss = 0.13907182\n",
      "Iteration 155, loss = 0.13898433\n",
      "Iteration 156, loss = 0.13874785\n",
      "Iteration 157, loss = 0.13863256\n",
      "Iteration 158, loss = 0.13848454\n",
      "Iteration 159, loss = 0.13826102\n",
      "Iteration 160, loss = 0.13811386\n",
      "Iteration 161, loss = 0.13794502\n",
      "Iteration 162, loss = 0.13774317\n",
      "Iteration 163, loss = 0.13759697\n",
      "Iteration 164, loss = 0.13751961\n",
      "Iteration 165, loss = 0.13733451\n",
      "Iteration 166, loss = 0.13713120\n",
      "Iteration 167, loss = 0.13699166\n",
      "Iteration 168, loss = 0.13688686\n",
      "Iteration 169, loss = 0.13668133\n",
      "Iteration 170, loss = 0.13660187\n",
      "Iteration 171, loss = 0.13641246\n",
      "Iteration 172, loss = 0.13626716\n",
      "Iteration 173, loss = 0.13612323\n",
      "Iteration 174, loss = 0.13599584\n",
      "Iteration 175, loss = 0.13583603\n",
      "Iteration 176, loss = 0.13572474\n",
      "Iteration 177, loss = 0.13553266\n",
      "Iteration 178, loss = 0.13537094\n",
      "Iteration 179, loss = 0.13526191\n",
      "Iteration 180, loss = 0.13504739\n",
      "Iteration 181, loss = 0.13506207\n",
      "Iteration 182, loss = 0.13476879\n",
      "Iteration 183, loss = 0.13464425\n",
      "Iteration 184, loss = 0.13449294\n",
      "Iteration 185, loss = 0.13434476\n",
      "Iteration 186, loss = 0.13429501\n",
      "Iteration 187, loss = 0.13403552\n",
      "Iteration 188, loss = 0.13391860\n",
      "Iteration 189, loss = 0.13374523\n",
      "Iteration 190, loss = 0.13358163\n",
      "Iteration 191, loss = 0.13346177\n",
      "Iteration 192, loss = 0.13335207\n",
      "Iteration 193, loss = 0.13318559\n",
      "Iteration 194, loss = 0.13308252\n",
      "Iteration 195, loss = 0.13289301\n",
      "Iteration 196, loss = 0.13273673\n",
      "Iteration 197, loss = 0.13264643\n",
      "Iteration 198, loss = 0.13245447\n",
      "Iteration 199, loss = 0.13232278\n",
      "Iteration 200, loss = 0.13227127\n",
      "Iteration 201, loss = 0.13203987\n",
      "Iteration 202, loss = 0.13190424\n",
      "Iteration 203, loss = 0.13180243\n",
      "Iteration 204, loss = 0.13165872\n",
      "Iteration 205, loss = 0.13151068\n",
      "Iteration 206, loss = 0.13139450\n",
      "Iteration 207, loss = 0.13123257\n",
      "Iteration 208, loss = 0.13120157\n",
      "Iteration 209, loss = 0.13098714\n",
      "Iteration 210, loss = 0.13092710\n",
      "Iteration 211, loss = 0.13073023\n",
      "Iteration 212, loss = 0.13067803\n",
      "Iteration 213, loss = 0.13053223\n",
      "Iteration 214, loss = 0.13038154\n",
      "Iteration 215, loss = 0.13024037\n",
      "Iteration 216, loss = 0.13014692\n",
      "Iteration 217, loss = 0.12998576\n",
      "Iteration 218, loss = 0.12990037\n",
      "Iteration 219, loss = 0.12968799\n",
      "Iteration 220, loss = 0.12960192\n",
      "Iteration 221, loss = 0.12945645\n",
      "Iteration 222, loss = 0.12931252\n",
      "Iteration 223, loss = 0.12920664\n",
      "Iteration 224, loss = 0.12907870\n",
      "Iteration 225, loss = 0.12898985\n",
      "Iteration 226, loss = 0.12890964\n",
      "Iteration 227, loss = 0.12870073\n",
      "Iteration 228, loss = 0.12863127\n",
      "Iteration 229, loss = 0.12854361\n",
      "Iteration 230, loss = 0.12841903\n",
      "Iteration 231, loss = 0.12827109\n",
      "Iteration 232, loss = 0.12816555\n",
      "Iteration 233, loss = 0.12805716\n",
      "Iteration 234, loss = 0.12793932\n",
      "Iteration 235, loss = 0.12786890\n",
      "Iteration 236, loss = 0.12769980\n",
      "Iteration 237, loss = 0.12770484\n",
      "Iteration 238, loss = 0.12752394\n",
      "Iteration 239, loss = 0.12741442\n",
      "Iteration 240, loss = 0.12729081\n",
      "Iteration 241, loss = 0.12719299\n",
      "Iteration 242, loss = 0.12715349\n",
      "Iteration 243, loss = 0.12697498\n",
      "Iteration 244, loss = 0.12690063\n",
      "Iteration 245, loss = 0.12676120\n",
      "Iteration 246, loss = 0.12667722\n",
      "Iteration 247, loss = 0.12659610\n",
      "Iteration 248, loss = 0.12644525\n",
      "Iteration 249, loss = 0.12633588\n",
      "Iteration 250, loss = 0.12625176\n",
      "Iteration 251, loss = 0.12615912\n",
      "Iteration 252, loss = 0.12612977\n",
      "Iteration 253, loss = 0.12593847\n",
      "Iteration 254, loss = 0.12587328\n",
      "Iteration 255, loss = 0.12573477\n",
      "Iteration 256, loss = 0.12563124\n",
      "Iteration 257, loss = 0.12552536\n",
      "Iteration 258, loss = 0.12542691\n",
      "Iteration 259, loss = 0.12533998\n",
      "Iteration 260, loss = 0.12525664\n",
      "Iteration 261, loss = 0.12508135\n",
      "Iteration 262, loss = 0.12500698\n",
      "Iteration 263, loss = 0.12495406\n",
      "Iteration 264, loss = 0.12478590\n",
      "Iteration 265, loss = 0.12478352\n",
      "Iteration 266, loss = 0.12455165\n",
      "Iteration 267, loss = 0.12452371\n",
      "Iteration 268, loss = 0.12437294\n",
      "Iteration 269, loss = 0.12425654\n",
      "Iteration 270, loss = 0.12418766\n",
      "Iteration 271, loss = 0.12405609\n",
      "Iteration 272, loss = 0.12400507\n",
      "Iteration 273, loss = 0.12387981\n",
      "Iteration 274, loss = 0.12384639\n",
      "Iteration 275, loss = 0.12368577\n",
      "Iteration 276, loss = 0.12364408\n",
      "Iteration 277, loss = 0.12348506\n",
      "Iteration 278, loss = 0.12342150\n",
      "Iteration 279, loss = 0.12332904\n",
      "Iteration 280, loss = 0.12322411\n",
      "Iteration 281, loss = 0.12310148\n",
      "Iteration 282, loss = 0.12303671\n",
      "Iteration 283, loss = 0.12297550\n",
      "Iteration 284, loss = 0.12280972\n",
      "Iteration 285, loss = 0.12276333\n",
      "Iteration 286, loss = 0.12263180\n",
      "Iteration 287, loss = 0.12253867\n",
      "Iteration 288, loss = 0.12247577\n",
      "Iteration 289, loss = 0.12236860\n",
      "Iteration 290, loss = 0.12227148\n",
      "Iteration 291, loss = 0.12216681\n",
      "Iteration 292, loss = 0.12217971\n",
      "Iteration 293, loss = 0.12213905\n",
      "Iteration 294, loss = 0.12189955\n",
      "Iteration 295, loss = 0.12179223\n",
      "Iteration 296, loss = 0.12175250\n",
      "Iteration 297, loss = 0.12164783\n",
      "Iteration 298, loss = 0.12152613\n",
      "Iteration 299, loss = 0.12149102\n",
      "Iteration 300, loss = 0.12139968\n",
      "Iteration 301, loss = 0.12128899\n",
      "Iteration 302, loss = 0.12120600\n",
      "Iteration 303, loss = 0.12107640\n",
      "Iteration 304, loss = 0.12101257\n",
      "Iteration 305, loss = 0.12090681\n",
      "Iteration 306, loss = 0.12081406\n",
      "Iteration 307, loss = 0.12072922\n",
      "Iteration 308, loss = 0.12064143\n",
      "Iteration 309, loss = 0.12057142\n",
      "Iteration 310, loss = 0.12045347\n",
      "Iteration 311, loss = 0.12035734\n",
      "Iteration 312, loss = 0.12031345\n",
      "Iteration 313, loss = 0.12020646\n",
      "Iteration 314, loss = 0.12008318\n",
      "Iteration 315, loss = 0.12006499\n",
      "Iteration 316, loss = 0.11989275\n",
      "Iteration 317, loss = 0.11980236\n",
      "Iteration 318, loss = 0.11981548\n",
      "Iteration 319, loss = 0.11963569\n",
      "Iteration 320, loss = 0.11956496\n",
      "Iteration 321, loss = 0.11945077\n",
      "Iteration 322, loss = 0.11934785\n",
      "Iteration 323, loss = 0.11927318\n",
      "Iteration 324, loss = 0.11916938\n",
      "Iteration 325, loss = 0.11906536\n",
      "Iteration 326, loss = 0.11901317\n",
      "Iteration 327, loss = 0.11893116\n",
      "Iteration 328, loss = 0.11889413\n",
      "Iteration 329, loss = 0.11871116\n",
      "Iteration 330, loss = 0.11866543\n",
      "Iteration 331, loss = 0.11856095\n",
      "Iteration 332, loss = 0.11844777\n",
      "Iteration 333, loss = 0.11842158\n",
      "Iteration 334, loss = 0.11827342\n",
      "Iteration 335, loss = 0.11820402\n",
      "Iteration 336, loss = 0.11807604\n",
      "Iteration 337, loss = 0.11797855\n",
      "Iteration 338, loss = 0.11796800\n",
      "Iteration 339, loss = 0.11784557\n",
      "Iteration 340, loss = 0.11777181\n",
      "Iteration 341, loss = 0.11764181\n",
      "Iteration 342, loss = 0.11759669\n",
      "Iteration 343, loss = 0.11745495\n",
      "Iteration 344, loss = 0.11739118\n",
      "Iteration 345, loss = 0.11733360\n",
      "Iteration 346, loss = 0.11725881\n",
      "Iteration 347, loss = 0.11716677\n",
      "Iteration 348, loss = 0.11715602\n",
      "Iteration 349, loss = 0.11700462\n",
      "Iteration 350, loss = 0.11687331\n",
      "Iteration 351, loss = 0.11679680\n",
      "Iteration 352, loss = 0.11672152\n",
      "Iteration 353, loss = 0.11667391\n",
      "Iteration 354, loss = 0.11654427\n",
      "Iteration 355, loss = 0.11651405\n",
      "Iteration 356, loss = 0.11637319\n",
      "Iteration 357, loss = 0.11632584\n",
      "Iteration 358, loss = 0.11629552\n",
      "Iteration 359, loss = 0.11616384\n",
      "Iteration 360, loss = 0.11606251\n",
      "Iteration 361, loss = 0.11595470\n",
      "Iteration 362, loss = 0.11588022\n",
      "Iteration 363, loss = 0.11580031\n",
      "Iteration 364, loss = 0.11580687\n",
      "Iteration 365, loss = 0.11563823\n",
      "Iteration 366, loss = 0.11556195\n",
      "Iteration 367, loss = 0.11547338\n",
      "Iteration 368, loss = 0.11541710\n",
      "Iteration 369, loss = 0.11533217\n",
      "Iteration 370, loss = 0.11527402\n",
      "Iteration 371, loss = 0.11521389\n",
      "Iteration 372, loss = 0.11511527\n",
      "Iteration 373, loss = 0.11502388\n",
      "Iteration 374, loss = 0.11496975\n",
      "Iteration 375, loss = 0.11498090\n",
      "Iteration 376, loss = 0.11485554\n",
      "Iteration 377, loss = 0.11476716\n",
      "Iteration 378, loss = 0.11467796\n",
      "Iteration 379, loss = 0.11459173\n",
      "Iteration 380, loss = 0.11452261\n",
      "Iteration 381, loss = 0.11452144\n",
      "Iteration 382, loss = 0.11442472\n",
      "Iteration 383, loss = 0.11435427\n",
      "Iteration 384, loss = 0.11432876\n",
      "Iteration 385, loss = 0.11425564\n",
      "Iteration 386, loss = 0.11412959\n",
      "Iteration 387, loss = 0.11410052\n",
      "Iteration 388, loss = 0.11403184\n",
      "Iteration 389, loss = 0.11395731\n",
      "Iteration 390, loss = 0.11390565\n",
      "Iteration 391, loss = 0.11400774\n",
      "Iteration 392, loss = 0.11381322\n",
      "Iteration 393, loss = 0.11369281\n",
      "Iteration 394, loss = 0.11364909\n",
      "Iteration 395, loss = 0.11354988\n",
      "Iteration 396, loss = 0.11354467\n",
      "Iteration 397, loss = 0.11349776\n",
      "Iteration 398, loss = 0.11334910\n",
      "Iteration 399, loss = 0.11324148\n",
      "Iteration 400, loss = 0.11317193\n",
      "Iteration 401, loss = 0.11319416\n",
      "Iteration 402, loss = 0.11306164\n",
      "Iteration 403, loss = 0.11296558\n",
      "Iteration 404, loss = 0.11293276\n",
      "Iteration 405, loss = 0.11281870\n",
      "Iteration 406, loss = 0.11276843\n",
      "Iteration 407, loss = 0.11271463\n",
      "Iteration 408, loss = 0.11263892\n",
      "Iteration 409, loss = 0.11254524\n",
      "Iteration 410, loss = 0.11250094\n",
      "Iteration 411, loss = 0.11239953\n",
      "Iteration 412, loss = 0.11240980\n",
      "Iteration 413, loss = 0.11229984\n",
      "Iteration 414, loss = 0.11223097\n",
      "Iteration 415, loss = 0.11214559\n",
      "Iteration 416, loss = 0.11208765\n",
      "Iteration 417, loss = 0.11209085\n",
      "Iteration 418, loss = 0.11193730\n",
      "Iteration 419, loss = 0.11185651\n",
      "Iteration 420, loss = 0.11182455\n",
      "Iteration 421, loss = 0.11173638\n",
      "Iteration 422, loss = 0.11163964\n",
      "Iteration 423, loss = 0.11158226\n",
      "Iteration 424, loss = 0.11158378\n",
      "Iteration 425, loss = 0.11150147\n",
      "Iteration 426, loss = 0.11140906\n",
      "Iteration 427, loss = 0.11132594\n",
      "Iteration 428, loss = 0.11128334\n",
      "Iteration 429, loss = 0.11123751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96758139\n",
      "Iteration 2, loss = 0.83260131\n",
      "Iteration 3, loss = 0.70092947\n",
      "Iteration 4, loss = 0.59566951\n",
      "Iteration 5, loss = 0.52001988\n",
      "Iteration 6, loss = 0.46361353\n",
      "Iteration 7, loss = 0.42085975\n",
      "Iteration 8, loss = 0.38708238\n",
      "Iteration 9, loss = 0.36038154\n",
      "Iteration 10, loss = 0.33835153\n",
      "Iteration 11, loss = 0.31974661\n",
      "Iteration 12, loss = 0.30402515\n",
      "Iteration 13, loss = 0.29052493\n",
      "Iteration 14, loss = 0.27894534\n",
      "Iteration 15, loss = 0.26847532\n",
      "Iteration 16, loss = 0.25929574\n",
      "Iteration 17, loss = 0.25158262\n",
      "Iteration 18, loss = 0.24424163\n",
      "Iteration 19, loss = 0.23819079\n",
      "Iteration 20, loss = 0.23239741\n",
      "Iteration 21, loss = 0.22740400\n",
      "Iteration 22, loss = 0.22272766\n",
      "Iteration 23, loss = 0.21853503\n",
      "Iteration 24, loss = 0.21460793\n",
      "Iteration 25, loss = 0.21105909\n",
      "Iteration 26, loss = 0.20775607\n",
      "Iteration 27, loss = 0.20468022\n",
      "Iteration 28, loss = 0.20202202\n",
      "Iteration 29, loss = 0.19934520\n",
      "Iteration 30, loss = 0.19680868\n",
      "Iteration 31, loss = 0.19461452\n",
      "Iteration 32, loss = 0.19238378\n",
      "Iteration 33, loss = 0.19045514\n",
      "Iteration 34, loss = 0.18854297\n",
      "Iteration 35, loss = 0.18672457\n",
      "Iteration 36, loss = 0.18499049\n",
      "Iteration 37, loss = 0.18341464\n",
      "Iteration 38, loss = 0.18186325\n",
      "Iteration 39, loss = 0.18029698\n",
      "Iteration 40, loss = 0.17894878\n",
      "Iteration 41, loss = 0.17756053\n",
      "Iteration 42, loss = 0.17620125\n",
      "Iteration 43, loss = 0.17497308\n",
      "Iteration 44, loss = 0.17373130\n",
      "Iteration 45, loss = 0.17258709\n",
      "Iteration 46, loss = 0.17148410\n",
      "Iteration 47, loss = 0.17050338\n",
      "Iteration 48, loss = 0.16938267\n",
      "Iteration 49, loss = 0.16841123\n",
      "Iteration 50, loss = 0.16730025\n",
      "Iteration 51, loss = 0.16636826\n",
      "Iteration 52, loss = 0.16547763\n",
      "Iteration 53, loss = 0.16449800\n",
      "Iteration 54, loss = 0.16363139\n",
      "Iteration 55, loss = 0.16279626\n",
      "Iteration 56, loss = 0.16197102\n",
      "Iteration 57, loss = 0.16118345\n",
      "Iteration 58, loss = 0.16038015\n",
      "Iteration 59, loss = 0.15975122\n",
      "Iteration 60, loss = 0.15895074\n",
      "Iteration 61, loss = 0.15815058\n",
      "Iteration 62, loss = 0.15752638\n",
      "Iteration 63, loss = 0.15679133\n",
      "Iteration 64, loss = 0.15617659\n",
      "Iteration 65, loss = 0.15549258\n",
      "Iteration 66, loss = 0.15488622\n",
      "Iteration 67, loss = 0.15426105\n",
      "Iteration 68, loss = 0.15361764\n",
      "Iteration 69, loss = 0.15303289\n",
      "Iteration 70, loss = 0.15248445\n",
      "Iteration 71, loss = 0.15193754\n",
      "Iteration 72, loss = 0.15131846\n",
      "Iteration 73, loss = 0.15079114\n",
      "Iteration 74, loss = 0.15024762\n",
      "Iteration 75, loss = 0.14973310\n",
      "Iteration 76, loss = 0.14926902\n",
      "Iteration 77, loss = 0.14876861\n",
      "Iteration 78, loss = 0.14830160\n",
      "Iteration 79, loss = 0.14783531\n",
      "Iteration 80, loss = 0.14744802\n",
      "Iteration 81, loss = 0.14694167\n",
      "Iteration 82, loss = 0.14650034\n",
      "Iteration 83, loss = 0.14616666\n",
      "Iteration 84, loss = 0.14573015\n",
      "Iteration 85, loss = 0.14529545\n",
      "Iteration 86, loss = 0.14488277\n",
      "Iteration 87, loss = 0.14447756\n",
      "Iteration 88, loss = 0.14411109\n",
      "Iteration 89, loss = 0.14371720\n",
      "Iteration 90, loss = 0.14337732\n",
      "Iteration 91, loss = 0.14299009\n",
      "Iteration 92, loss = 0.14263254\n",
      "Iteration 93, loss = 0.14229268\n",
      "Iteration 94, loss = 0.14198338\n",
      "Iteration 95, loss = 0.14163187\n",
      "Iteration 96, loss = 0.14130656\n",
      "Iteration 97, loss = 0.14098796\n",
      "Iteration 98, loss = 0.14064443\n",
      "Iteration 99, loss = 0.14037158\n",
      "Iteration 100, loss = 0.14010698\n",
      "Iteration 101, loss = 0.13974214\n",
      "Iteration 102, loss = 0.13942538\n",
      "Iteration 103, loss = 0.13912487\n",
      "Iteration 104, loss = 0.13893929\n",
      "Iteration 105, loss = 0.13858931\n",
      "Iteration 106, loss = 0.13831553\n",
      "Iteration 107, loss = 0.13798179\n",
      "Iteration 108, loss = 0.13774576\n",
      "Iteration 109, loss = 0.13746199\n",
      "Iteration 110, loss = 0.13721094\n",
      "Iteration 111, loss = 0.13690632\n",
      "Iteration 112, loss = 0.13665885\n",
      "Iteration 113, loss = 0.13644814\n",
      "Iteration 114, loss = 0.13615952\n",
      "Iteration 115, loss = 0.13594059\n",
      "Iteration 116, loss = 0.13572928\n",
      "Iteration 117, loss = 0.13543579\n",
      "Iteration 118, loss = 0.13520433\n",
      "Iteration 119, loss = 0.13496609\n",
      "Iteration 120, loss = 0.13473400\n",
      "Iteration 121, loss = 0.13451536\n",
      "Iteration 122, loss = 0.13427527\n",
      "Iteration 123, loss = 0.13407521\n",
      "Iteration 124, loss = 0.13391271\n",
      "Iteration 125, loss = 0.13365685\n",
      "Iteration 126, loss = 0.13344184\n",
      "Iteration 127, loss = 0.13325375\n",
      "Iteration 128, loss = 0.13305677\n",
      "Iteration 129, loss = 0.13283837\n",
      "Iteration 130, loss = 0.13260302\n",
      "Iteration 131, loss = 0.13247248\n",
      "Iteration 132, loss = 0.13224722\n",
      "Iteration 133, loss = 0.13204386\n",
      "Iteration 134, loss = 0.13182852\n",
      "Iteration 135, loss = 0.13161064\n",
      "Iteration 136, loss = 0.13139335\n",
      "Iteration 137, loss = 0.13122648\n",
      "Iteration 138, loss = 0.13105582\n",
      "Iteration 139, loss = 0.13085386\n",
      "Iteration 140, loss = 0.13070858\n",
      "Iteration 141, loss = 0.13045380\n",
      "Iteration 142, loss = 0.13024960\n",
      "Iteration 143, loss = 0.13008615\n",
      "Iteration 144, loss = 0.12992803\n",
      "Iteration 145, loss = 0.12970216\n",
      "Iteration 146, loss = 0.12955057\n",
      "Iteration 147, loss = 0.12936208\n",
      "Iteration 148, loss = 0.12918879\n",
      "Iteration 149, loss = 0.12905066\n",
      "Iteration 150, loss = 0.12883580\n",
      "Iteration 151, loss = 0.12870007\n",
      "Iteration 152, loss = 0.12850962\n",
      "Iteration 153, loss = 0.12835321\n",
      "Iteration 154, loss = 0.12820636\n",
      "Iteration 155, loss = 0.12813404\n",
      "Iteration 156, loss = 0.12788112\n",
      "Iteration 157, loss = 0.12776555\n",
      "Iteration 158, loss = 0.12765021\n",
      "Iteration 159, loss = 0.12743890\n",
      "Iteration 160, loss = 0.12730675\n",
      "Iteration 161, loss = 0.12712820\n",
      "Iteration 162, loss = 0.12698380\n",
      "Iteration 163, loss = 0.12683381\n",
      "Iteration 164, loss = 0.12674360\n",
      "Iteration 165, loss = 0.12661798\n",
      "Iteration 166, loss = 0.12643235\n",
      "Iteration 167, loss = 0.12628020\n",
      "Iteration 168, loss = 0.12617023\n",
      "Iteration 169, loss = 0.12599511\n",
      "Iteration 170, loss = 0.12591706\n",
      "Iteration 171, loss = 0.12574473\n",
      "Iteration 172, loss = 0.12559880\n",
      "Iteration 173, loss = 0.12550400\n",
      "Iteration 174, loss = 0.12535044\n",
      "Iteration 175, loss = 0.12524009\n",
      "Iteration 176, loss = 0.12516484\n",
      "Iteration 177, loss = 0.12499256\n",
      "Iteration 178, loss = 0.12483499\n",
      "Iteration 179, loss = 0.12474674\n",
      "Iteration 180, loss = 0.12456677\n",
      "Iteration 181, loss = 0.12452930\n",
      "Iteration 182, loss = 0.12432920\n",
      "Iteration 183, loss = 0.12422899\n",
      "Iteration 184, loss = 0.12410549\n",
      "Iteration 185, loss = 0.12398181\n",
      "Iteration 186, loss = 0.12393233\n",
      "Iteration 187, loss = 0.12373407\n",
      "Iteration 188, loss = 0.12368407\n",
      "Iteration 189, loss = 0.12352362\n",
      "Iteration 190, loss = 0.12337742\n",
      "Iteration 191, loss = 0.12328667\n",
      "Iteration 192, loss = 0.12319728\n",
      "Iteration 193, loss = 0.12308221\n",
      "Iteration 194, loss = 0.12296423\n",
      "Iteration 195, loss = 0.12283081\n",
      "Iteration 196, loss = 0.12271354\n",
      "Iteration 197, loss = 0.12263430\n",
      "Iteration 198, loss = 0.12250068\n",
      "Iteration 199, loss = 0.12241363\n",
      "Iteration 200, loss = 0.12239330\n",
      "Iteration 201, loss = 0.12217350\n",
      "Iteration 202, loss = 0.12206579\n",
      "Iteration 203, loss = 0.12197571\n",
      "Iteration 204, loss = 0.12186293\n",
      "Iteration 205, loss = 0.12174937\n",
      "Iteration 206, loss = 0.12167846\n",
      "Iteration 207, loss = 0.12154782\n",
      "Iteration 208, loss = 0.12147189\n",
      "Iteration 209, loss = 0.12134114\n",
      "Iteration 210, loss = 0.12124574\n",
      "Iteration 211, loss = 0.12111902\n",
      "Iteration 212, loss = 0.12103540\n",
      "Iteration 213, loss = 0.12094533\n",
      "Iteration 214, loss = 0.12081441\n",
      "Iteration 215, loss = 0.12071482\n",
      "Iteration 216, loss = 0.12062118\n",
      "Iteration 217, loss = 0.12050021\n",
      "Iteration 218, loss = 0.12043865\n",
      "Iteration 219, loss = 0.12026033\n",
      "Iteration 220, loss = 0.12019869\n",
      "Iteration 221, loss = 0.12007111\n",
      "Iteration 222, loss = 0.11997284\n",
      "Iteration 223, loss = 0.11988868\n",
      "Iteration 224, loss = 0.11978326\n",
      "Iteration 225, loss = 0.11971010\n",
      "Iteration 226, loss = 0.11962525\n",
      "Iteration 227, loss = 0.11946144\n",
      "Iteration 228, loss = 0.11938510\n",
      "Iteration 229, loss = 0.11931160\n",
      "Iteration 230, loss = 0.11922258\n",
      "Iteration 231, loss = 0.11913361\n",
      "Iteration 232, loss = 0.11905094\n",
      "Iteration 233, loss = 0.11896164\n",
      "Iteration 234, loss = 0.11881839\n",
      "Iteration 235, loss = 0.11878841\n",
      "Iteration 236, loss = 0.11862918\n",
      "Iteration 237, loss = 0.11861858\n",
      "Iteration 238, loss = 0.11844902\n",
      "Iteration 239, loss = 0.11839493\n",
      "Iteration 240, loss = 0.11828109\n",
      "Iteration 241, loss = 0.11819908\n",
      "Iteration 242, loss = 0.11814439\n",
      "Iteration 243, loss = 0.11802273\n",
      "Iteration 244, loss = 0.11798016\n",
      "Iteration 245, loss = 0.11783527\n",
      "Iteration 246, loss = 0.11775292\n",
      "Iteration 247, loss = 0.11767371\n",
      "Iteration 248, loss = 0.11755544\n",
      "Iteration 249, loss = 0.11746107\n",
      "Iteration 250, loss = 0.11738558\n",
      "Iteration 251, loss = 0.11730379\n",
      "Iteration 252, loss = 0.11723203\n",
      "Iteration 253, loss = 0.11712944\n",
      "Iteration 254, loss = 0.11705755\n",
      "Iteration 255, loss = 0.11693794\n",
      "Iteration 256, loss = 0.11688621\n",
      "Iteration 257, loss = 0.11676832\n",
      "Iteration 258, loss = 0.11669990\n",
      "Iteration 259, loss = 0.11663370\n",
      "Iteration 260, loss = 0.11658545\n",
      "Iteration 261, loss = 0.11644112\n",
      "Iteration 262, loss = 0.11636515\n",
      "Iteration 263, loss = 0.11632061\n",
      "Iteration 264, loss = 0.11620628\n",
      "Iteration 265, loss = 0.11620272\n",
      "Iteration 266, loss = 0.11600423\n",
      "Iteration 267, loss = 0.11598292\n",
      "Iteration 268, loss = 0.11585712\n",
      "Iteration 269, loss = 0.11578827\n",
      "Iteration 270, loss = 0.11572056\n",
      "Iteration 271, loss = 0.11559681\n",
      "Iteration 272, loss = 0.11555863\n",
      "Iteration 273, loss = 0.11547760\n",
      "Iteration 274, loss = 0.11543285\n",
      "Iteration 275, loss = 0.11531411\n",
      "Iteration 276, loss = 0.11526012\n",
      "Iteration 277, loss = 0.11514691\n",
      "Iteration 278, loss = 0.11506686\n",
      "Iteration 279, loss = 0.11502607\n",
      "Iteration 280, loss = 0.11492927\n",
      "Iteration 281, loss = 0.11481539\n",
      "Iteration 282, loss = 0.11477986\n",
      "Iteration 283, loss = 0.11475267\n",
      "Iteration 284, loss = 0.11459682\n",
      "Iteration 285, loss = 0.11456479\n",
      "Iteration 286, loss = 0.11446640\n",
      "Iteration 287, loss = 0.11440317\n",
      "Iteration 288, loss = 0.11433998\n",
      "Iteration 289, loss = 0.11426374\n",
      "Iteration 290, loss = 0.11417867\n",
      "Iteration 291, loss = 0.11414305\n",
      "Iteration 292, loss = 0.11414798\n",
      "Iteration 293, loss = 0.11406089\n",
      "Iteration 294, loss = 0.11392714\n",
      "Iteration 295, loss = 0.11386002\n",
      "Iteration 296, loss = 0.11382258\n",
      "Iteration 297, loss = 0.11373200\n",
      "Iteration 298, loss = 0.11366757\n",
      "Iteration 299, loss = 0.11360827\n",
      "Iteration 300, loss = 0.11352161\n",
      "Iteration 301, loss = 0.11349702\n",
      "Iteration 302, loss = 0.11341960\n",
      "Iteration 303, loss = 0.11333154\n",
      "Iteration 304, loss = 0.11329776\n",
      "Iteration 305, loss = 0.11322445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96686347\n",
      "Iteration 2, loss = 0.83417482\n",
      "Iteration 3, loss = 0.70189671\n",
      "Iteration 4, loss = 0.59748111\n",
      "Iteration 5, loss = 0.52189148\n",
      "Iteration 6, loss = 0.46490642\n",
      "Iteration 7, loss = 0.42227158\n",
      "Iteration 8, loss = 0.38881605\n",
      "Iteration 9, loss = 0.36214062\n",
      "Iteration 10, loss = 0.34018450\n",
      "Iteration 11, loss = 0.32179015\n",
      "Iteration 12, loss = 0.30613407\n",
      "Iteration 13, loss = 0.29283466\n",
      "Iteration 14, loss = 0.28145988\n",
      "Iteration 15, loss = 0.27151610\n",
      "Iteration 16, loss = 0.26278455\n",
      "Iteration 17, loss = 0.25529557\n",
      "Iteration 18, loss = 0.24851034\n",
      "Iteration 19, loss = 0.24255868\n",
      "Iteration 20, loss = 0.23732431\n",
      "Iteration 21, loss = 0.23252348\n",
      "Iteration 22, loss = 0.22805499\n",
      "Iteration 23, loss = 0.22436560\n",
      "Iteration 24, loss = 0.22065254\n",
      "Iteration 25, loss = 0.21730456\n",
      "Iteration 26, loss = 0.21433047\n",
      "Iteration 27, loss = 0.21146172\n",
      "Iteration 28, loss = 0.20900236\n",
      "Iteration 29, loss = 0.20649042\n",
      "Iteration 30, loss = 0.20425485\n",
      "Iteration 31, loss = 0.20211209\n",
      "Iteration 32, loss = 0.20013270\n",
      "Iteration 33, loss = 0.19840666\n",
      "Iteration 34, loss = 0.19663903\n",
      "Iteration 35, loss = 0.19494286\n",
      "Iteration 36, loss = 0.19340379\n",
      "Iteration 37, loss = 0.19204098\n",
      "Iteration 38, loss = 0.19067876\n",
      "Iteration 39, loss = 0.18928052\n",
      "Iteration 40, loss = 0.18805145\n",
      "Iteration 41, loss = 0.18693102\n",
      "Iteration 42, loss = 0.18568752\n",
      "Iteration 43, loss = 0.18451598\n",
      "Iteration 44, loss = 0.18344710\n",
      "Iteration 45, loss = 0.18239346\n",
      "Iteration 46, loss = 0.18140519\n",
      "Iteration 47, loss = 0.18045712\n",
      "Iteration 48, loss = 0.17952221\n",
      "Iteration 49, loss = 0.17864777\n",
      "Iteration 50, loss = 0.17773486\n",
      "Iteration 51, loss = 0.17688843\n",
      "Iteration 52, loss = 0.17607419\n",
      "Iteration 53, loss = 0.17523978\n",
      "Iteration 54, loss = 0.17446313\n",
      "Iteration 55, loss = 0.17370176\n",
      "Iteration 56, loss = 0.17296156\n",
      "Iteration 57, loss = 0.17228040\n",
      "Iteration 58, loss = 0.17151237\n",
      "Iteration 59, loss = 0.17086000\n",
      "Iteration 60, loss = 0.17018860\n",
      "Iteration 61, loss = 0.16950725\n",
      "Iteration 62, loss = 0.16882548\n",
      "Iteration 63, loss = 0.16823820\n",
      "Iteration 64, loss = 0.16753869\n",
      "Iteration 65, loss = 0.16691423\n",
      "Iteration 66, loss = 0.16635028\n",
      "Iteration 67, loss = 0.16576497\n",
      "Iteration 68, loss = 0.16521074\n",
      "Iteration 69, loss = 0.16466029\n",
      "Iteration 70, loss = 0.16410514\n",
      "Iteration 71, loss = 0.16356321\n",
      "Iteration 72, loss = 0.16307800\n",
      "Iteration 73, loss = 0.16261949\n",
      "Iteration 74, loss = 0.16212752\n",
      "Iteration 75, loss = 0.16161539\n",
      "Iteration 76, loss = 0.16121803\n",
      "Iteration 77, loss = 0.16074870\n",
      "Iteration 78, loss = 0.16033961\n",
      "Iteration 79, loss = 0.15992363\n",
      "Iteration 80, loss = 0.15953345\n",
      "Iteration 81, loss = 0.15911086\n",
      "Iteration 82, loss = 0.15863863\n",
      "Iteration 83, loss = 0.15828434\n",
      "Iteration 84, loss = 0.15793237\n",
      "Iteration 85, loss = 0.15756265\n",
      "Iteration 86, loss = 0.15711579\n",
      "Iteration 87, loss = 0.15681163\n",
      "Iteration 88, loss = 0.15641863\n",
      "Iteration 89, loss = 0.15610796\n",
      "Iteration 90, loss = 0.15579253\n",
      "Iteration 91, loss = 0.15536405\n",
      "Iteration 92, loss = 0.15507254\n",
      "Iteration 93, loss = 0.15478069\n",
      "Iteration 94, loss = 0.15442139\n",
      "Iteration 95, loss = 0.15408787\n",
      "Iteration 96, loss = 0.15377822\n",
      "Iteration 97, loss = 0.15348772\n",
      "Iteration 98, loss = 0.15320175\n",
      "Iteration 99, loss = 0.15289893\n",
      "Iteration 100, loss = 0.15263260\n",
      "Iteration 101, loss = 0.15236690\n",
      "Iteration 102, loss = 0.15210946\n",
      "Iteration 103, loss = 0.15191715\n",
      "Iteration 104, loss = 0.15157572\n",
      "Iteration 105, loss = 0.15132074\n",
      "Iteration 106, loss = 0.15114221\n",
      "Iteration 107, loss = 0.15088179\n",
      "Iteration 108, loss = 0.15069758\n",
      "Iteration 109, loss = 0.15036139\n",
      "Iteration 110, loss = 0.15012517\n",
      "Iteration 111, loss = 0.14992155\n",
      "Iteration 112, loss = 0.14965956\n",
      "Iteration 113, loss = 0.14946679\n",
      "Iteration 114, loss = 0.14925124\n",
      "Iteration 115, loss = 0.14900732\n",
      "Iteration 116, loss = 0.14890234\n",
      "Iteration 117, loss = 0.14857852\n",
      "Iteration 118, loss = 0.14842085\n",
      "Iteration 119, loss = 0.14817713\n",
      "Iteration 120, loss = 0.14798030\n",
      "Iteration 121, loss = 0.14776565\n",
      "Iteration 122, loss = 0.14757728\n",
      "Iteration 123, loss = 0.14744544\n",
      "Iteration 124, loss = 0.14725238\n",
      "Iteration 125, loss = 0.14700640\n",
      "Iteration 126, loss = 0.14683042\n",
      "Iteration 127, loss = 0.14663210\n",
      "Iteration 128, loss = 0.14646180\n",
      "Iteration 129, loss = 0.14631511\n",
      "Iteration 130, loss = 0.14610298\n",
      "Iteration 131, loss = 0.14594495\n",
      "Iteration 132, loss = 0.14581616\n",
      "Iteration 133, loss = 0.14565173\n",
      "Iteration 134, loss = 0.14540760\n",
      "Iteration 135, loss = 0.14525532\n",
      "Iteration 136, loss = 0.14510488\n",
      "Iteration 137, loss = 0.14497082\n",
      "Iteration 138, loss = 0.14477796\n",
      "Iteration 139, loss = 0.14464439\n",
      "Iteration 140, loss = 0.14446959\n",
      "Iteration 141, loss = 0.14430749\n",
      "Iteration 142, loss = 0.14415706\n",
      "Iteration 143, loss = 0.14394351\n",
      "Iteration 144, loss = 0.14380878\n",
      "Iteration 145, loss = 0.14368339\n",
      "Iteration 146, loss = 0.14354673\n",
      "Iteration 147, loss = 0.14338661\n",
      "Iteration 148, loss = 0.14320468\n",
      "Iteration 149, loss = 0.14311192\n",
      "Iteration 150, loss = 0.14292767\n",
      "Iteration 151, loss = 0.14279929\n",
      "Iteration 152, loss = 0.14262459\n",
      "Iteration 153, loss = 0.14250407\n",
      "Iteration 154, loss = 0.14235021\n",
      "Iteration 155, loss = 0.14230699\n",
      "Iteration 156, loss = 0.14207437\n",
      "Iteration 157, loss = 0.14198357\n",
      "Iteration 158, loss = 0.14177050\n",
      "Iteration 159, loss = 0.14165014\n",
      "Iteration 160, loss = 0.14153295\n",
      "Iteration 161, loss = 0.14134758\n",
      "Iteration 162, loss = 0.14123900\n",
      "Iteration 163, loss = 0.14109277\n",
      "Iteration 164, loss = 0.14095187\n",
      "Iteration 165, loss = 0.14082944\n",
      "Iteration 166, loss = 0.14070438\n",
      "Iteration 167, loss = 0.14053072\n",
      "Iteration 168, loss = 0.14041247\n",
      "Iteration 169, loss = 0.14027180\n",
      "Iteration 170, loss = 0.14020859\n",
      "Iteration 171, loss = 0.14006222\n",
      "Iteration 172, loss = 0.13990149\n",
      "Iteration 173, loss = 0.13973170\n",
      "Iteration 174, loss = 0.13962445\n",
      "Iteration 175, loss = 0.13949177\n",
      "Iteration 176, loss = 0.13936820\n",
      "Iteration 177, loss = 0.13929304\n",
      "Iteration 178, loss = 0.13912375\n",
      "Iteration 179, loss = 0.13903711\n",
      "Iteration 180, loss = 0.13892093\n",
      "Iteration 181, loss = 0.13877813\n",
      "Iteration 182, loss = 0.13864462\n",
      "Iteration 183, loss = 0.13854984\n",
      "Iteration 184, loss = 0.13838700\n",
      "Iteration 185, loss = 0.13824425\n",
      "Iteration 186, loss = 0.13812489\n",
      "Iteration 187, loss = 0.13801879\n",
      "Iteration 188, loss = 0.13791638\n",
      "Iteration 189, loss = 0.13783492\n",
      "Iteration 190, loss = 0.13763581\n",
      "Iteration 191, loss = 0.13750039\n",
      "Iteration 192, loss = 0.13738396\n",
      "Iteration 193, loss = 0.13727274\n",
      "Iteration 194, loss = 0.13715605\n",
      "Iteration 195, loss = 0.13700802\n",
      "Iteration 196, loss = 0.13689241\n",
      "Iteration 197, loss = 0.13680625\n",
      "Iteration 198, loss = 0.13665931\n",
      "Iteration 199, loss = 0.13653214\n",
      "Iteration 200, loss = 0.13651363\n",
      "Iteration 201, loss = 0.13633941\n",
      "Iteration 202, loss = 0.13625570\n",
      "Iteration 203, loss = 0.13611391\n",
      "Iteration 204, loss = 0.13597974\n",
      "Iteration 205, loss = 0.13590233\n",
      "Iteration 206, loss = 0.13574871\n",
      "Iteration 207, loss = 0.13561086\n",
      "Iteration 208, loss = 0.13555044\n",
      "Iteration 209, loss = 0.13542050\n",
      "Iteration 210, loss = 0.13532688\n",
      "Iteration 211, loss = 0.13517133\n",
      "Iteration 212, loss = 0.13510316\n",
      "Iteration 213, loss = 0.13498760\n",
      "Iteration 214, loss = 0.13490165\n",
      "Iteration 215, loss = 0.13473912\n",
      "Iteration 216, loss = 0.13460030\n",
      "Iteration 217, loss = 0.13451417\n",
      "Iteration 218, loss = 0.13446513\n",
      "Iteration 219, loss = 0.13431481\n",
      "Iteration 220, loss = 0.13418496\n",
      "Iteration 221, loss = 0.13406449\n",
      "Iteration 222, loss = 0.13399224\n",
      "Iteration 223, loss = 0.13382252\n",
      "Iteration 224, loss = 0.13380455\n",
      "Iteration 225, loss = 0.13361254\n",
      "Iteration 226, loss = 0.13358393\n",
      "Iteration 227, loss = 0.13339666\n",
      "Iteration 228, loss = 0.13330420\n",
      "Iteration 229, loss = 0.13321648\n",
      "Iteration 230, loss = 0.13311722\n",
      "Iteration 231, loss = 0.13298775\n",
      "Iteration 232, loss = 0.13287482\n",
      "Iteration 233, loss = 0.13278344\n",
      "Iteration 234, loss = 0.13266921\n",
      "Iteration 235, loss = 0.13258991\n",
      "Iteration 236, loss = 0.13250157\n",
      "Iteration 237, loss = 0.13236938\n",
      "Iteration 238, loss = 0.13225487\n",
      "Iteration 239, loss = 0.13214584\n",
      "Iteration 240, loss = 0.13205672\n",
      "Iteration 241, loss = 0.13191581\n",
      "Iteration 242, loss = 0.13185211\n",
      "Iteration 243, loss = 0.13171091\n",
      "Iteration 244, loss = 0.13161376\n",
      "Iteration 245, loss = 0.13152429\n",
      "Iteration 246, loss = 0.13141956\n",
      "Iteration 247, loss = 0.13128760\n",
      "Iteration 248, loss = 0.13120078\n",
      "Iteration 249, loss = 0.13112670\n",
      "Iteration 250, loss = 0.13096969\n",
      "Iteration 251, loss = 0.13087318\n",
      "Iteration 252, loss = 0.13074642\n",
      "Iteration 253, loss = 0.13063316\n",
      "Iteration 254, loss = 0.13063575\n",
      "Iteration 255, loss = 0.13049424\n",
      "Iteration 256, loss = 0.13033552\n",
      "Iteration 257, loss = 0.13024225\n",
      "Iteration 258, loss = 0.13015669\n",
      "Iteration 259, loss = 0.13006305\n",
      "Iteration 260, loss = 0.13000825\n",
      "Iteration 261, loss = 0.12986680\n",
      "Iteration 262, loss = 0.12977133\n",
      "Iteration 263, loss = 0.12970409\n",
      "Iteration 264, loss = 0.12959993\n",
      "Iteration 265, loss = 0.12950678\n",
      "Iteration 266, loss = 0.12941994\n",
      "Iteration 267, loss = 0.12932398\n",
      "Iteration 268, loss = 0.12921897\n",
      "Iteration 269, loss = 0.12918560\n",
      "Iteration 270, loss = 0.12904118\n",
      "Iteration 271, loss = 0.12897825\n",
      "Iteration 272, loss = 0.12881534\n",
      "Iteration 273, loss = 0.12865071\n",
      "Iteration 274, loss = 0.12861591\n",
      "Iteration 275, loss = 0.12851773\n",
      "Iteration 276, loss = 0.12834250\n",
      "Iteration 277, loss = 0.12827021\n",
      "Iteration 278, loss = 0.12813829\n",
      "Iteration 279, loss = 0.12809891\n",
      "Iteration 280, loss = 0.12796327\n",
      "Iteration 281, loss = 0.12784854\n",
      "Iteration 282, loss = 0.12777571\n",
      "Iteration 283, loss = 0.12769764\n",
      "Iteration 284, loss = 0.12754846\n",
      "Iteration 285, loss = 0.12742470\n",
      "Iteration 286, loss = 0.12733991\n",
      "Iteration 287, loss = 0.12726916\n",
      "Iteration 288, loss = 0.12718660\n",
      "Iteration 289, loss = 0.12701763\n",
      "Iteration 290, loss = 0.12700347\n",
      "Iteration 291, loss = 0.12687441\n",
      "Iteration 292, loss = 0.12674998\n",
      "Iteration 293, loss = 0.12669505\n",
      "Iteration 294, loss = 0.12660140\n",
      "Iteration 295, loss = 0.12646489\n",
      "Iteration 296, loss = 0.12642720\n",
      "Iteration 297, loss = 0.12633706\n",
      "Iteration 298, loss = 0.12617159\n",
      "Iteration 299, loss = 0.12607609\n",
      "Iteration 300, loss = 0.12602318\n",
      "Iteration 301, loss = 0.12590434\n",
      "Iteration 302, loss = 0.12583374\n",
      "Iteration 303, loss = 0.12571754\n",
      "Iteration 304, loss = 0.12563973\n",
      "Iteration 305, loss = 0.12556077\n",
      "Iteration 306, loss = 0.12545391\n",
      "Iteration 307, loss = 0.12549516\n",
      "Iteration 308, loss = 0.12531056\n",
      "Iteration 309, loss = 0.12520233\n",
      "Iteration 310, loss = 0.12513075\n",
      "Iteration 311, loss = 0.12501719\n",
      "Iteration 312, loss = 0.12497795\n",
      "Iteration 313, loss = 0.12488081\n",
      "Iteration 314, loss = 0.12477836\n",
      "Iteration 315, loss = 0.12475390\n",
      "Iteration 316, loss = 0.12465680\n",
      "Iteration 317, loss = 0.12450486\n",
      "Iteration 318, loss = 0.12451169\n",
      "Iteration 319, loss = 0.12434204\n",
      "Iteration 320, loss = 0.12426998\n",
      "Iteration 321, loss = 0.12424474\n",
      "Iteration 322, loss = 0.12417525\n",
      "Iteration 323, loss = 0.12404366\n",
      "Iteration 324, loss = 0.12399706\n",
      "Iteration 325, loss = 0.12384704\n",
      "Iteration 326, loss = 0.12376215\n",
      "Iteration 327, loss = 0.12372370\n",
      "Iteration 328, loss = 0.12366129\n",
      "Iteration 329, loss = 0.12355819\n",
      "Iteration 330, loss = 0.12347535\n",
      "Iteration 331, loss = 0.12347136\n",
      "Iteration 332, loss = 0.12328031\n",
      "Iteration 333, loss = 0.12322921\n",
      "Iteration 334, loss = 0.12313255\n",
      "Iteration 335, loss = 0.12307173\n",
      "Iteration 336, loss = 0.12302741\n",
      "Iteration 337, loss = 0.12292282\n",
      "Iteration 338, loss = 0.12287620\n",
      "Iteration 339, loss = 0.12279216\n",
      "Iteration 340, loss = 0.12273633\n",
      "Iteration 341, loss = 0.12262695\n",
      "Iteration 342, loss = 0.12253803\n",
      "Iteration 343, loss = 0.12253672\n",
      "Iteration 344, loss = 0.12240227\n",
      "Iteration 345, loss = 0.12229335\n",
      "Iteration 346, loss = 0.12223744\n",
      "Iteration 347, loss = 0.12218564\n",
      "Iteration 348, loss = 0.12212220\n",
      "Iteration 349, loss = 0.12206873\n",
      "Iteration 350, loss = 0.12194835\n",
      "Iteration 351, loss = 0.12184160\n",
      "Iteration 352, loss = 0.12177606\n",
      "Iteration 353, loss = 0.12175059\n",
      "Iteration 354, loss = 0.12174768\n",
      "Iteration 355, loss = 0.12161913\n",
      "Iteration 356, loss = 0.12153778\n",
      "Iteration 357, loss = 0.12143099\n",
      "Iteration 358, loss = 0.12145252\n",
      "Iteration 359, loss = 0.12130627\n",
      "Iteration 360, loss = 0.12124587\n",
      "Iteration 361, loss = 0.12115639\n",
      "Iteration 362, loss = 0.12107399\n",
      "Iteration 363, loss = 0.12099690\n",
      "Iteration 364, loss = 0.12096457\n",
      "Iteration 365, loss = 0.12090138\n",
      "Iteration 366, loss = 0.12081037\n",
      "Iteration 367, loss = 0.12068684\n",
      "Iteration 368, loss = 0.12061705\n",
      "Iteration 369, loss = 0.12062391\n",
      "Iteration 370, loss = 0.12047098\n",
      "Iteration 371, loss = 0.12040404\n",
      "Iteration 372, loss = 0.12034802\n",
      "Iteration 373, loss = 0.12022537\n",
      "Iteration 374, loss = 0.12021873\n",
      "Iteration 375, loss = 0.12019656\n",
      "Iteration 376, loss = 0.12004602\n",
      "Iteration 377, loss = 0.12000607\n",
      "Iteration 378, loss = 0.11988372\n",
      "Iteration 379, loss = 0.11981798\n",
      "Iteration 380, loss = 0.11978084\n",
      "Iteration 381, loss = 0.11968956\n",
      "Iteration 382, loss = 0.11961760\n",
      "Iteration 383, loss = 0.11955475\n",
      "Iteration 384, loss = 0.11951468\n",
      "Iteration 385, loss = 0.11944256\n",
      "Iteration 386, loss = 0.11935119\n",
      "Iteration 387, loss = 0.11928214\n",
      "Iteration 388, loss = 0.11918469\n",
      "Iteration 389, loss = 0.11914742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96862240\n",
      "Iteration 2, loss = 0.83315037\n",
      "Iteration 3, loss = 0.69803419\n",
      "Iteration 4, loss = 0.59205148\n",
      "Iteration 5, loss = 0.51640724\n",
      "Iteration 6, loss = 0.45901821\n",
      "Iteration 7, loss = 0.41584846\n",
      "Iteration 8, loss = 0.38203839\n",
      "Iteration 9, loss = 0.35525319\n",
      "Iteration 10, loss = 0.33330464\n",
      "Iteration 11, loss = 0.31482817\n",
      "Iteration 12, loss = 0.29915866\n",
      "Iteration 13, loss = 0.28585091\n",
      "Iteration 14, loss = 0.27443116\n",
      "Iteration 15, loss = 0.26433321\n",
      "Iteration 16, loss = 0.25564434\n",
      "Iteration 17, loss = 0.24804915\n",
      "Iteration 18, loss = 0.24122826\n",
      "Iteration 19, loss = 0.23515010\n",
      "Iteration 20, loss = 0.22982592\n",
      "Iteration 21, loss = 0.22486816\n",
      "Iteration 22, loss = 0.22026367\n",
      "Iteration 23, loss = 0.21639884\n",
      "Iteration 24, loss = 0.21253856\n",
      "Iteration 25, loss = 0.20910687\n",
      "Iteration 26, loss = 0.20597020\n",
      "Iteration 27, loss = 0.20296265\n",
      "Iteration 28, loss = 0.20039105\n",
      "Iteration 29, loss = 0.19777986\n",
      "Iteration 30, loss = 0.19539570\n",
      "Iteration 31, loss = 0.19319008\n",
      "Iteration 32, loss = 0.19107624\n",
      "Iteration 33, loss = 0.18915162\n",
      "Iteration 34, loss = 0.18724824\n",
      "Iteration 35, loss = 0.18542979\n",
      "Iteration 36, loss = 0.18373317\n",
      "Iteration 37, loss = 0.18217062\n",
      "Iteration 38, loss = 0.18066305\n",
      "Iteration 39, loss = 0.17912361\n",
      "Iteration 40, loss = 0.17770506\n",
      "Iteration 41, loss = 0.17643737\n",
      "Iteration 42, loss = 0.17510335\n",
      "Iteration 43, loss = 0.17379536\n",
      "Iteration 44, loss = 0.17262409\n",
      "Iteration 45, loss = 0.17145919\n",
      "Iteration 46, loss = 0.17039544\n",
      "Iteration 47, loss = 0.16925355\n",
      "Iteration 48, loss = 0.16821372\n",
      "Iteration 49, loss = 0.16720892\n",
      "Iteration 50, loss = 0.16621362\n",
      "Iteration 51, loss = 0.16521336\n",
      "Iteration 52, loss = 0.16428110\n",
      "Iteration 53, loss = 0.16332084\n",
      "Iteration 54, loss = 0.16240471\n",
      "Iteration 55, loss = 0.16152201\n",
      "Iteration 56, loss = 0.16066085\n",
      "Iteration 57, loss = 0.15986159\n",
      "Iteration 58, loss = 0.15894461\n",
      "Iteration 59, loss = 0.15817699\n",
      "Iteration 60, loss = 0.15734740\n",
      "Iteration 61, loss = 0.15660280\n",
      "Iteration 62, loss = 0.15584980\n",
      "Iteration 63, loss = 0.15519650\n",
      "Iteration 64, loss = 0.15441484\n",
      "Iteration 65, loss = 0.15373811\n",
      "Iteration 66, loss = 0.15310241\n",
      "Iteration 67, loss = 0.15238541\n",
      "Iteration 68, loss = 0.15176753\n",
      "Iteration 69, loss = 0.15110452\n",
      "Iteration 70, loss = 0.15045137\n",
      "Iteration 71, loss = 0.14981963\n",
      "Iteration 72, loss = 0.14920501\n",
      "Iteration 73, loss = 0.14864585\n",
      "Iteration 74, loss = 0.14805399\n",
      "Iteration 75, loss = 0.14743109\n",
      "Iteration 76, loss = 0.14693439\n",
      "Iteration 77, loss = 0.14634784\n",
      "Iteration 78, loss = 0.14583605\n",
      "Iteration 79, loss = 0.14535173\n",
      "Iteration 80, loss = 0.14481811\n",
      "Iteration 81, loss = 0.14434344\n",
      "Iteration 82, loss = 0.14379943\n",
      "Iteration 83, loss = 0.14331550\n",
      "Iteration 84, loss = 0.14291252\n",
      "Iteration 85, loss = 0.14242830\n",
      "Iteration 86, loss = 0.14196047\n",
      "Iteration 87, loss = 0.14156618\n",
      "Iteration 88, loss = 0.14113739\n",
      "Iteration 89, loss = 0.14077054\n",
      "Iteration 90, loss = 0.14038267\n",
      "Iteration 91, loss = 0.13998071\n",
      "Iteration 92, loss = 0.13962946\n",
      "Iteration 93, loss = 0.13933699\n",
      "Iteration 94, loss = 0.13893101\n",
      "Iteration 95, loss = 0.13854424\n",
      "Iteration 96, loss = 0.13818368\n",
      "Iteration 97, loss = 0.13784022\n",
      "Iteration 98, loss = 0.13748213\n",
      "Iteration 99, loss = 0.13714640\n",
      "Iteration 100, loss = 0.13679335\n",
      "Iteration 101, loss = 0.13648912\n",
      "Iteration 102, loss = 0.13616330\n",
      "Iteration 103, loss = 0.13593783\n",
      "Iteration 104, loss = 0.13553201\n",
      "Iteration 105, loss = 0.13523123\n",
      "Iteration 106, loss = 0.13495084\n",
      "Iteration 107, loss = 0.13466745\n",
      "Iteration 108, loss = 0.13447461\n",
      "Iteration 109, loss = 0.13406974\n",
      "Iteration 110, loss = 0.13378108\n",
      "Iteration 111, loss = 0.13350659\n",
      "Iteration 112, loss = 0.13322024\n",
      "Iteration 113, loss = 0.13297267\n",
      "Iteration 114, loss = 0.13272248\n",
      "Iteration 115, loss = 0.13245775\n",
      "Iteration 116, loss = 0.13224325\n",
      "Iteration 117, loss = 0.13192219\n",
      "Iteration 118, loss = 0.13171272\n",
      "Iteration 119, loss = 0.13142523\n",
      "Iteration 120, loss = 0.13119032\n",
      "Iteration 121, loss = 0.13098070\n",
      "Iteration 122, loss = 0.13073035\n",
      "Iteration 123, loss = 0.13052074\n",
      "Iteration 124, loss = 0.13033880\n",
      "Iteration 125, loss = 0.13006930\n",
      "Iteration 126, loss = 0.12987237\n",
      "Iteration 127, loss = 0.12964450\n",
      "Iteration 128, loss = 0.12946130\n",
      "Iteration 129, loss = 0.12927624\n",
      "Iteration 130, loss = 0.12904159\n",
      "Iteration 131, loss = 0.12886404\n",
      "Iteration 132, loss = 0.12867919\n",
      "Iteration 133, loss = 0.12850178\n",
      "Iteration 134, loss = 0.12825091\n",
      "Iteration 135, loss = 0.12807882\n",
      "Iteration 136, loss = 0.12790857\n",
      "Iteration 137, loss = 0.12774122\n",
      "Iteration 138, loss = 0.12751219\n",
      "Iteration 139, loss = 0.12735573\n",
      "Iteration 140, loss = 0.12719659\n",
      "Iteration 141, loss = 0.12698840\n",
      "Iteration 142, loss = 0.12684097\n",
      "Iteration 143, loss = 0.12660943\n",
      "Iteration 144, loss = 0.12644860\n",
      "Iteration 145, loss = 0.12631192\n",
      "Iteration 146, loss = 0.12613773\n",
      "Iteration 147, loss = 0.12596003\n",
      "Iteration 148, loss = 0.12578801\n",
      "Iteration 149, loss = 0.12563761\n",
      "Iteration 150, loss = 0.12546721\n",
      "Iteration 151, loss = 0.12528632\n",
      "Iteration 152, loss = 0.12514381\n",
      "Iteration 153, loss = 0.12500640\n",
      "Iteration 154, loss = 0.12483387\n",
      "Iteration 155, loss = 0.12476206\n",
      "Iteration 156, loss = 0.12454544\n",
      "Iteration 157, loss = 0.12442180\n",
      "Iteration 158, loss = 0.12422795\n",
      "Iteration 159, loss = 0.12411345\n",
      "Iteration 160, loss = 0.12397440\n",
      "Iteration 161, loss = 0.12379723\n",
      "Iteration 162, loss = 0.12366955\n",
      "Iteration 163, loss = 0.12352825\n",
      "Iteration 164, loss = 0.12338321\n",
      "Iteration 165, loss = 0.12327409\n",
      "Iteration 166, loss = 0.12311842\n",
      "Iteration 167, loss = 0.12297485\n",
      "Iteration 168, loss = 0.12283336\n",
      "Iteration 169, loss = 0.12269007\n",
      "Iteration 170, loss = 0.12264930\n",
      "Iteration 171, loss = 0.12249292\n",
      "Iteration 172, loss = 0.12234247\n",
      "Iteration 173, loss = 0.12217919\n",
      "Iteration 174, loss = 0.12204342\n",
      "Iteration 175, loss = 0.12193911\n",
      "Iteration 176, loss = 0.12180992\n",
      "Iteration 177, loss = 0.12169674\n",
      "Iteration 178, loss = 0.12156845\n",
      "Iteration 179, loss = 0.12147436\n",
      "Iteration 180, loss = 0.12135895\n",
      "Iteration 181, loss = 0.12121508\n",
      "Iteration 182, loss = 0.12110843\n",
      "Iteration 183, loss = 0.12098513\n",
      "Iteration 184, loss = 0.12083941\n",
      "Iteration 185, loss = 0.12069943\n",
      "Iteration 186, loss = 0.12056802\n",
      "Iteration 187, loss = 0.12045604\n",
      "Iteration 188, loss = 0.12039804\n",
      "Iteration 189, loss = 0.12027369\n",
      "Iteration 190, loss = 0.12014620\n",
      "Iteration 191, loss = 0.11998122\n",
      "Iteration 192, loss = 0.11989347\n",
      "Iteration 193, loss = 0.11976173\n",
      "Iteration 194, loss = 0.11966837\n",
      "Iteration 195, loss = 0.11952145\n",
      "Iteration 196, loss = 0.11940405\n",
      "Iteration 197, loss = 0.11932100\n",
      "Iteration 198, loss = 0.11920251\n",
      "Iteration 199, loss = 0.11906440\n",
      "Iteration 200, loss = 0.11899659\n",
      "Iteration 201, loss = 0.11886776\n",
      "Iteration 202, loss = 0.11878521\n",
      "Iteration 203, loss = 0.11866244\n",
      "Iteration 204, loss = 0.11852205\n",
      "Iteration 205, loss = 0.11845498\n",
      "Iteration 206, loss = 0.11832255\n",
      "Iteration 207, loss = 0.11818692\n",
      "Iteration 208, loss = 0.11811847\n",
      "Iteration 209, loss = 0.11799381\n",
      "Iteration 210, loss = 0.11790225\n",
      "Iteration 211, loss = 0.11774251\n",
      "Iteration 212, loss = 0.11765745\n",
      "Iteration 213, loss = 0.11757671\n",
      "Iteration 214, loss = 0.11748983\n",
      "Iteration 215, loss = 0.11733906\n",
      "Iteration 216, loss = 0.11721759\n",
      "Iteration 217, loss = 0.11715025\n",
      "Iteration 218, loss = 0.11707580\n",
      "Iteration 219, loss = 0.11698125\n",
      "Iteration 220, loss = 0.11685280\n",
      "Iteration 221, loss = 0.11674324\n",
      "Iteration 222, loss = 0.11666190\n",
      "Iteration 223, loss = 0.11651442\n",
      "Iteration 224, loss = 0.11652672\n",
      "Iteration 225, loss = 0.11632468\n",
      "Iteration 226, loss = 0.11628398\n",
      "Iteration 227, loss = 0.11616331\n",
      "Iteration 228, loss = 0.11604494\n",
      "Iteration 229, loss = 0.11596384\n",
      "Iteration 230, loss = 0.11583963\n",
      "Iteration 231, loss = 0.11574235\n",
      "Iteration 232, loss = 0.11567529\n",
      "Iteration 233, loss = 0.11556171\n",
      "Iteration 234, loss = 0.11547496\n",
      "Iteration 235, loss = 0.11538435\n",
      "Iteration 236, loss = 0.11530435\n",
      "Iteration 237, loss = 0.11518629\n",
      "Iteration 238, loss = 0.11511025\n",
      "Iteration 239, loss = 0.11501246\n",
      "Iteration 240, loss = 0.11491608\n",
      "Iteration 241, loss = 0.11479644\n",
      "Iteration 242, loss = 0.11476570\n",
      "Iteration 243, loss = 0.11464599\n",
      "Iteration 244, loss = 0.11455354\n",
      "Iteration 245, loss = 0.11448067\n",
      "Iteration 246, loss = 0.11437484\n",
      "Iteration 247, loss = 0.11427792\n",
      "Iteration 248, loss = 0.11420452\n",
      "Iteration 249, loss = 0.11413922\n",
      "Iteration 250, loss = 0.11400214\n",
      "Iteration 251, loss = 0.11394489\n",
      "Iteration 252, loss = 0.11384290\n",
      "Iteration 253, loss = 0.11372433\n",
      "Iteration 254, loss = 0.11369923\n",
      "Iteration 255, loss = 0.11359303\n",
      "Iteration 256, loss = 0.11346508\n",
      "Iteration 257, loss = 0.11338571\n",
      "Iteration 258, loss = 0.11329342\n",
      "Iteration 259, loss = 0.11320449\n",
      "Iteration 260, loss = 0.11315160\n",
      "Iteration 261, loss = 0.11303097\n",
      "Iteration 262, loss = 0.11294745\n",
      "Iteration 263, loss = 0.11290265\n",
      "Iteration 264, loss = 0.11279907\n",
      "Iteration 265, loss = 0.11271530\n",
      "Iteration 266, loss = 0.11264354\n",
      "Iteration 267, loss = 0.11255754\n",
      "Iteration 268, loss = 0.11244031\n",
      "Iteration 269, loss = 0.11241891\n",
      "Iteration 270, loss = 0.11227840\n",
      "Iteration 271, loss = 0.11224027\n",
      "Iteration 272, loss = 0.11213457\n",
      "Iteration 273, loss = 0.11201924\n",
      "Iteration 274, loss = 0.11196837\n",
      "Iteration 275, loss = 0.11190311\n",
      "Iteration 276, loss = 0.11180102\n",
      "Iteration 277, loss = 0.11176114\n",
      "Iteration 278, loss = 0.11163149\n",
      "Iteration 279, loss = 0.11161159\n",
      "Iteration 280, loss = 0.11149858\n",
      "Iteration 281, loss = 0.11146971\n",
      "Iteration 282, loss = 0.11137494\n",
      "Iteration 283, loss = 0.11129406\n",
      "Iteration 284, loss = 0.11120244\n",
      "Iteration 285, loss = 0.11111619\n",
      "Iteration 286, loss = 0.11103179\n",
      "Iteration 287, loss = 0.11096556\n",
      "Iteration 288, loss = 0.11093257\n",
      "Iteration 289, loss = 0.11077185\n",
      "Iteration 290, loss = 0.11071792\n",
      "Iteration 291, loss = 0.11067477\n",
      "Iteration 292, loss = 0.11056965\n",
      "Iteration 293, loss = 0.11050625\n",
      "Iteration 294, loss = 0.11046782\n",
      "Iteration 295, loss = 0.11035345\n",
      "Iteration 296, loss = 0.11033039\n",
      "Iteration 297, loss = 0.11022665\n",
      "Iteration 298, loss = 0.11011272\n",
      "Iteration 299, loss = 0.11006819\n",
      "Iteration 300, loss = 0.11000258\n",
      "Iteration 301, loss = 0.10991001\n",
      "Iteration 302, loss = 0.10984763\n",
      "Iteration 303, loss = 0.10977977\n",
      "Iteration 304, loss = 0.10971296\n",
      "Iteration 305, loss = 0.10965304\n",
      "Iteration 306, loss = 0.10957071\n",
      "Iteration 307, loss = 0.10960762\n",
      "Iteration 308, loss = 0.10946571\n",
      "Iteration 309, loss = 0.10938100\n",
      "Iteration 310, loss = 0.10933808\n",
      "Iteration 311, loss = 0.10924179\n",
      "Iteration 312, loss = 0.10920872\n",
      "Iteration 313, loss = 0.10911277\n",
      "Iteration 314, loss = 0.10905687\n",
      "Iteration 315, loss = 0.10902411\n",
      "Iteration 316, loss = 0.10897393\n",
      "Iteration 317, loss = 0.10887782\n",
      "Iteration 318, loss = 0.10886641\n",
      "Iteration 319, loss = 0.10872457\n",
      "Iteration 320, loss = 0.10866437\n",
      "Iteration 321, loss = 0.10865277\n",
      "Iteration 322, loss = 0.10862035\n",
      "Iteration 323, loss = 0.10852057\n",
      "Iteration 324, loss = 0.10845599\n",
      "Iteration 325, loss = 0.10833779\n",
      "Iteration 326, loss = 0.10829114\n",
      "Iteration 327, loss = 0.10826399\n",
      "Iteration 328, loss = 0.10820909\n",
      "Iteration 329, loss = 0.10813976\n",
      "Iteration 330, loss = 0.10809312\n",
      "Iteration 331, loss = 0.10805796\n",
      "Iteration 332, loss = 0.10791544\n",
      "Iteration 333, loss = 0.10790591\n",
      "Iteration 334, loss = 0.10781080\n",
      "Iteration 335, loss = 0.10777655\n",
      "Iteration 336, loss = 0.10771280\n",
      "Iteration 337, loss = 0.10768264\n",
      "Iteration 338, loss = 0.10760386\n",
      "Iteration 339, loss = 0.10754088\n",
      "Iteration 340, loss = 0.10751042\n",
      "Iteration 341, loss = 0.10742067\n",
      "Iteration 342, loss = 0.10738320\n",
      "Iteration 343, loss = 0.10740140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96392986\n",
      "Iteration 2, loss = 0.83030295\n",
      "Iteration 3, loss = 0.69774116\n",
      "Iteration 4, loss = 0.59400973\n",
      "Iteration 5, loss = 0.51819337\n",
      "Iteration 6, loss = 0.46245746\n",
      "Iteration 7, loss = 0.42076289\n",
      "Iteration 8, loss = 0.38789373\n",
      "Iteration 9, loss = 0.36196905\n",
      "Iteration 10, loss = 0.34031336\n",
      "Iteration 11, loss = 0.32251564\n",
      "Iteration 12, loss = 0.30707959\n",
      "Iteration 13, loss = 0.29409561\n",
      "Iteration 14, loss = 0.28278656\n",
      "Iteration 15, loss = 0.27288649\n",
      "Iteration 16, loss = 0.26415301\n",
      "Iteration 17, loss = 0.25652759\n",
      "Iteration 18, loss = 0.24987846\n",
      "Iteration 19, loss = 0.24401437\n",
      "Iteration 20, loss = 0.23872422\n",
      "Iteration 21, loss = 0.23397673\n",
      "Iteration 22, loss = 0.22973476\n",
      "Iteration 23, loss = 0.22576816\n",
      "Iteration 24, loss = 0.22222623\n",
      "Iteration 25, loss = 0.21900722\n",
      "Iteration 26, loss = 0.21593259\n",
      "Iteration 27, loss = 0.21316867\n",
      "Iteration 28, loss = 0.21058856\n",
      "Iteration 29, loss = 0.20820587\n",
      "Iteration 30, loss = 0.20602083\n",
      "Iteration 31, loss = 0.20390522\n",
      "Iteration 32, loss = 0.20186935\n",
      "Iteration 33, loss = 0.20001493\n",
      "Iteration 34, loss = 0.19826275\n",
      "Iteration 35, loss = 0.19663574\n",
      "Iteration 36, loss = 0.19507069\n",
      "Iteration 37, loss = 0.19355308\n",
      "Iteration 38, loss = 0.19215272\n",
      "Iteration 39, loss = 0.19077982\n",
      "Iteration 40, loss = 0.18937534\n",
      "Iteration 41, loss = 0.18819183\n",
      "Iteration 42, loss = 0.18689713\n",
      "Iteration 43, loss = 0.18574783\n",
      "Iteration 44, loss = 0.18467139\n",
      "Iteration 45, loss = 0.18346594\n",
      "Iteration 46, loss = 0.18241649\n",
      "Iteration 47, loss = 0.18142417\n",
      "Iteration 48, loss = 0.18039663\n",
      "Iteration 49, loss = 0.17944132\n",
      "Iteration 50, loss = 0.17851101\n",
      "Iteration 51, loss = 0.17770133\n",
      "Iteration 52, loss = 0.17676518\n",
      "Iteration 53, loss = 0.17585058\n",
      "Iteration 54, loss = 0.17507978\n",
      "Iteration 55, loss = 0.17425906\n",
      "Iteration 56, loss = 0.17352300\n",
      "Iteration 57, loss = 0.17272803\n",
      "Iteration 58, loss = 0.17200197\n",
      "Iteration 59, loss = 0.17124357\n",
      "Iteration 60, loss = 0.17054441\n",
      "Iteration 61, loss = 0.16988681\n",
      "Iteration 62, loss = 0.16923117\n",
      "Iteration 63, loss = 0.16858572\n",
      "Iteration 64, loss = 0.16790730\n",
      "Iteration 65, loss = 0.16722130\n",
      "Iteration 66, loss = 0.16661459\n",
      "Iteration 67, loss = 0.16603525\n",
      "Iteration 68, loss = 0.16540200\n",
      "Iteration 69, loss = 0.16485882\n",
      "Iteration 70, loss = 0.16430857\n",
      "Iteration 71, loss = 0.16378294\n",
      "Iteration 72, loss = 0.16332603\n",
      "Iteration 73, loss = 0.16274322\n",
      "Iteration 74, loss = 0.16228110\n",
      "Iteration 75, loss = 0.16178910\n",
      "Iteration 76, loss = 0.16136672\n",
      "Iteration 77, loss = 0.16085325\n",
      "Iteration 78, loss = 0.16039569\n",
      "Iteration 79, loss = 0.15992967\n",
      "Iteration 80, loss = 0.15952680\n",
      "Iteration 81, loss = 0.15912032\n",
      "Iteration 82, loss = 0.15875084\n",
      "Iteration 83, loss = 0.15830908\n",
      "Iteration 84, loss = 0.15792720\n",
      "Iteration 85, loss = 0.15755612\n",
      "Iteration 86, loss = 0.15719925\n",
      "Iteration 87, loss = 0.15687609\n",
      "Iteration 88, loss = 0.15651551\n",
      "Iteration 89, loss = 0.15615575\n",
      "Iteration 90, loss = 0.15580072\n",
      "Iteration 91, loss = 0.15544464\n",
      "Iteration 92, loss = 0.15511043\n",
      "Iteration 93, loss = 0.15484814\n",
      "Iteration 94, loss = 0.15450705\n",
      "Iteration 95, loss = 0.15417143\n",
      "Iteration 96, loss = 0.15388343\n",
      "Iteration 97, loss = 0.15362971\n",
      "Iteration 98, loss = 0.15332520\n",
      "Iteration 99, loss = 0.15300353\n",
      "Iteration 100, loss = 0.15275991\n",
      "Iteration 101, loss = 0.15243518\n",
      "Iteration 102, loss = 0.15218789\n",
      "Iteration 103, loss = 0.15186735\n",
      "Iteration 104, loss = 0.15164113\n",
      "Iteration 105, loss = 0.15136816\n",
      "Iteration 106, loss = 0.15112927\n",
      "Iteration 107, loss = 0.15086932\n",
      "Iteration 108, loss = 0.15066027\n",
      "Iteration 109, loss = 0.15038834\n",
      "Iteration 110, loss = 0.15021505\n",
      "Iteration 111, loss = 0.14995146\n",
      "Iteration 112, loss = 0.14974744\n",
      "Iteration 113, loss = 0.14951508\n",
      "Iteration 114, loss = 0.14925973\n",
      "Iteration 115, loss = 0.14904009\n",
      "Iteration 116, loss = 0.14888214\n",
      "Iteration 117, loss = 0.14861480\n",
      "Iteration 118, loss = 0.14843436\n",
      "Iteration 119, loss = 0.14821116\n",
      "Iteration 120, loss = 0.14805478\n",
      "Iteration 121, loss = 0.14785533\n",
      "Iteration 122, loss = 0.14760751\n",
      "Iteration 123, loss = 0.14743242\n",
      "Iteration 124, loss = 0.14725507\n",
      "Iteration 125, loss = 0.14708553\n",
      "Iteration 126, loss = 0.14689943\n",
      "Iteration 127, loss = 0.14667538\n",
      "Iteration 128, loss = 0.14650335\n",
      "Iteration 129, loss = 0.14631730\n",
      "Iteration 130, loss = 0.14621275\n",
      "Iteration 131, loss = 0.14595312\n",
      "Iteration 132, loss = 0.14578852\n",
      "Iteration 133, loss = 0.14557334\n",
      "Iteration 134, loss = 0.14540777\n",
      "Iteration 135, loss = 0.14525298\n",
      "Iteration 136, loss = 0.14503311\n",
      "Iteration 137, loss = 0.14489518\n",
      "Iteration 138, loss = 0.14471937\n",
      "Iteration 139, loss = 0.14455484\n",
      "Iteration 140, loss = 0.14438074\n",
      "Iteration 141, loss = 0.14429111\n",
      "Iteration 142, loss = 0.14407621\n",
      "Iteration 143, loss = 0.14391548\n",
      "Iteration 144, loss = 0.14374458\n",
      "Iteration 145, loss = 0.14360042\n",
      "Iteration 146, loss = 0.14346490\n",
      "Iteration 147, loss = 0.14327413\n",
      "Iteration 148, loss = 0.14318049\n",
      "Iteration 149, loss = 0.14304126\n",
      "Iteration 150, loss = 0.14290399\n",
      "Iteration 151, loss = 0.14271873\n",
      "Iteration 152, loss = 0.14259238\n",
      "Iteration 153, loss = 0.14243041\n",
      "Iteration 154, loss = 0.14227279\n",
      "Iteration 155, loss = 0.14209974\n",
      "Iteration 156, loss = 0.14196823\n",
      "Iteration 157, loss = 0.14183658\n",
      "Iteration 158, loss = 0.14167798\n",
      "Iteration 159, loss = 0.14158256\n",
      "Iteration 160, loss = 0.14142422\n",
      "Iteration 161, loss = 0.14127789\n",
      "Iteration 162, loss = 0.14112444\n",
      "Iteration 163, loss = 0.14106188\n",
      "Iteration 164, loss = 0.14088992\n",
      "Iteration 165, loss = 0.14079045\n",
      "Iteration 166, loss = 0.14062328\n",
      "Iteration 167, loss = 0.14048198\n",
      "Iteration 168, loss = 0.14035353\n",
      "Iteration 169, loss = 0.14028693\n",
      "Iteration 170, loss = 0.14009055\n",
      "Iteration 171, loss = 0.14002750\n",
      "Iteration 172, loss = 0.13982191\n",
      "Iteration 173, loss = 0.13972215\n",
      "Iteration 174, loss = 0.13960164\n",
      "Iteration 175, loss = 0.13947288\n",
      "Iteration 176, loss = 0.13932827\n",
      "Iteration 177, loss = 0.13920457\n",
      "Iteration 178, loss = 0.13913726\n",
      "Iteration 179, loss = 0.13899413\n",
      "Iteration 180, loss = 0.13888332\n",
      "Iteration 181, loss = 0.13871143\n",
      "Iteration 182, loss = 0.13860723\n",
      "Iteration 183, loss = 0.13855914\n",
      "Iteration 184, loss = 0.13837554\n",
      "Iteration 185, loss = 0.13836978\n",
      "Iteration 186, loss = 0.13822299\n",
      "Iteration 187, loss = 0.13806405\n",
      "Iteration 188, loss = 0.13793210\n",
      "Iteration 189, loss = 0.13782720\n",
      "Iteration 190, loss = 0.13769822\n",
      "Iteration 191, loss = 0.13757514\n",
      "Iteration 192, loss = 0.13751228\n",
      "Iteration 193, loss = 0.13738966\n",
      "Iteration 194, loss = 0.13728318\n",
      "Iteration 195, loss = 0.13714000\n",
      "Iteration 196, loss = 0.13705918\n",
      "Iteration 197, loss = 0.13693239\n",
      "Iteration 198, loss = 0.13684285\n",
      "Iteration 199, loss = 0.13674911\n",
      "Iteration 200, loss = 0.13661780\n",
      "Iteration 201, loss = 0.13652788\n",
      "Iteration 202, loss = 0.13642768\n",
      "Iteration 203, loss = 0.13631904\n",
      "Iteration 204, loss = 0.13619528\n",
      "Iteration 205, loss = 0.13607112\n",
      "Iteration 206, loss = 0.13597794\n",
      "Iteration 207, loss = 0.13586673\n",
      "Iteration 208, loss = 0.13577373\n",
      "Iteration 209, loss = 0.13567995\n",
      "Iteration 210, loss = 0.13551993\n",
      "Iteration 211, loss = 0.13541852\n",
      "Iteration 212, loss = 0.13537651\n",
      "Iteration 213, loss = 0.13517860\n",
      "Iteration 214, loss = 0.13509923\n",
      "Iteration 215, loss = 0.13492565\n",
      "Iteration 216, loss = 0.13477512\n",
      "Iteration 217, loss = 0.13466573\n",
      "Iteration 218, loss = 0.13453459\n",
      "Iteration 219, loss = 0.13453749\n",
      "Iteration 220, loss = 0.13433491\n",
      "Iteration 221, loss = 0.13426731\n",
      "Iteration 222, loss = 0.13410090\n",
      "Iteration 223, loss = 0.13397556\n",
      "Iteration 224, loss = 0.13384442\n",
      "Iteration 225, loss = 0.13372843\n",
      "Iteration 226, loss = 0.13362932\n",
      "Iteration 227, loss = 0.13352456\n",
      "Iteration 228, loss = 0.13354845\n",
      "Iteration 229, loss = 0.13330919\n",
      "Iteration 230, loss = 0.13327088\n",
      "Iteration 231, loss = 0.13308175\n",
      "Iteration 232, loss = 0.13294766\n",
      "Iteration 233, loss = 0.13285955\n",
      "Iteration 234, loss = 0.13276706\n",
      "Iteration 235, loss = 0.13267586\n",
      "Iteration 236, loss = 0.13254186\n",
      "Iteration 237, loss = 0.13240493\n",
      "Iteration 238, loss = 0.13233184\n",
      "Iteration 239, loss = 0.13219843\n",
      "Iteration 240, loss = 0.13209185\n",
      "Iteration 241, loss = 0.13200595\n",
      "Iteration 242, loss = 0.13193018\n",
      "Iteration 243, loss = 0.13179317\n",
      "Iteration 244, loss = 0.13171390\n",
      "Iteration 245, loss = 0.13156644\n",
      "Iteration 246, loss = 0.13152124\n",
      "Iteration 247, loss = 0.13134304\n",
      "Iteration 248, loss = 0.13128671\n",
      "Iteration 249, loss = 0.13123689\n",
      "Iteration 250, loss = 0.13107320\n",
      "Iteration 251, loss = 0.13094024\n",
      "Iteration 252, loss = 0.13090223\n",
      "Iteration 253, loss = 0.13085047\n",
      "Iteration 254, loss = 0.13066019\n",
      "Iteration 255, loss = 0.13054504\n",
      "Iteration 256, loss = 0.13049484\n",
      "Iteration 257, loss = 0.13048843\n",
      "Iteration 258, loss = 0.13028395\n",
      "Iteration 259, loss = 0.13017863\n",
      "Iteration 260, loss = 0.13005228\n",
      "Iteration 261, loss = 0.12994796\n",
      "Iteration 262, loss = 0.12982884\n",
      "Iteration 263, loss = 0.12974234\n",
      "Iteration 264, loss = 0.12967416\n",
      "Iteration 265, loss = 0.12955438\n",
      "Iteration 266, loss = 0.12947644\n",
      "Iteration 267, loss = 0.12939285\n",
      "Iteration 268, loss = 0.12923583\n",
      "Iteration 269, loss = 0.12916349\n",
      "Iteration 270, loss = 0.12907292\n",
      "Iteration 271, loss = 0.12896839\n",
      "Iteration 272, loss = 0.12888599\n",
      "Iteration 273, loss = 0.12876925\n",
      "Iteration 274, loss = 0.12868511\n",
      "Iteration 275, loss = 0.12855854\n",
      "Iteration 276, loss = 0.12846693\n",
      "Iteration 277, loss = 0.12839721\n",
      "Iteration 278, loss = 0.12832896\n",
      "Iteration 279, loss = 0.12821547\n",
      "Iteration 280, loss = 0.12807204\n",
      "Iteration 281, loss = 0.12799919\n",
      "Iteration 282, loss = 0.12794272\n",
      "Iteration 283, loss = 0.12781025\n",
      "Iteration 284, loss = 0.12770714\n",
      "Iteration 285, loss = 0.12757143\n",
      "Iteration 286, loss = 0.12753520\n",
      "Iteration 287, loss = 0.12749032\n",
      "Iteration 288, loss = 0.12734700\n",
      "Iteration 289, loss = 0.12722405\n",
      "Iteration 290, loss = 0.12713357\n",
      "Iteration 291, loss = 0.12707165\n",
      "Iteration 292, loss = 0.12696185\n",
      "Iteration 293, loss = 0.12693282\n",
      "Iteration 294, loss = 0.12681592\n",
      "Iteration 295, loss = 0.12670323\n",
      "Iteration 296, loss = 0.12661377\n",
      "Iteration 297, loss = 0.12653329\n",
      "Iteration 298, loss = 0.12647664\n",
      "Iteration 299, loss = 0.12641308\n",
      "Iteration 300, loss = 0.12634295\n",
      "Iteration 301, loss = 0.12624268\n",
      "Iteration 302, loss = 0.12614599\n",
      "Iteration 303, loss = 0.12609690\n",
      "Iteration 304, loss = 0.12599219\n",
      "Iteration 305, loss = 0.12592704\n",
      "Iteration 306, loss = 0.12582085\n",
      "Iteration 307, loss = 0.12578468\n",
      "Iteration 308, loss = 0.12566743\n",
      "Iteration 309, loss = 0.12556365\n",
      "Iteration 310, loss = 0.12553046\n",
      "Iteration 311, loss = 0.12543255\n",
      "Iteration 312, loss = 0.12536137\n",
      "Iteration 313, loss = 0.12530101\n",
      "Iteration 314, loss = 0.12518518\n",
      "Iteration 315, loss = 0.12513295\n",
      "Iteration 316, loss = 0.12510004\n",
      "Iteration 317, loss = 0.12499004\n",
      "Iteration 318, loss = 0.12489301\n",
      "Iteration 319, loss = 0.12484193\n",
      "Iteration 320, loss = 0.12474632\n",
      "Iteration 321, loss = 0.12469070\n",
      "Iteration 322, loss = 0.12463421\n",
      "Iteration 323, loss = 0.12457702\n",
      "Iteration 324, loss = 0.12452376\n",
      "Iteration 325, loss = 0.12443977\n",
      "Iteration 326, loss = 0.12436252\n",
      "Iteration 327, loss = 0.12424618\n",
      "Iteration 328, loss = 0.12417807\n",
      "Iteration 329, loss = 0.12414211\n",
      "Iteration 330, loss = 0.12410406\n",
      "Iteration 331, loss = 0.12401984\n",
      "Iteration 332, loss = 0.12388953\n",
      "Iteration 333, loss = 0.12383563\n",
      "Iteration 334, loss = 0.12376106\n",
      "Iteration 335, loss = 0.12369486\n",
      "Iteration 336, loss = 0.12370077\n",
      "Iteration 337, loss = 0.12354946\n",
      "Iteration 338, loss = 0.12345011\n",
      "Iteration 339, loss = 0.12342313\n",
      "Iteration 340, loss = 0.12334617\n",
      "Iteration 341, loss = 0.12331627\n",
      "Iteration 342, loss = 0.12320468\n",
      "Iteration 343, loss = 0.12312773\n",
      "Iteration 344, loss = 0.12306182\n",
      "Iteration 345, loss = 0.12303641\n",
      "Iteration 346, loss = 0.12293860\n",
      "Iteration 347, loss = 0.12286518\n",
      "Iteration 348, loss = 0.12283789\n",
      "Iteration 349, loss = 0.12276501\n",
      "Iteration 350, loss = 0.12271884\n",
      "Iteration 351, loss = 0.12262157\n",
      "Iteration 352, loss = 0.12252391\n",
      "Iteration 353, loss = 0.12247338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96362296\n",
      "Iteration 2, loss = 0.82945199\n",
      "Iteration 3, loss = 0.69461995\n",
      "Iteration 4, loss = 0.59101039\n",
      "Iteration 5, loss = 0.51483582\n",
      "Iteration 6, loss = 0.45942562\n",
      "Iteration 7, loss = 0.41741278\n",
      "Iteration 8, loss = 0.38451944\n",
      "Iteration 9, loss = 0.35847547\n",
      "Iteration 10, loss = 0.33696427\n",
      "Iteration 11, loss = 0.31935711\n",
      "Iteration 12, loss = 0.30402019\n",
      "Iteration 13, loss = 0.29119599\n",
      "Iteration 14, loss = 0.27986194\n",
      "Iteration 15, loss = 0.27000099\n",
      "Iteration 16, loss = 0.26145376\n",
      "Iteration 17, loss = 0.25375389\n",
      "Iteration 18, loss = 0.24712250\n",
      "Iteration 19, loss = 0.24133622\n",
      "Iteration 20, loss = 0.23588646\n",
      "Iteration 21, loss = 0.23107357\n",
      "Iteration 22, loss = 0.22680212\n",
      "Iteration 23, loss = 0.22279017\n",
      "Iteration 24, loss = 0.21913686\n",
      "Iteration 25, loss = 0.21582366\n",
      "Iteration 26, loss = 0.21268899\n",
      "Iteration 27, loss = 0.20985233\n",
      "Iteration 28, loss = 0.20724594\n",
      "Iteration 29, loss = 0.20475411\n",
      "Iteration 30, loss = 0.20250543\n",
      "Iteration 31, loss = 0.20036990\n",
      "Iteration 32, loss = 0.19833323\n",
      "Iteration 33, loss = 0.19642908\n",
      "Iteration 34, loss = 0.19461130\n",
      "Iteration 35, loss = 0.19295199\n",
      "Iteration 36, loss = 0.19134647\n",
      "Iteration 37, loss = 0.18978053\n",
      "Iteration 38, loss = 0.18829294\n",
      "Iteration 39, loss = 0.18692618\n",
      "Iteration 40, loss = 0.18545416\n",
      "Iteration 41, loss = 0.18423743\n",
      "Iteration 42, loss = 0.18294429\n",
      "Iteration 43, loss = 0.18173697\n",
      "Iteration 44, loss = 0.18063373\n",
      "Iteration 45, loss = 0.17944735\n",
      "Iteration 46, loss = 0.17836707\n",
      "Iteration 47, loss = 0.17735027\n",
      "Iteration 48, loss = 0.17631932\n",
      "Iteration 49, loss = 0.17534180\n",
      "Iteration 50, loss = 0.17432987\n",
      "Iteration 51, loss = 0.17344142\n",
      "Iteration 52, loss = 0.17245402\n",
      "Iteration 53, loss = 0.17153337\n",
      "Iteration 54, loss = 0.17069908\n",
      "Iteration 55, loss = 0.16985354\n",
      "Iteration 56, loss = 0.16909295\n",
      "Iteration 57, loss = 0.16827568\n",
      "Iteration 58, loss = 0.16753393\n",
      "Iteration 59, loss = 0.16671738\n",
      "Iteration 60, loss = 0.16600356\n",
      "Iteration 61, loss = 0.16530467\n",
      "Iteration 62, loss = 0.16461613\n",
      "Iteration 63, loss = 0.16397042\n",
      "Iteration 64, loss = 0.16328249\n",
      "Iteration 65, loss = 0.16262933\n",
      "Iteration 66, loss = 0.16205180\n",
      "Iteration 67, loss = 0.16149132\n",
      "Iteration 68, loss = 0.16087449\n",
      "Iteration 69, loss = 0.16038425\n",
      "Iteration 70, loss = 0.15984038\n",
      "Iteration 71, loss = 0.15932603\n",
      "Iteration 72, loss = 0.15886256\n",
      "Iteration 73, loss = 0.15830021\n",
      "Iteration 74, loss = 0.15785648\n",
      "Iteration 75, loss = 0.15735999\n",
      "Iteration 76, loss = 0.15695377\n",
      "Iteration 77, loss = 0.15642959\n",
      "Iteration 78, loss = 0.15598004\n",
      "Iteration 79, loss = 0.15553719\n",
      "Iteration 80, loss = 0.15512716\n",
      "Iteration 81, loss = 0.15473186\n",
      "Iteration 82, loss = 0.15435628\n",
      "Iteration 83, loss = 0.15391187\n",
      "Iteration 84, loss = 0.15352836\n",
      "Iteration 85, loss = 0.15316594\n",
      "Iteration 86, loss = 0.15281298\n",
      "Iteration 87, loss = 0.15248520\n",
      "Iteration 88, loss = 0.15210354\n",
      "Iteration 89, loss = 0.15172440\n",
      "Iteration 90, loss = 0.15135474\n",
      "Iteration 91, loss = 0.15099341\n",
      "Iteration 92, loss = 0.15064466\n",
      "Iteration 93, loss = 0.15041212\n",
      "Iteration 94, loss = 0.15001771\n",
      "Iteration 95, loss = 0.14969769\n",
      "Iteration 96, loss = 0.14942271\n",
      "Iteration 97, loss = 0.14912785\n",
      "Iteration 98, loss = 0.14882130\n",
      "Iteration 99, loss = 0.14851357\n",
      "Iteration 100, loss = 0.14828811\n",
      "Iteration 101, loss = 0.14796604\n",
      "Iteration 102, loss = 0.14770715\n",
      "Iteration 103, loss = 0.14739865\n",
      "Iteration 104, loss = 0.14716634\n",
      "Iteration 105, loss = 0.14687988\n",
      "Iteration 106, loss = 0.14662329\n",
      "Iteration 107, loss = 0.14637030\n",
      "Iteration 108, loss = 0.14616017\n",
      "Iteration 109, loss = 0.14587583\n",
      "Iteration 110, loss = 0.14569710\n",
      "Iteration 111, loss = 0.14540219\n",
      "Iteration 112, loss = 0.14519196\n",
      "Iteration 113, loss = 0.14497591\n",
      "Iteration 114, loss = 0.14470088\n",
      "Iteration 115, loss = 0.14444504\n",
      "Iteration 116, loss = 0.14428804\n",
      "Iteration 117, loss = 0.14399590\n",
      "Iteration 118, loss = 0.14380947\n",
      "Iteration 119, loss = 0.14357859\n",
      "Iteration 120, loss = 0.14337520\n",
      "Iteration 121, loss = 0.14317592\n",
      "Iteration 122, loss = 0.14291123\n",
      "Iteration 123, loss = 0.14271098\n",
      "Iteration 124, loss = 0.14254733\n",
      "Iteration 125, loss = 0.14232118\n",
      "Iteration 126, loss = 0.14216565\n",
      "Iteration 127, loss = 0.14192598\n",
      "Iteration 128, loss = 0.14176456\n",
      "Iteration 129, loss = 0.14157454\n",
      "Iteration 130, loss = 0.14145472\n",
      "Iteration 131, loss = 0.14119484\n",
      "Iteration 132, loss = 0.14102838\n",
      "Iteration 133, loss = 0.14081753\n",
      "Iteration 134, loss = 0.14064335\n",
      "Iteration 135, loss = 0.14049032\n",
      "Iteration 136, loss = 0.14027946\n",
      "Iteration 137, loss = 0.14012582\n",
      "Iteration 138, loss = 0.13994867\n",
      "Iteration 139, loss = 0.13978132\n",
      "Iteration 140, loss = 0.13961939\n",
      "Iteration 141, loss = 0.13948811\n",
      "Iteration 142, loss = 0.13929713\n",
      "Iteration 143, loss = 0.13911416\n",
      "Iteration 144, loss = 0.13895257\n",
      "Iteration 145, loss = 0.13879888\n",
      "Iteration 146, loss = 0.13869976\n",
      "Iteration 147, loss = 0.13848998\n",
      "Iteration 148, loss = 0.13839118\n",
      "Iteration 149, loss = 0.13825859\n",
      "Iteration 150, loss = 0.13808685\n",
      "Iteration 151, loss = 0.13790482\n",
      "Iteration 152, loss = 0.13779605\n",
      "Iteration 153, loss = 0.13762514\n",
      "Iteration 154, loss = 0.13744925\n",
      "Iteration 155, loss = 0.13732008\n",
      "Iteration 156, loss = 0.13715835\n",
      "Iteration 157, loss = 0.13701918\n",
      "Iteration 158, loss = 0.13686019\n",
      "Iteration 159, loss = 0.13676912\n",
      "Iteration 160, loss = 0.13659476\n",
      "Iteration 161, loss = 0.13645418\n",
      "Iteration 162, loss = 0.13629728\n",
      "Iteration 163, loss = 0.13623724\n",
      "Iteration 164, loss = 0.13603501\n",
      "Iteration 165, loss = 0.13594339\n",
      "Iteration 166, loss = 0.13575614\n",
      "Iteration 167, loss = 0.13562945\n",
      "Iteration 168, loss = 0.13548510\n",
      "Iteration 169, loss = 0.13542193\n",
      "Iteration 170, loss = 0.13521951\n",
      "Iteration 171, loss = 0.13515869\n",
      "Iteration 172, loss = 0.13493988\n",
      "Iteration 173, loss = 0.13482321\n",
      "Iteration 174, loss = 0.13473050\n",
      "Iteration 175, loss = 0.13458850\n",
      "Iteration 176, loss = 0.13444624\n",
      "Iteration 177, loss = 0.13431929\n",
      "Iteration 178, loss = 0.13426097\n",
      "Iteration 179, loss = 0.13408952\n",
      "Iteration 180, loss = 0.13396589\n",
      "Iteration 181, loss = 0.13379149\n",
      "Iteration 182, loss = 0.13368927\n",
      "Iteration 183, loss = 0.13361813\n",
      "Iteration 184, loss = 0.13343481\n",
      "Iteration 185, loss = 0.13341009\n",
      "Iteration 186, loss = 0.13326391\n",
      "Iteration 187, loss = 0.13309834\n",
      "Iteration 188, loss = 0.13295982\n",
      "Iteration 189, loss = 0.13286020\n",
      "Iteration 190, loss = 0.13271598\n",
      "Iteration 191, loss = 0.13259301\n",
      "Iteration 192, loss = 0.13252762\n",
      "Iteration 193, loss = 0.13237785\n",
      "Iteration 194, loss = 0.13226360\n",
      "Iteration 195, loss = 0.13212720\n",
      "Iteration 196, loss = 0.13204369\n",
      "Iteration 197, loss = 0.13190770\n",
      "Iteration 198, loss = 0.13180503\n",
      "Iteration 199, loss = 0.13169195\n",
      "Iteration 200, loss = 0.13157492\n",
      "Iteration 201, loss = 0.13147807\n",
      "Iteration 202, loss = 0.13137085\n",
      "Iteration 203, loss = 0.13123353\n",
      "Iteration 204, loss = 0.13112267\n",
      "Iteration 205, loss = 0.13099519\n",
      "Iteration 206, loss = 0.13089228\n",
      "Iteration 207, loss = 0.13077679\n",
      "Iteration 208, loss = 0.13068956\n",
      "Iteration 209, loss = 0.13060313\n",
      "Iteration 210, loss = 0.13044179\n",
      "Iteration 211, loss = 0.13035270\n",
      "Iteration 212, loss = 0.13036588\n",
      "Iteration 213, loss = 0.13016712\n",
      "Iteration 214, loss = 0.13008969\n",
      "Iteration 215, loss = 0.12992876\n",
      "Iteration 216, loss = 0.12980349\n",
      "Iteration 217, loss = 0.12970422\n",
      "Iteration 218, loss = 0.12959886\n",
      "Iteration 219, loss = 0.12958899\n",
      "Iteration 220, loss = 0.12941337\n",
      "Iteration 221, loss = 0.12933748\n",
      "Iteration 222, loss = 0.12919041\n",
      "Iteration 223, loss = 0.12908564\n",
      "Iteration 224, loss = 0.12896957\n",
      "Iteration 225, loss = 0.12888083\n",
      "Iteration 226, loss = 0.12876164\n",
      "Iteration 227, loss = 0.12867780\n",
      "Iteration 228, loss = 0.12866945\n",
      "Iteration 229, loss = 0.12848628\n",
      "Iteration 230, loss = 0.12843083\n",
      "Iteration 231, loss = 0.12828056\n",
      "Iteration 232, loss = 0.12815530\n",
      "Iteration 233, loss = 0.12805593\n",
      "Iteration 234, loss = 0.12798208\n",
      "Iteration 235, loss = 0.12788789\n",
      "Iteration 236, loss = 0.12776006\n",
      "Iteration 237, loss = 0.12764086\n",
      "Iteration 238, loss = 0.12755797\n",
      "Iteration 239, loss = 0.12741941\n",
      "Iteration 240, loss = 0.12731691\n",
      "Iteration 241, loss = 0.12723855\n",
      "Iteration 242, loss = 0.12717355\n",
      "Iteration 243, loss = 0.12706173\n",
      "Iteration 244, loss = 0.12696314\n",
      "Iteration 245, loss = 0.12684816\n",
      "Iteration 246, loss = 0.12677982\n",
      "Iteration 247, loss = 0.12664939\n",
      "Iteration 248, loss = 0.12661463\n",
      "Iteration 249, loss = 0.12654378\n",
      "Iteration 250, loss = 0.12640497\n",
      "Iteration 251, loss = 0.12629875\n",
      "Iteration 252, loss = 0.12624126\n",
      "Iteration 253, loss = 0.12623360\n",
      "Iteration 254, loss = 0.12604552\n",
      "Iteration 255, loss = 0.12593602\n",
      "Iteration 256, loss = 0.12590480\n",
      "Iteration 257, loss = 0.12586689\n",
      "Iteration 258, loss = 0.12573176\n",
      "Iteration 259, loss = 0.12562219\n",
      "Iteration 260, loss = 0.12552777\n",
      "Iteration 261, loss = 0.12542022\n",
      "Iteration 262, loss = 0.12534597\n",
      "Iteration 263, loss = 0.12526453\n",
      "Iteration 264, loss = 0.12521968\n",
      "Iteration 265, loss = 0.12511056\n",
      "Iteration 266, loss = 0.12503834\n",
      "Iteration 267, loss = 0.12497455\n",
      "Iteration 268, loss = 0.12482850\n",
      "Iteration 269, loss = 0.12477871\n",
      "Iteration 270, loss = 0.12470454\n",
      "Iteration 271, loss = 0.12461004\n",
      "Iteration 272, loss = 0.12454835\n",
      "Iteration 273, loss = 0.12443815\n",
      "Iteration 274, loss = 0.12437944\n",
      "Iteration 275, loss = 0.12426810\n",
      "Iteration 276, loss = 0.12418810\n",
      "Iteration 277, loss = 0.12413430\n",
      "Iteration 278, loss = 0.12407119\n",
      "Iteration 279, loss = 0.12396882\n",
      "Iteration 280, loss = 0.12387481\n",
      "Iteration 281, loss = 0.12379987\n",
      "Iteration 282, loss = 0.12377314\n",
      "Iteration 283, loss = 0.12366829\n",
      "Iteration 284, loss = 0.12357151\n",
      "Iteration 285, loss = 0.12347651\n",
      "Iteration 286, loss = 0.12342653\n",
      "Iteration 287, loss = 0.12338565\n",
      "Iteration 288, loss = 0.12328180\n",
      "Iteration 289, loss = 0.12317900\n",
      "Iteration 290, loss = 0.12310221\n",
      "Iteration 291, loss = 0.12304452\n",
      "Iteration 292, loss = 0.12296327\n",
      "Iteration 293, loss = 0.12294749\n",
      "Iteration 294, loss = 0.12281251\n",
      "Iteration 295, loss = 0.12271607\n",
      "Iteration 296, loss = 0.12263217\n",
      "Iteration 297, loss = 0.12257306\n",
      "Iteration 298, loss = 0.12252721\n",
      "Iteration 299, loss = 0.12246063\n",
      "Iteration 300, loss = 0.12240147\n",
      "Iteration 301, loss = 0.12228555\n",
      "Iteration 302, loss = 0.12221571\n",
      "Iteration 303, loss = 0.12217401\n",
      "Iteration 304, loss = 0.12210008\n",
      "Iteration 305, loss = 0.12201633\n",
      "Iteration 306, loss = 0.12192687\n",
      "Iteration 307, loss = 0.12189636\n",
      "Iteration 308, loss = 0.12178188\n",
      "Iteration 309, loss = 0.12169431\n",
      "Iteration 310, loss = 0.12165220\n",
      "Iteration 311, loss = 0.12155275\n",
      "Iteration 312, loss = 0.12150286\n",
      "Iteration 313, loss = 0.12145705\n",
      "Iteration 314, loss = 0.12132988\n",
      "Iteration 315, loss = 0.12127856\n",
      "Iteration 316, loss = 0.12124529\n",
      "Iteration 317, loss = 0.12116019\n",
      "Iteration 318, loss = 0.12103841\n",
      "Iteration 319, loss = 0.12100267\n",
      "Iteration 320, loss = 0.12090231\n",
      "Iteration 321, loss = 0.12085366\n",
      "Iteration 322, loss = 0.12079079\n",
      "Iteration 323, loss = 0.12076513\n",
      "Iteration 324, loss = 0.12071549\n",
      "Iteration 325, loss = 0.12062659\n",
      "Iteration 326, loss = 0.12051896\n",
      "Iteration 327, loss = 0.12042382\n",
      "Iteration 328, loss = 0.12036268\n",
      "Iteration 329, loss = 0.12032277\n",
      "Iteration 330, loss = 0.12028163\n",
      "Iteration 331, loss = 0.12020566\n",
      "Iteration 332, loss = 0.12009137\n",
      "Iteration 333, loss = 0.12004484\n",
      "Iteration 334, loss = 0.11998407\n",
      "Iteration 335, loss = 0.11990795\n",
      "Iteration 336, loss = 0.11989124\n",
      "Iteration 337, loss = 0.11975740\n",
      "Iteration 338, loss = 0.11968535\n",
      "Iteration 339, loss = 0.11965155\n",
      "Iteration 340, loss = 0.11958104\n",
      "Iteration 341, loss = 0.11954785\n",
      "Iteration 342, loss = 0.11943699\n",
      "Iteration 343, loss = 0.11935600\n",
      "Iteration 344, loss = 0.11931048\n",
      "Iteration 345, loss = 0.11929526\n",
      "Iteration 346, loss = 0.11918168\n",
      "Iteration 347, loss = 0.11911797\n",
      "Iteration 348, loss = 0.11910176\n",
      "Iteration 349, loss = 0.11902209\n",
      "Iteration 350, loss = 0.11896979\n",
      "Iteration 351, loss = 0.11889681\n",
      "Iteration 352, loss = 0.11880483\n",
      "Iteration 353, loss = 0.11874370\n",
      "Iteration 354, loss = 0.11872412\n",
      "Iteration 355, loss = 0.11867947\n",
      "Iteration 356, loss = 0.11859183\n",
      "Iteration 357, loss = 0.11850172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96614253\n",
      "Iteration 2, loss = 0.83150826\n",
      "Iteration 3, loss = 0.69609331\n",
      "Iteration 4, loss = 0.59140491\n",
      "Iteration 5, loss = 0.51414612\n",
      "Iteration 6, loss = 0.45764602\n",
      "Iteration 7, loss = 0.41458963\n",
      "Iteration 8, loss = 0.38092422\n",
      "Iteration 9, loss = 0.35385240\n",
      "Iteration 10, loss = 0.33196637\n",
      "Iteration 11, loss = 0.31377858\n",
      "Iteration 12, loss = 0.29780348\n",
      "Iteration 13, loss = 0.28446900\n",
      "Iteration 14, loss = 0.27267775\n",
      "Iteration 15, loss = 0.26255200\n",
      "Iteration 16, loss = 0.25376682\n",
      "Iteration 17, loss = 0.24594699\n",
      "Iteration 18, loss = 0.23908266\n",
      "Iteration 19, loss = 0.23324244\n",
      "Iteration 20, loss = 0.22757408\n",
      "Iteration 21, loss = 0.22262949\n",
      "Iteration 22, loss = 0.21824481\n",
      "Iteration 23, loss = 0.21405872\n",
      "Iteration 24, loss = 0.21032402\n",
      "Iteration 25, loss = 0.20694840\n",
      "Iteration 26, loss = 0.20375475\n",
      "Iteration 27, loss = 0.20093930\n",
      "Iteration 28, loss = 0.19826102\n",
      "Iteration 29, loss = 0.19572807\n",
      "Iteration 30, loss = 0.19345415\n",
      "Iteration 31, loss = 0.19138037\n",
      "Iteration 32, loss = 0.18932514\n",
      "Iteration 33, loss = 0.18748816\n",
      "Iteration 34, loss = 0.18561639\n",
      "Iteration 35, loss = 0.18406112\n",
      "Iteration 36, loss = 0.18247358\n",
      "Iteration 37, loss = 0.18099566\n",
      "Iteration 38, loss = 0.17953617\n",
      "Iteration 39, loss = 0.17826103\n",
      "Iteration 40, loss = 0.17688130\n",
      "Iteration 41, loss = 0.17574436\n",
      "Iteration 42, loss = 0.17454608\n",
      "Iteration 43, loss = 0.17342780\n",
      "Iteration 44, loss = 0.17237001\n",
      "Iteration 45, loss = 0.17129226\n",
      "Iteration 46, loss = 0.17030680\n",
      "Iteration 47, loss = 0.16937183\n",
      "Iteration 48, loss = 0.16844312\n",
      "Iteration 49, loss = 0.16757040\n",
      "Iteration 50, loss = 0.16659299\n",
      "Iteration 51, loss = 0.16580866\n",
      "Iteration 52, loss = 0.16487821\n",
      "Iteration 53, loss = 0.16401264\n",
      "Iteration 54, loss = 0.16324835\n",
      "Iteration 55, loss = 0.16236064\n",
      "Iteration 56, loss = 0.16162554\n",
      "Iteration 57, loss = 0.16074395\n",
      "Iteration 58, loss = 0.16002629\n",
      "Iteration 59, loss = 0.15919867\n",
      "Iteration 60, loss = 0.15846655\n",
      "Iteration 61, loss = 0.15775043\n",
      "Iteration 62, loss = 0.15699670\n",
      "Iteration 63, loss = 0.15635550\n",
      "Iteration 64, loss = 0.15564970\n",
      "Iteration 65, loss = 0.15496253\n",
      "Iteration 66, loss = 0.15432189\n",
      "Iteration 67, loss = 0.15372429\n",
      "Iteration 68, loss = 0.15304386\n",
      "Iteration 69, loss = 0.15248936\n",
      "Iteration 70, loss = 0.15192332\n",
      "Iteration 71, loss = 0.15134337\n",
      "Iteration 72, loss = 0.15082830\n",
      "Iteration 73, loss = 0.15026282\n",
      "Iteration 74, loss = 0.14980044\n",
      "Iteration 75, loss = 0.14924206\n",
      "Iteration 76, loss = 0.14878390\n",
      "Iteration 77, loss = 0.14822404\n",
      "Iteration 78, loss = 0.14774745\n",
      "Iteration 79, loss = 0.14726449\n",
      "Iteration 80, loss = 0.14681465\n",
      "Iteration 81, loss = 0.14638005\n",
      "Iteration 82, loss = 0.14599466\n",
      "Iteration 83, loss = 0.14545182\n",
      "Iteration 84, loss = 0.14501116\n",
      "Iteration 85, loss = 0.14460886\n",
      "Iteration 86, loss = 0.14421080\n",
      "Iteration 87, loss = 0.14380664\n",
      "Iteration 88, loss = 0.14340836\n",
      "Iteration 89, loss = 0.14298233\n",
      "Iteration 90, loss = 0.14261882\n",
      "Iteration 91, loss = 0.14220564\n",
      "Iteration 92, loss = 0.14182766\n",
      "Iteration 93, loss = 0.14154399\n",
      "Iteration 94, loss = 0.14113209\n",
      "Iteration 95, loss = 0.14076598\n",
      "Iteration 96, loss = 0.14048398\n",
      "Iteration 97, loss = 0.14013202\n",
      "Iteration 98, loss = 0.13979430\n",
      "Iteration 99, loss = 0.13946232\n",
      "Iteration 100, loss = 0.13921872\n",
      "Iteration 101, loss = 0.13886335\n",
      "Iteration 102, loss = 0.13855579\n",
      "Iteration 103, loss = 0.13820877\n",
      "Iteration 104, loss = 0.13793503\n",
      "Iteration 105, loss = 0.13760222\n",
      "Iteration 106, loss = 0.13730550\n",
      "Iteration 107, loss = 0.13703763\n",
      "Iteration 108, loss = 0.13678654\n",
      "Iteration 109, loss = 0.13648327\n",
      "Iteration 110, loss = 0.13625613\n",
      "Iteration 111, loss = 0.13592623\n",
      "Iteration 112, loss = 0.13568513\n",
      "Iteration 113, loss = 0.13544978\n",
      "Iteration 114, loss = 0.13513178\n",
      "Iteration 115, loss = 0.13486697\n",
      "Iteration 116, loss = 0.13465046\n",
      "Iteration 117, loss = 0.13436642\n",
      "Iteration 118, loss = 0.13418608\n",
      "Iteration 119, loss = 0.13392323\n",
      "Iteration 120, loss = 0.13371870\n",
      "Iteration 121, loss = 0.13350947\n",
      "Iteration 122, loss = 0.13321950\n",
      "Iteration 123, loss = 0.13298690\n",
      "Iteration 124, loss = 0.13280447\n",
      "Iteration 125, loss = 0.13255788\n",
      "Iteration 126, loss = 0.13241791\n",
      "Iteration 127, loss = 0.13212996\n",
      "Iteration 128, loss = 0.13197904\n",
      "Iteration 129, loss = 0.13176527\n",
      "Iteration 130, loss = 0.13160438\n",
      "Iteration 131, loss = 0.13135039\n",
      "Iteration 132, loss = 0.13119631\n",
      "Iteration 133, loss = 0.13095485\n",
      "Iteration 134, loss = 0.13078193\n",
      "Iteration 135, loss = 0.13059068\n",
      "Iteration 136, loss = 0.13040671\n",
      "Iteration 137, loss = 0.13023635\n",
      "Iteration 138, loss = 0.13005471\n",
      "Iteration 139, loss = 0.12984479\n",
      "Iteration 140, loss = 0.12969531\n",
      "Iteration 141, loss = 0.12951936\n",
      "Iteration 142, loss = 0.12930199\n",
      "Iteration 143, loss = 0.12910323\n",
      "Iteration 144, loss = 0.12891574\n",
      "Iteration 145, loss = 0.12874532\n",
      "Iteration 146, loss = 0.12862319\n",
      "Iteration 147, loss = 0.12840633\n",
      "Iteration 148, loss = 0.12825900\n",
      "Iteration 149, loss = 0.12809936\n",
      "Iteration 150, loss = 0.12790526\n",
      "Iteration 151, loss = 0.12770363\n",
      "Iteration 152, loss = 0.12756724\n",
      "Iteration 153, loss = 0.12739950\n",
      "Iteration 154, loss = 0.12723243\n",
      "Iteration 155, loss = 0.12707272\n",
      "Iteration 156, loss = 0.12691542\n",
      "Iteration 157, loss = 0.12673679\n",
      "Iteration 158, loss = 0.12658497\n",
      "Iteration 159, loss = 0.12647347\n",
      "Iteration 160, loss = 0.12630589\n",
      "Iteration 161, loss = 0.12613895\n",
      "Iteration 162, loss = 0.12600480\n",
      "Iteration 163, loss = 0.12585809\n",
      "Iteration 164, loss = 0.12567222\n",
      "Iteration 165, loss = 0.12558108\n",
      "Iteration 166, loss = 0.12537256\n",
      "Iteration 167, loss = 0.12521570\n",
      "Iteration 168, loss = 0.12508138\n",
      "Iteration 169, loss = 0.12498905\n",
      "Iteration 170, loss = 0.12480686\n",
      "Iteration 171, loss = 0.12470171\n",
      "Iteration 172, loss = 0.12450376\n",
      "Iteration 173, loss = 0.12437324\n",
      "Iteration 174, loss = 0.12424119\n",
      "Iteration 175, loss = 0.12414650\n",
      "Iteration 176, loss = 0.12394038\n",
      "Iteration 177, loss = 0.12382422\n",
      "Iteration 178, loss = 0.12375581\n",
      "Iteration 179, loss = 0.12356561\n",
      "Iteration 180, loss = 0.12344070\n",
      "Iteration 181, loss = 0.12325432\n",
      "Iteration 182, loss = 0.12315958\n",
      "Iteration 183, loss = 0.12305854\n",
      "Iteration 184, loss = 0.12287321\n",
      "Iteration 185, loss = 0.12282270\n",
      "Iteration 186, loss = 0.12270094\n",
      "Iteration 187, loss = 0.12252079\n",
      "Iteration 188, loss = 0.12239670\n",
      "Iteration 189, loss = 0.12226902\n",
      "Iteration 190, loss = 0.12213061\n",
      "Iteration 191, loss = 0.12201353\n",
      "Iteration 192, loss = 0.12199063\n",
      "Iteration 193, loss = 0.12177332\n",
      "Iteration 194, loss = 0.12166395\n",
      "Iteration 195, loss = 0.12154815\n",
      "Iteration 196, loss = 0.12143293\n",
      "Iteration 197, loss = 0.12130946\n",
      "Iteration 198, loss = 0.12121847\n",
      "Iteration 199, loss = 0.12108231\n",
      "Iteration 200, loss = 0.12097164\n",
      "Iteration 201, loss = 0.12087181\n",
      "Iteration 202, loss = 0.12072092\n",
      "Iteration 203, loss = 0.12058617\n",
      "Iteration 204, loss = 0.12046715\n",
      "Iteration 205, loss = 0.12035016\n",
      "Iteration 206, loss = 0.12023989\n",
      "Iteration 207, loss = 0.12014738\n",
      "Iteration 208, loss = 0.12004630\n",
      "Iteration 209, loss = 0.11993078\n",
      "Iteration 210, loss = 0.11981519\n",
      "Iteration 211, loss = 0.11972718\n",
      "Iteration 212, loss = 0.11966704\n",
      "Iteration 213, loss = 0.11949287\n",
      "Iteration 214, loss = 0.11940108\n",
      "Iteration 215, loss = 0.11928604\n",
      "Iteration 216, loss = 0.11913469\n",
      "Iteration 217, loss = 0.11903873\n",
      "Iteration 218, loss = 0.11895079\n",
      "Iteration 219, loss = 0.11895729\n",
      "Iteration 220, loss = 0.11874395\n",
      "Iteration 221, loss = 0.11865717\n",
      "Iteration 222, loss = 0.11853516\n",
      "Iteration 223, loss = 0.11844883\n",
      "Iteration 224, loss = 0.11831461\n",
      "Iteration 225, loss = 0.11823402\n",
      "Iteration 226, loss = 0.11810869\n",
      "Iteration 227, loss = 0.11800583\n",
      "Iteration 228, loss = 0.11796566\n",
      "Iteration 229, loss = 0.11782423\n",
      "Iteration 230, loss = 0.11775462\n",
      "Iteration 231, loss = 0.11760226\n",
      "Iteration 232, loss = 0.11750150\n",
      "Iteration 233, loss = 0.11742639\n",
      "Iteration 234, loss = 0.11735180\n",
      "Iteration 235, loss = 0.11724825\n",
      "Iteration 236, loss = 0.11715034\n",
      "Iteration 237, loss = 0.11703067\n",
      "Iteration 238, loss = 0.11695508\n",
      "Iteration 239, loss = 0.11682288\n",
      "Iteration 240, loss = 0.11672049\n",
      "Iteration 241, loss = 0.11663682\n",
      "Iteration 242, loss = 0.11657885\n",
      "Iteration 243, loss = 0.11646740\n",
      "Iteration 244, loss = 0.11640610\n",
      "Iteration 245, loss = 0.11625842\n",
      "Iteration 246, loss = 0.11618535\n",
      "Iteration 247, loss = 0.11606708\n",
      "Iteration 248, loss = 0.11599938\n",
      "Iteration 249, loss = 0.11595100\n",
      "Iteration 250, loss = 0.11579626\n",
      "Iteration 251, loss = 0.11570206\n",
      "Iteration 252, loss = 0.11563520\n",
      "Iteration 253, loss = 0.11559800\n",
      "Iteration 254, loss = 0.11545121\n",
      "Iteration 255, loss = 0.11534821\n",
      "Iteration 256, loss = 0.11531805\n",
      "Iteration 257, loss = 0.11525711\n",
      "Iteration 258, loss = 0.11510509\n",
      "Iteration 259, loss = 0.11502794\n",
      "Iteration 260, loss = 0.11493077\n",
      "Iteration 261, loss = 0.11482448\n",
      "Iteration 262, loss = 0.11474095\n",
      "Iteration 263, loss = 0.11466653\n",
      "Iteration 264, loss = 0.11458508\n",
      "Iteration 265, loss = 0.11445556\n",
      "Iteration 266, loss = 0.11440510\n",
      "Iteration 267, loss = 0.11433897\n",
      "Iteration 268, loss = 0.11418128\n",
      "Iteration 269, loss = 0.11413969\n",
      "Iteration 270, loss = 0.11408111\n",
      "Iteration 271, loss = 0.11393822\n",
      "Iteration 272, loss = 0.11385786\n",
      "Iteration 273, loss = 0.11376539\n",
      "Iteration 274, loss = 0.11369786\n",
      "Iteration 275, loss = 0.11358225\n",
      "Iteration 276, loss = 0.11349614\n",
      "Iteration 277, loss = 0.11340795\n",
      "Iteration 278, loss = 0.11334801\n",
      "Iteration 279, loss = 0.11324284\n",
      "Iteration 280, loss = 0.11315895\n",
      "Iteration 281, loss = 0.11310291\n",
      "Iteration 282, loss = 0.11300137\n",
      "Iteration 283, loss = 0.11296068\n",
      "Iteration 284, loss = 0.11283292\n",
      "Iteration 285, loss = 0.11274113\n",
      "Iteration 286, loss = 0.11269063\n",
      "Iteration 287, loss = 0.11263741\n",
      "Iteration 288, loss = 0.11253182\n",
      "Iteration 289, loss = 0.11244186\n",
      "Iteration 290, loss = 0.11234418\n",
      "Iteration 291, loss = 0.11229276\n",
      "Iteration 292, loss = 0.11223385\n",
      "Iteration 293, loss = 0.11221370\n",
      "Iteration 294, loss = 0.11205996\n",
      "Iteration 295, loss = 0.11199899\n",
      "Iteration 296, loss = 0.11188939\n",
      "Iteration 297, loss = 0.11180604\n",
      "Iteration 298, loss = 0.11176151\n",
      "Iteration 299, loss = 0.11168287\n",
      "Iteration 300, loss = 0.11164929\n",
      "Iteration 301, loss = 0.11150941\n",
      "Iteration 302, loss = 0.11144879\n",
      "Iteration 303, loss = 0.11141662\n",
      "Iteration 304, loss = 0.11131423\n",
      "Iteration 305, loss = 0.11123046\n",
      "Iteration 306, loss = 0.11112670\n",
      "Iteration 307, loss = 0.11108383\n",
      "Iteration 308, loss = 0.11097882\n",
      "Iteration 309, loss = 0.11088115\n",
      "Iteration 310, loss = 0.11083397\n",
      "Iteration 311, loss = 0.11072146\n",
      "Iteration 312, loss = 0.11067447\n",
      "Iteration 313, loss = 0.11060791\n",
      "Iteration 314, loss = 0.11048200\n",
      "Iteration 315, loss = 0.11043441\n",
      "Iteration 316, loss = 0.11038161\n",
      "Iteration 317, loss = 0.11029883\n",
      "Iteration 318, loss = 0.11018580\n",
      "Iteration 319, loss = 0.11013080\n",
      "Iteration 320, loss = 0.11003928\n",
      "Iteration 321, loss = 0.10995909\n",
      "Iteration 322, loss = 0.10990289\n",
      "Iteration 323, loss = 0.10988363\n",
      "Iteration 324, loss = 0.10979386\n",
      "Iteration 325, loss = 0.10972150\n",
      "Iteration 326, loss = 0.10960282\n",
      "Iteration 327, loss = 0.10952819\n",
      "Iteration 328, loss = 0.10945756\n",
      "Iteration 329, loss = 0.10941246\n",
      "Iteration 330, loss = 0.10935516\n",
      "Iteration 331, loss = 0.10928024\n",
      "Iteration 332, loss = 0.10916950\n",
      "Iteration 333, loss = 0.10911497\n",
      "Iteration 334, loss = 0.10905245\n",
      "Iteration 335, loss = 0.10897196\n",
      "Iteration 336, loss = 0.10896421\n",
      "Iteration 337, loss = 0.10880798\n",
      "Iteration 338, loss = 0.10875573\n",
      "Iteration 339, loss = 0.10869024\n",
      "Iteration 340, loss = 0.10860775\n",
      "Iteration 341, loss = 0.10855016\n",
      "Iteration 342, loss = 0.10847139\n",
      "Iteration 343, loss = 0.10840215\n",
      "Iteration 344, loss = 0.10835649\n",
      "Iteration 345, loss = 0.10828465\n",
      "Iteration 346, loss = 0.10820868\n",
      "Iteration 347, loss = 0.10814618\n",
      "Iteration 348, loss = 0.10809719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96911906\n",
      "Iteration 2, loss = 0.83660691\n",
      "Iteration 3, loss = 0.70074358\n",
      "Iteration 4, loss = 0.59664255\n",
      "Iteration 5, loss = 0.51900039\n",
      "Iteration 6, loss = 0.46281721\n",
      "Iteration 7, loss = 0.41927660\n",
      "Iteration 8, loss = 0.38529546\n",
      "Iteration 9, loss = 0.35820625\n",
      "Iteration 10, loss = 0.33613515\n",
      "Iteration 11, loss = 0.31759735\n",
      "Iteration 12, loss = 0.30132174\n",
      "Iteration 13, loss = 0.28767304\n",
      "Iteration 14, loss = 0.27577730\n",
      "Iteration 15, loss = 0.26530615\n",
      "Iteration 16, loss = 0.25633774\n",
      "Iteration 17, loss = 0.24822869\n",
      "Iteration 18, loss = 0.24126199\n",
      "Iteration 19, loss = 0.23503982\n",
      "Iteration 20, loss = 0.22920358\n",
      "Iteration 21, loss = 0.22399163\n",
      "Iteration 22, loss = 0.21945173\n",
      "Iteration 23, loss = 0.21499320\n",
      "Iteration 24, loss = 0.21109770\n",
      "Iteration 25, loss = 0.20751126\n",
      "Iteration 26, loss = 0.20412608\n",
      "Iteration 27, loss = 0.20109653\n",
      "Iteration 28, loss = 0.19831026\n",
      "Iteration 29, loss = 0.19563082\n",
      "Iteration 30, loss = 0.19325828\n",
      "Iteration 31, loss = 0.19103147\n",
      "Iteration 32, loss = 0.18885367\n",
      "Iteration 33, loss = 0.18687606\n",
      "Iteration 34, loss = 0.18494008\n",
      "Iteration 35, loss = 0.18332720\n",
      "Iteration 36, loss = 0.18164170\n",
      "Iteration 37, loss = 0.18003066\n",
      "Iteration 38, loss = 0.17852917\n",
      "Iteration 39, loss = 0.17721169\n",
      "Iteration 40, loss = 0.17587557\n",
      "Iteration 41, loss = 0.17464463\n",
      "Iteration 42, loss = 0.17343682\n",
      "Iteration 43, loss = 0.17233873\n",
      "Iteration 44, loss = 0.17123886\n",
      "Iteration 45, loss = 0.17017546\n",
      "Iteration 46, loss = 0.16921187\n",
      "Iteration 47, loss = 0.16827575\n",
      "Iteration 48, loss = 0.16733185\n",
      "Iteration 49, loss = 0.16651350\n",
      "Iteration 50, loss = 0.16568284\n",
      "Iteration 51, loss = 0.16486036\n",
      "Iteration 52, loss = 0.16407718\n",
      "Iteration 53, loss = 0.16331649\n",
      "Iteration 54, loss = 0.16260772\n",
      "Iteration 55, loss = 0.16191182\n",
      "Iteration 56, loss = 0.16126817\n",
      "Iteration 57, loss = 0.16053451\n",
      "Iteration 58, loss = 0.15992050\n",
      "Iteration 59, loss = 0.15924508\n",
      "Iteration 60, loss = 0.15861532\n",
      "Iteration 61, loss = 0.15802558\n",
      "Iteration 62, loss = 0.15738179\n",
      "Iteration 63, loss = 0.15684264\n",
      "Iteration 64, loss = 0.15623813\n",
      "Iteration 65, loss = 0.15570586\n",
      "Iteration 66, loss = 0.15519425\n",
      "Iteration 67, loss = 0.15468683\n",
      "Iteration 68, loss = 0.15409573\n",
      "Iteration 69, loss = 0.15361301\n",
      "Iteration 70, loss = 0.15313195\n",
      "Iteration 71, loss = 0.15265779\n",
      "Iteration 72, loss = 0.15221122\n",
      "Iteration 73, loss = 0.15176506\n",
      "Iteration 74, loss = 0.15131013\n",
      "Iteration 75, loss = 0.15091875\n",
      "Iteration 76, loss = 0.15050257\n",
      "Iteration 77, loss = 0.15006635\n",
      "Iteration 78, loss = 0.14964508\n",
      "Iteration 79, loss = 0.14927925\n",
      "Iteration 80, loss = 0.14892346\n",
      "Iteration 81, loss = 0.14854178\n",
      "Iteration 82, loss = 0.14821772\n",
      "Iteration 83, loss = 0.14782431\n",
      "Iteration 84, loss = 0.14749890\n",
      "Iteration 85, loss = 0.14716533\n",
      "Iteration 86, loss = 0.14686116\n",
      "Iteration 87, loss = 0.14655064\n",
      "Iteration 88, loss = 0.14623965\n",
      "Iteration 89, loss = 0.14592765\n",
      "Iteration 90, loss = 0.14568438\n",
      "Iteration 91, loss = 0.14533995\n",
      "Iteration 92, loss = 0.14504946\n",
      "Iteration 93, loss = 0.14480492\n",
      "Iteration 94, loss = 0.14448145\n",
      "Iteration 95, loss = 0.14419961\n",
      "Iteration 96, loss = 0.14402879\n",
      "Iteration 97, loss = 0.14371006\n",
      "Iteration 98, loss = 0.14343296\n",
      "Iteration 99, loss = 0.14312949\n",
      "Iteration 100, loss = 0.14290559\n",
      "Iteration 101, loss = 0.14266144\n",
      "Iteration 102, loss = 0.14243273\n",
      "Iteration 103, loss = 0.14220364\n",
      "Iteration 104, loss = 0.14194473\n",
      "Iteration 105, loss = 0.14170704\n",
      "Iteration 106, loss = 0.14142946\n",
      "Iteration 107, loss = 0.14120245\n",
      "Iteration 108, loss = 0.14101720\n",
      "Iteration 109, loss = 0.14074242\n",
      "Iteration 110, loss = 0.14057850\n",
      "Iteration 111, loss = 0.14032168\n",
      "Iteration 112, loss = 0.14012302\n",
      "Iteration 113, loss = 0.13989137\n",
      "Iteration 114, loss = 0.13967264\n",
      "Iteration 115, loss = 0.13946248\n",
      "Iteration 116, loss = 0.13927404\n",
      "Iteration 117, loss = 0.13908311\n",
      "Iteration 118, loss = 0.13890999\n",
      "Iteration 119, loss = 0.13867803\n",
      "Iteration 120, loss = 0.13850210\n",
      "Iteration 121, loss = 0.13829568\n",
      "Iteration 122, loss = 0.13810677\n",
      "Iteration 123, loss = 0.13793688\n",
      "Iteration 124, loss = 0.13775988\n",
      "Iteration 125, loss = 0.13755098\n",
      "Iteration 126, loss = 0.13738440\n",
      "Iteration 127, loss = 0.13716467\n",
      "Iteration 128, loss = 0.13712319\n",
      "Iteration 129, loss = 0.13685704\n",
      "Iteration 130, loss = 0.13672902\n",
      "Iteration 131, loss = 0.13652057\n",
      "Iteration 132, loss = 0.13637674\n",
      "Iteration 133, loss = 0.13621469\n",
      "Iteration 134, loss = 0.13604137\n",
      "Iteration 135, loss = 0.13589472\n",
      "Iteration 136, loss = 0.13577854\n",
      "Iteration 137, loss = 0.13562425\n",
      "Iteration 138, loss = 0.13548069\n",
      "Iteration 139, loss = 0.13533368\n",
      "Iteration 140, loss = 0.13520874\n",
      "Iteration 141, loss = 0.13505205\n",
      "Iteration 142, loss = 0.13487972\n",
      "Iteration 143, loss = 0.13476681\n",
      "Iteration 144, loss = 0.13460741\n",
      "Iteration 145, loss = 0.13446812\n",
      "Iteration 146, loss = 0.13440690\n",
      "Iteration 147, loss = 0.13419587\n",
      "Iteration 148, loss = 0.13411011\n",
      "Iteration 149, loss = 0.13400730\n",
      "Iteration 150, loss = 0.13382057\n",
      "Iteration 151, loss = 0.13368299\n",
      "Iteration 152, loss = 0.13354787\n",
      "Iteration 153, loss = 0.13347889\n",
      "Iteration 154, loss = 0.13334341\n",
      "Iteration 155, loss = 0.13319611\n",
      "Iteration 156, loss = 0.13306004\n",
      "Iteration 157, loss = 0.13292113\n",
      "Iteration 158, loss = 0.13279309\n",
      "Iteration 159, loss = 0.13266320\n",
      "Iteration 160, loss = 0.13257052\n",
      "Iteration 161, loss = 0.13244205\n",
      "Iteration 162, loss = 0.13230906\n",
      "Iteration 163, loss = 0.13219457\n",
      "Iteration 164, loss = 0.13205640\n",
      "Iteration 165, loss = 0.13198571\n",
      "Iteration 166, loss = 0.13183227\n",
      "Iteration 167, loss = 0.13171408\n",
      "Iteration 168, loss = 0.13158747\n",
      "Iteration 169, loss = 0.13150115\n",
      "Iteration 170, loss = 0.13137501\n",
      "Iteration 171, loss = 0.13132190\n",
      "Iteration 172, loss = 0.13110177\n",
      "Iteration 173, loss = 0.13103446\n",
      "Iteration 174, loss = 0.13089365\n",
      "Iteration 175, loss = 0.13082962\n",
      "Iteration 176, loss = 0.13068536\n",
      "Iteration 177, loss = 0.13060144\n",
      "Iteration 178, loss = 0.13054881\n",
      "Iteration 179, loss = 0.13038361\n",
      "Iteration 180, loss = 0.13025876\n",
      "Iteration 181, loss = 0.13015914\n",
      "Iteration 182, loss = 0.13004613\n",
      "Iteration 183, loss = 0.12995403\n",
      "Iteration 184, loss = 0.12982533\n",
      "Iteration 185, loss = 0.12977429\n",
      "Iteration 186, loss = 0.12968758\n",
      "Iteration 187, loss = 0.12953140\n",
      "Iteration 188, loss = 0.12942299\n",
      "Iteration 189, loss = 0.12936471\n",
      "Iteration 190, loss = 0.12920678\n",
      "Iteration 191, loss = 0.12910774\n",
      "Iteration 192, loss = 0.12906282\n",
      "Iteration 193, loss = 0.12892054\n",
      "Iteration 194, loss = 0.12884515\n",
      "Iteration 195, loss = 0.12873084\n",
      "Iteration 196, loss = 0.12861769\n",
      "Iteration 197, loss = 0.12854664\n",
      "Iteration 198, loss = 0.12845938\n",
      "Iteration 199, loss = 0.12835843\n",
      "Iteration 200, loss = 0.12825765\n",
      "Iteration 201, loss = 0.12816052\n",
      "Iteration 202, loss = 0.12804996\n",
      "Iteration 203, loss = 0.12795361\n",
      "Iteration 204, loss = 0.12785383\n",
      "Iteration 205, loss = 0.12775183\n",
      "Iteration 206, loss = 0.12765946\n",
      "Iteration 207, loss = 0.12758552\n",
      "Iteration 208, loss = 0.12751943\n",
      "Iteration 209, loss = 0.12742716\n",
      "Iteration 210, loss = 0.12728807\n",
      "Iteration 211, loss = 0.12722640\n",
      "Iteration 212, loss = 0.12718650\n",
      "Iteration 213, loss = 0.12703078\n",
      "Iteration 214, loss = 0.12696459\n",
      "Iteration 215, loss = 0.12686084\n",
      "Iteration 216, loss = 0.12676011\n",
      "Iteration 217, loss = 0.12666698\n",
      "Iteration 218, loss = 0.12663079\n",
      "Iteration 219, loss = 0.12658113\n",
      "Iteration 220, loss = 0.12641609\n",
      "Iteration 221, loss = 0.12636281\n",
      "Iteration 222, loss = 0.12627503\n",
      "Iteration 223, loss = 0.12617700\n",
      "Iteration 224, loss = 0.12606723\n",
      "Iteration 225, loss = 0.12600228\n",
      "Iteration 226, loss = 0.12588556\n",
      "Iteration 227, loss = 0.12582067\n",
      "Iteration 228, loss = 0.12575683\n",
      "Iteration 229, loss = 0.12567379\n",
      "Iteration 230, loss = 0.12559997\n",
      "Iteration 231, loss = 0.12548585\n",
      "Iteration 232, loss = 0.12543321\n",
      "Iteration 233, loss = 0.12532728\n",
      "Iteration 234, loss = 0.12528663\n",
      "Iteration 235, loss = 0.12513623\n",
      "Iteration 236, loss = 0.12511839\n",
      "Iteration 237, loss = 0.12499436\n",
      "Iteration 238, loss = 0.12491047\n",
      "Iteration 239, loss = 0.12482051\n",
      "Iteration 240, loss = 0.12472503\n",
      "Iteration 241, loss = 0.12467156\n",
      "Iteration 242, loss = 0.12457143\n",
      "Iteration 243, loss = 0.12452184\n",
      "Iteration 244, loss = 0.12445522\n",
      "Iteration 245, loss = 0.12432621\n",
      "Iteration 246, loss = 0.12426270\n",
      "Iteration 247, loss = 0.12418693\n",
      "Iteration 248, loss = 0.12411580\n",
      "Iteration 249, loss = 0.12409624\n",
      "Iteration 250, loss = 0.12391762\n",
      "Iteration 251, loss = 0.12386115\n",
      "Iteration 252, loss = 0.12380779\n",
      "Iteration 253, loss = 0.12372525\n",
      "Iteration 254, loss = 0.12362169\n",
      "Iteration 255, loss = 0.12354798\n",
      "Iteration 256, loss = 0.12349355\n",
      "Iteration 257, loss = 0.12338142\n",
      "Iteration 258, loss = 0.12333487\n",
      "Iteration 259, loss = 0.12322668\n",
      "Iteration 260, loss = 0.12316089\n",
      "Iteration 261, loss = 0.12305288\n",
      "Iteration 262, loss = 0.12298081\n",
      "Iteration 263, loss = 0.12291259\n",
      "Iteration 264, loss = 0.12286884\n",
      "Iteration 265, loss = 0.12275603\n",
      "Iteration 266, loss = 0.12270067\n",
      "Iteration 267, loss = 0.12268351\n",
      "Iteration 268, loss = 0.12255234\n",
      "Iteration 269, loss = 0.12252184\n",
      "Iteration 270, loss = 0.12245986\n",
      "Iteration 271, loss = 0.12232377\n",
      "Iteration 272, loss = 0.12226778\n",
      "Iteration 273, loss = 0.12216244\n",
      "Iteration 274, loss = 0.12208133\n",
      "Iteration 275, loss = 0.12204957\n",
      "Iteration 276, loss = 0.12191859\n",
      "Iteration 277, loss = 0.12184622\n",
      "Iteration 278, loss = 0.12179335\n",
      "Iteration 279, loss = 0.12170026\n",
      "Iteration 280, loss = 0.12162109\n",
      "Iteration 281, loss = 0.12157349\n",
      "Iteration 282, loss = 0.12152428\n",
      "Iteration 283, loss = 0.12142408\n",
      "Iteration 284, loss = 0.12132670\n",
      "Iteration 285, loss = 0.12128578\n",
      "Iteration 286, loss = 0.12123320\n",
      "Iteration 287, loss = 0.12115946\n",
      "Iteration 288, loss = 0.12104728\n",
      "Iteration 289, loss = 0.12099644\n",
      "Iteration 290, loss = 0.12093755\n",
      "Iteration 291, loss = 0.12081343\n",
      "Iteration 292, loss = 0.12078298\n",
      "Iteration 293, loss = 0.12081217\n",
      "Iteration 294, loss = 0.12058863\n",
      "Iteration 295, loss = 0.12058663\n",
      "Iteration 296, loss = 0.12041734\n",
      "Iteration 297, loss = 0.12035452\n",
      "Iteration 298, loss = 0.12033093\n",
      "Iteration 299, loss = 0.12025625\n",
      "Iteration 300, loss = 0.12016590\n",
      "Iteration 301, loss = 0.12006311\n",
      "Iteration 302, loss = 0.12001394\n",
      "Iteration 303, loss = 0.11994866\n",
      "Iteration 304, loss = 0.11988049\n",
      "Iteration 305, loss = 0.11981197\n",
      "Iteration 306, loss = 0.11973014\n",
      "Iteration 307, loss = 0.11967983\n",
      "Iteration 308, loss = 0.11960257\n",
      "Iteration 309, loss = 0.11949420\n",
      "Iteration 310, loss = 0.11945093\n",
      "Iteration 311, loss = 0.11935960\n",
      "Iteration 312, loss = 0.11931475\n",
      "Iteration 313, loss = 0.11927667\n",
      "Iteration 314, loss = 0.11914962\n",
      "Iteration 315, loss = 0.11909711\n",
      "Iteration 316, loss = 0.11901305\n",
      "Iteration 317, loss = 0.11898467\n",
      "Iteration 318, loss = 0.11886713\n",
      "Iteration 319, loss = 0.11879866\n",
      "Iteration 320, loss = 0.11872440\n",
      "Iteration 321, loss = 0.11868291\n",
      "Iteration 322, loss = 0.11863697\n",
      "Iteration 323, loss = 0.11858729\n",
      "Iteration 324, loss = 0.11852482\n",
      "Iteration 325, loss = 0.11841408\n",
      "Iteration 326, loss = 0.11831433\n",
      "Iteration 327, loss = 0.11824576\n",
      "Iteration 328, loss = 0.11819806\n",
      "Iteration 329, loss = 0.11811514\n",
      "Iteration 330, loss = 0.11809736\n",
      "Iteration 331, loss = 0.11802274\n",
      "Iteration 332, loss = 0.11791476\n",
      "Iteration 333, loss = 0.11789263\n",
      "Iteration 334, loss = 0.11780956\n",
      "Iteration 335, loss = 0.11773717\n",
      "Iteration 336, loss = 0.11765223\n",
      "Iteration 337, loss = 0.11758565\n",
      "Iteration 338, loss = 0.11754961\n",
      "Iteration 339, loss = 0.11752667\n",
      "Iteration 340, loss = 0.11740832\n",
      "Iteration 341, loss = 0.11734769\n",
      "Iteration 342, loss = 0.11731067\n",
      "Iteration 343, loss = 0.11722418\n",
      "Iteration 344, loss = 0.11717574\n",
      "Iteration 345, loss = 0.11710486\n",
      "Iteration 346, loss = 0.11705648\n",
      "Iteration 347, loss = 0.11702338\n",
      "Iteration 348, loss = 0.11695264\n",
      "Iteration 349, loss = 0.11689100\n",
      "Iteration 350, loss = 0.11681115\n",
      "Iteration 351, loss = 0.11680677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 352, loss = 0.11668084\n",
      "Iteration 353, loss = 0.11665371\n",
      "Iteration 354, loss = 0.11663991\n",
      "Iteration 355, loss = 0.11662157\n",
      "Iteration 356, loss = 0.11661520\n",
      "Iteration 357, loss = 0.11660118\n",
      "Iteration 358, loss = 0.11658705\n",
      "Iteration 359, loss = 0.11658712\n",
      "Iteration 360, loss = 0.11656684\n",
      "Iteration 361, loss = 0.11655168\n",
      "Iteration 362, loss = 0.11655273\n",
      "Iteration 363, loss = 0.11653012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 364, loss = 0.11651877\n",
      "Iteration 365, loss = 0.11650973\n",
      "Iteration 366, loss = 0.11650525\n",
      "Iteration 367, loss = 0.11650351\n",
      "Iteration 368, loss = 0.11650020\n",
      "Iteration 369, loss = 0.11649835\n",
      "Iteration 370, loss = 0.11649834\n",
      "Iteration 371, loss = 0.11649607\n",
      "Iteration 372, loss = 0.11649142\n",
      "Iteration 373, loss = 0.11649113\n",
      "Iteration 374, loss = 0.11648587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 375, loss = 0.11648308\n",
      "Iteration 376, loss = 0.11648189\n",
      "Iteration 377, loss = 0.11648113\n",
      "Iteration 378, loss = 0.11648100\n",
      "Iteration 379, loss = 0.11648015\n",
      "Iteration 380, loss = 0.11647955\n",
      "Iteration 381, loss = 0.11647945\n",
      "Iteration 382, loss = 0.11647873\n",
      "Iteration 383, loss = 0.11647841\n",
      "Iteration 384, loss = 0.11647819\n",
      "Iteration 385, loss = 0.11647740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 386, loss = 0.11647674\n",
      "Iteration 387, loss = 0.11647647\n",
      "Iteration 388, loss = 0.11647640\n",
      "Iteration 389, loss = 0.11647628\n",
      "Iteration 390, loss = 0.11647614\n",
      "Iteration 391, loss = 0.11647610\n",
      "Iteration 392, loss = 0.11647594\n",
      "Iteration 393, loss = 0.11647589\n",
      "Iteration 394, loss = 0.11647575\n",
      "Iteration 395, loss = 0.11647566\n",
      "Iteration 396, loss = 0.11647557\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 397, loss = 0.11647544\n",
      "Iteration 398, loss = 0.11647539\n",
      "Iteration 399, loss = 0.11647536\n",
      "Iteration 400, loss = 0.11647534\n",
      "Iteration 401, loss = 0.11647533\n",
      "Iteration 402, loss = 0.11647531\n",
      "Iteration 403, loss = 0.11647530\n",
      "Iteration 404, loss = 0.11647527\n",
      "Iteration 405, loss = 0.11647528\n",
      "Iteration 406, loss = 0.11647523\n",
      "Iteration 407, loss = 0.11647521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 408, loss = 0.11647518\n",
      "Iteration 409, loss = 0.11647518\n",
      "Iteration 410, loss = 0.11647517\n",
      "Iteration 411, loss = 0.11647517\n",
      "Iteration 412, loss = 0.11647516\n",
      "Iteration 413, loss = 0.11647516\n",
      "Iteration 414, loss = 0.11647516\n",
      "Iteration 415, loss = 0.11647515\n",
      "Iteration 416, loss = 0.11647515\n",
      "Iteration 417, loss = 0.11647514\n",
      "Iteration 418, loss = 0.11647514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96239663\n",
      "Iteration 2, loss = 0.83071627\n",
      "Iteration 3, loss = 0.69512938\n",
      "Iteration 4, loss = 0.59071980\n",
      "Iteration 5, loss = 0.51468360\n",
      "Iteration 6, loss = 0.46042040\n",
      "Iteration 7, loss = 0.41832437\n",
      "Iteration 8, loss = 0.38560443\n",
      "Iteration 9, loss = 0.35932508\n",
      "Iteration 10, loss = 0.33792827\n",
      "Iteration 11, loss = 0.31987851\n",
      "Iteration 12, loss = 0.30423694\n",
      "Iteration 13, loss = 0.29107313\n",
      "Iteration 14, loss = 0.27940349\n",
      "Iteration 15, loss = 0.26928887\n",
      "Iteration 16, loss = 0.26028657\n",
      "Iteration 17, loss = 0.25228612\n",
      "Iteration 18, loss = 0.24537970\n",
      "Iteration 19, loss = 0.23919871\n",
      "Iteration 20, loss = 0.23367372\n",
      "Iteration 21, loss = 0.22854146\n",
      "Iteration 22, loss = 0.22410223\n",
      "Iteration 23, loss = 0.21986235\n",
      "Iteration 24, loss = 0.21608753\n",
      "Iteration 25, loss = 0.21250713\n",
      "Iteration 26, loss = 0.20925709\n",
      "Iteration 27, loss = 0.20635107\n",
      "Iteration 28, loss = 0.20356710\n",
      "Iteration 29, loss = 0.20100501\n",
      "Iteration 30, loss = 0.19862453\n",
      "Iteration 31, loss = 0.19647087\n",
      "Iteration 32, loss = 0.19428991\n",
      "Iteration 33, loss = 0.19242116\n",
      "Iteration 34, loss = 0.19057343\n",
      "Iteration 35, loss = 0.18896724\n",
      "Iteration 36, loss = 0.18733495\n",
      "Iteration 37, loss = 0.18572080\n",
      "Iteration 38, loss = 0.18429368\n",
      "Iteration 39, loss = 0.18295292\n",
      "Iteration 40, loss = 0.18167811\n",
      "Iteration 41, loss = 0.18043509\n",
      "Iteration 42, loss = 0.17927442\n",
      "Iteration 43, loss = 0.17811997\n",
      "Iteration 44, loss = 0.17703315\n",
      "Iteration 45, loss = 0.17600184\n",
      "Iteration 46, loss = 0.17504082\n",
      "Iteration 47, loss = 0.17402807\n",
      "Iteration 48, loss = 0.17309611\n",
      "Iteration 49, loss = 0.17221522\n",
      "Iteration 50, loss = 0.17131827\n",
      "Iteration 51, loss = 0.17044483\n",
      "Iteration 52, loss = 0.16958038\n",
      "Iteration 53, loss = 0.16874530\n",
      "Iteration 54, loss = 0.16799446\n",
      "Iteration 55, loss = 0.16728482\n",
      "Iteration 56, loss = 0.16660657\n",
      "Iteration 57, loss = 0.16581018\n",
      "Iteration 58, loss = 0.16519163\n",
      "Iteration 59, loss = 0.16454846\n",
      "Iteration 60, loss = 0.16391212\n",
      "Iteration 61, loss = 0.16330233\n",
      "Iteration 62, loss = 0.16265258\n",
      "Iteration 63, loss = 0.16208746\n",
      "Iteration 64, loss = 0.16151106\n",
      "Iteration 65, loss = 0.16095950\n",
      "Iteration 66, loss = 0.16040154\n",
      "Iteration 67, loss = 0.15985870\n",
      "Iteration 68, loss = 0.15929120\n",
      "Iteration 69, loss = 0.15883401\n",
      "Iteration 70, loss = 0.15832588\n",
      "Iteration 71, loss = 0.15782398\n",
      "Iteration 72, loss = 0.15733513\n",
      "Iteration 73, loss = 0.15689202\n",
      "Iteration 74, loss = 0.15636194\n",
      "Iteration 75, loss = 0.15589208\n",
      "Iteration 76, loss = 0.15543658\n",
      "Iteration 77, loss = 0.15499136\n",
      "Iteration 78, loss = 0.15456211\n",
      "Iteration 79, loss = 0.15410509\n",
      "Iteration 80, loss = 0.15374352\n",
      "Iteration 81, loss = 0.15329447\n",
      "Iteration 82, loss = 0.15293897\n",
      "Iteration 83, loss = 0.15252348\n",
      "Iteration 84, loss = 0.15214658\n",
      "Iteration 85, loss = 0.15178497\n",
      "Iteration 86, loss = 0.15141683\n",
      "Iteration 87, loss = 0.15109916\n",
      "Iteration 88, loss = 0.15076505\n",
      "Iteration 89, loss = 0.15039434\n",
      "Iteration 90, loss = 0.15012476\n",
      "Iteration 91, loss = 0.14976238\n",
      "Iteration 92, loss = 0.14944090\n",
      "Iteration 93, loss = 0.14912138\n",
      "Iteration 94, loss = 0.14880720\n",
      "Iteration 95, loss = 0.14850916\n",
      "Iteration 96, loss = 0.14834417\n",
      "Iteration 97, loss = 0.14795447\n",
      "Iteration 98, loss = 0.14766979\n",
      "Iteration 99, loss = 0.14736809\n",
      "Iteration 100, loss = 0.14708624\n",
      "Iteration 101, loss = 0.14683870\n",
      "Iteration 102, loss = 0.14659059\n",
      "Iteration 103, loss = 0.14633001\n",
      "Iteration 104, loss = 0.14604888\n",
      "Iteration 105, loss = 0.14579354\n",
      "Iteration 106, loss = 0.14550911\n",
      "Iteration 107, loss = 0.14526509\n",
      "Iteration 108, loss = 0.14506133\n",
      "Iteration 109, loss = 0.14474358\n",
      "Iteration 110, loss = 0.14457697\n",
      "Iteration 111, loss = 0.14428651\n",
      "Iteration 112, loss = 0.14409651\n",
      "Iteration 113, loss = 0.14383541\n",
      "Iteration 114, loss = 0.14363147\n",
      "Iteration 115, loss = 0.14341847\n",
      "Iteration 116, loss = 0.14319483\n",
      "Iteration 117, loss = 0.14297204\n",
      "Iteration 118, loss = 0.14275346\n",
      "Iteration 119, loss = 0.14254524\n",
      "Iteration 120, loss = 0.14238944\n",
      "Iteration 121, loss = 0.14216842\n",
      "Iteration 122, loss = 0.14193996\n",
      "Iteration 123, loss = 0.14173476\n",
      "Iteration 124, loss = 0.14157162\n",
      "Iteration 125, loss = 0.14135303\n",
      "Iteration 126, loss = 0.14121962\n",
      "Iteration 127, loss = 0.14095186\n",
      "Iteration 128, loss = 0.14086024\n",
      "Iteration 129, loss = 0.14059950\n",
      "Iteration 130, loss = 0.14046829\n",
      "Iteration 131, loss = 0.14022909\n",
      "Iteration 132, loss = 0.14002516\n",
      "Iteration 133, loss = 0.13987104\n",
      "Iteration 134, loss = 0.13966807\n",
      "Iteration 135, loss = 0.13949128\n",
      "Iteration 136, loss = 0.13934795\n",
      "Iteration 137, loss = 0.13918770\n",
      "Iteration 138, loss = 0.13898952\n",
      "Iteration 139, loss = 0.13885281\n",
      "Iteration 140, loss = 0.13871570\n",
      "Iteration 141, loss = 0.13850396\n",
      "Iteration 142, loss = 0.13833741\n",
      "Iteration 143, loss = 0.13818414\n",
      "Iteration 144, loss = 0.13802534\n",
      "Iteration 145, loss = 0.13787046\n",
      "Iteration 146, loss = 0.13777593\n",
      "Iteration 147, loss = 0.13757079\n",
      "Iteration 148, loss = 0.13750278\n",
      "Iteration 149, loss = 0.13730403\n",
      "Iteration 150, loss = 0.13715451\n",
      "Iteration 151, loss = 0.13704250\n",
      "Iteration 152, loss = 0.13691495\n",
      "Iteration 153, loss = 0.13676220\n",
      "Iteration 154, loss = 0.13663847\n",
      "Iteration 155, loss = 0.13648943\n",
      "Iteration 156, loss = 0.13634318\n",
      "Iteration 157, loss = 0.13611585\n",
      "Iteration 158, loss = 0.13599795\n",
      "Iteration 159, loss = 0.13577214\n",
      "Iteration 160, loss = 0.13564815\n",
      "Iteration 161, loss = 0.13546863\n",
      "Iteration 162, loss = 0.13530384\n",
      "Iteration 163, loss = 0.13514032\n",
      "Iteration 164, loss = 0.13495566\n",
      "Iteration 165, loss = 0.13483694\n",
      "Iteration 166, loss = 0.13468201\n",
      "Iteration 167, loss = 0.13447999\n",
      "Iteration 168, loss = 0.13430974\n",
      "Iteration 169, loss = 0.13419607\n",
      "Iteration 170, loss = 0.13403035\n",
      "Iteration 171, loss = 0.13388881\n",
      "Iteration 172, loss = 0.13369933\n",
      "Iteration 173, loss = 0.13358205\n",
      "Iteration 174, loss = 0.13338942\n",
      "Iteration 175, loss = 0.13326670\n",
      "Iteration 176, loss = 0.13309252\n",
      "Iteration 177, loss = 0.13294280\n",
      "Iteration 178, loss = 0.13288177\n",
      "Iteration 179, loss = 0.13268357\n",
      "Iteration 180, loss = 0.13249096\n",
      "Iteration 181, loss = 0.13239299\n",
      "Iteration 182, loss = 0.13224418\n",
      "Iteration 183, loss = 0.13206325\n",
      "Iteration 184, loss = 0.13191536\n",
      "Iteration 185, loss = 0.13178915\n",
      "Iteration 186, loss = 0.13171957\n",
      "Iteration 187, loss = 0.13148253\n",
      "Iteration 188, loss = 0.13136966\n",
      "Iteration 189, loss = 0.13128408\n",
      "Iteration 190, loss = 0.13103831\n",
      "Iteration 191, loss = 0.13089912\n",
      "Iteration 192, loss = 0.13082378\n",
      "Iteration 193, loss = 0.13061997\n",
      "Iteration 194, loss = 0.13050937\n",
      "Iteration 195, loss = 0.13040847\n",
      "Iteration 196, loss = 0.13022646\n",
      "Iteration 197, loss = 0.13010915\n",
      "Iteration 198, loss = 0.12997574\n",
      "Iteration 199, loss = 0.12982401\n",
      "Iteration 200, loss = 0.12969258\n",
      "Iteration 201, loss = 0.12957216\n",
      "Iteration 202, loss = 0.12937965\n",
      "Iteration 203, loss = 0.12926904\n",
      "Iteration 204, loss = 0.12915355\n",
      "Iteration 205, loss = 0.12900233\n",
      "Iteration 206, loss = 0.12886172\n",
      "Iteration 207, loss = 0.12875894\n",
      "Iteration 208, loss = 0.12864545\n",
      "Iteration 209, loss = 0.12851059\n",
      "Iteration 210, loss = 0.12836103\n",
      "Iteration 211, loss = 0.12824909\n",
      "Iteration 212, loss = 0.12816060\n",
      "Iteration 213, loss = 0.12796959\n",
      "Iteration 214, loss = 0.12785671\n",
      "Iteration 215, loss = 0.12771811\n",
      "Iteration 216, loss = 0.12757592\n",
      "Iteration 217, loss = 0.12745107\n",
      "Iteration 218, loss = 0.12735766\n",
      "Iteration 219, loss = 0.12727025\n",
      "Iteration 220, loss = 0.12708453\n",
      "Iteration 221, loss = 0.12695687\n",
      "Iteration 222, loss = 0.12687076\n",
      "Iteration 223, loss = 0.12672960\n",
      "Iteration 224, loss = 0.12657838\n",
      "Iteration 225, loss = 0.12649171\n",
      "Iteration 226, loss = 0.12634373\n",
      "Iteration 227, loss = 0.12623508\n",
      "Iteration 228, loss = 0.12611235\n",
      "Iteration 229, loss = 0.12602487\n",
      "Iteration 230, loss = 0.12590185\n",
      "Iteration 231, loss = 0.12575072\n",
      "Iteration 232, loss = 0.12569160\n",
      "Iteration 233, loss = 0.12557661\n",
      "Iteration 234, loss = 0.12552332\n",
      "Iteration 235, loss = 0.12531173\n",
      "Iteration 236, loss = 0.12529513\n",
      "Iteration 237, loss = 0.12509809\n",
      "Iteration 238, loss = 0.12499016\n",
      "Iteration 239, loss = 0.12487077\n",
      "Iteration 240, loss = 0.12476909\n",
      "Iteration 241, loss = 0.12467440\n",
      "Iteration 242, loss = 0.12455858\n",
      "Iteration 243, loss = 0.12447813\n",
      "Iteration 244, loss = 0.12436767\n",
      "Iteration 245, loss = 0.12423613\n",
      "Iteration 246, loss = 0.12411517\n",
      "Iteration 247, loss = 0.12401859\n",
      "Iteration 248, loss = 0.12397583\n",
      "Iteration 249, loss = 0.12392170\n",
      "Iteration 250, loss = 0.12373351\n",
      "Iteration 251, loss = 0.12365694\n",
      "Iteration 252, loss = 0.12355055\n",
      "Iteration 253, loss = 0.12345016\n",
      "Iteration 254, loss = 0.12334924\n",
      "Iteration 255, loss = 0.12323499\n",
      "Iteration 256, loss = 0.12314961\n",
      "Iteration 257, loss = 0.12301856\n",
      "Iteration 258, loss = 0.12295104\n",
      "Iteration 259, loss = 0.12284106\n",
      "Iteration 260, loss = 0.12277544\n",
      "Iteration 261, loss = 0.12265336\n",
      "Iteration 262, loss = 0.12255373\n",
      "Iteration 263, loss = 0.12244635\n",
      "Iteration 264, loss = 0.12239902\n",
      "Iteration 265, loss = 0.12229468\n",
      "Iteration 266, loss = 0.12219415\n",
      "Iteration 267, loss = 0.12217019\n",
      "Iteration 268, loss = 0.12200664\n",
      "Iteration 269, loss = 0.12191877\n",
      "Iteration 270, loss = 0.12188371\n",
      "Iteration 271, loss = 0.12175723\n",
      "Iteration 272, loss = 0.12168555\n",
      "Iteration 273, loss = 0.12155739\n",
      "Iteration 274, loss = 0.12148754\n",
      "Iteration 275, loss = 0.12136426\n",
      "Iteration 276, loss = 0.12126195\n",
      "Iteration 277, loss = 0.12120444\n",
      "Iteration 278, loss = 0.12114079\n",
      "Iteration 279, loss = 0.12100899\n",
      "Iteration 280, loss = 0.12092821\n",
      "Iteration 281, loss = 0.12086902\n",
      "Iteration 282, loss = 0.12080621\n",
      "Iteration 283, loss = 0.12068295\n",
      "Iteration 284, loss = 0.12058655\n",
      "Iteration 285, loss = 0.12055100\n",
      "Iteration 286, loss = 0.12048417\n",
      "Iteration 287, loss = 0.12036238\n",
      "Iteration 288, loss = 0.12027368\n",
      "Iteration 289, loss = 0.12021402\n",
      "Iteration 290, loss = 0.12009646\n",
      "Iteration 291, loss = 0.12001152\n",
      "Iteration 292, loss = 0.11997195\n",
      "Iteration 293, loss = 0.11998252\n",
      "Iteration 294, loss = 0.11978135\n",
      "Iteration 295, loss = 0.11981953\n",
      "Iteration 296, loss = 0.11959154\n",
      "Iteration 297, loss = 0.11951866\n",
      "Iteration 298, loss = 0.11950789\n",
      "Iteration 299, loss = 0.11944595\n",
      "Iteration 300, loss = 0.11932303\n",
      "Iteration 301, loss = 0.11923440\n",
      "Iteration 302, loss = 0.11916302\n",
      "Iteration 303, loss = 0.11909632\n",
      "Iteration 304, loss = 0.11904572\n",
      "Iteration 305, loss = 0.11896298\n",
      "Iteration 306, loss = 0.11889588\n",
      "Iteration 307, loss = 0.11881704\n",
      "Iteration 308, loss = 0.11873512\n",
      "Iteration 309, loss = 0.11861929\n",
      "Iteration 310, loss = 0.11859825\n",
      "Iteration 311, loss = 0.11848702\n",
      "Iteration 312, loss = 0.11841553\n",
      "Iteration 313, loss = 0.11848372\n",
      "Iteration 314, loss = 0.11827415\n",
      "Iteration 315, loss = 0.11823461\n",
      "Iteration 316, loss = 0.11811987\n",
      "Iteration 317, loss = 0.11811427\n",
      "Iteration 318, loss = 0.11798080\n",
      "Iteration 319, loss = 0.11792634\n",
      "Iteration 320, loss = 0.11785326\n",
      "Iteration 321, loss = 0.11781198\n",
      "Iteration 322, loss = 0.11773838\n",
      "Iteration 323, loss = 0.11765667\n",
      "Iteration 324, loss = 0.11768319\n",
      "Iteration 325, loss = 0.11752830\n",
      "Iteration 326, loss = 0.11743539\n",
      "Iteration 327, loss = 0.11734891\n",
      "Iteration 328, loss = 0.11731156\n",
      "Iteration 329, loss = 0.11724231\n",
      "Iteration 330, loss = 0.11718089\n",
      "Iteration 331, loss = 0.11712697\n",
      "Iteration 332, loss = 0.11701063\n",
      "Iteration 333, loss = 0.11702424\n",
      "Iteration 334, loss = 0.11694054\n",
      "Iteration 335, loss = 0.11681612\n",
      "Iteration 336, loss = 0.11675830\n",
      "Iteration 337, loss = 0.11668409\n",
      "Iteration 338, loss = 0.11664954\n",
      "Iteration 339, loss = 0.11658301\n",
      "Iteration 340, loss = 0.11647528\n",
      "Iteration 341, loss = 0.11643639\n",
      "Iteration 342, loss = 0.11638253\n",
      "Iteration 343, loss = 0.11629753\n",
      "Iteration 344, loss = 0.11624182\n",
      "Iteration 345, loss = 0.11616775\n",
      "Iteration 346, loss = 0.11612079\n",
      "Iteration 347, loss = 0.11606288\n",
      "Iteration 348, loss = 0.11601114\n",
      "Iteration 349, loss = 0.11591278\n",
      "Iteration 350, loss = 0.11584598\n",
      "Iteration 351, loss = 0.11582171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Iteration 1, loss = 0.96911416\n",
      "Iteration 2, loss = 0.83947619\n",
      "Iteration 3, loss = 0.70249083\n",
      "Iteration 4, loss = 0.59873642\n",
      "Iteration 5, loss = 0.52088336\n",
      "Iteration 6, loss = 0.46542908\n",
      "Iteration 7, loss = 0.42277253\n",
      "Iteration 8, loss = 0.38906337\n",
      "Iteration 9, loss = 0.36214876\n",
      "Iteration 10, loss = 0.34025422\n",
      "Iteration 11, loss = 0.32193172\n",
      "Iteration 12, loss = 0.30617047\n",
      "Iteration 13, loss = 0.29271930\n",
      "Iteration 14, loss = 0.28103159\n",
      "Iteration 15, loss = 0.27085372\n",
      "Iteration 16, loss = 0.26218261\n",
      "Iteration 17, loss = 0.25441926\n",
      "Iteration 18, loss = 0.24775137\n",
      "Iteration 19, loss = 0.24181595\n",
      "Iteration 20, loss = 0.23646827\n",
      "Iteration 21, loss = 0.23154165\n",
      "Iteration 22, loss = 0.22728008\n",
      "Iteration 23, loss = 0.22324167\n",
      "Iteration 24, loss = 0.21957460\n",
      "Iteration 25, loss = 0.21621940\n",
      "Iteration 26, loss = 0.21311812\n",
      "Iteration 27, loss = 0.21032905\n",
      "Iteration 28, loss = 0.20768491\n",
      "Iteration 29, loss = 0.20524501\n",
      "Iteration 30, loss = 0.20301286\n",
      "Iteration 31, loss = 0.20096138\n",
      "Iteration 32, loss = 0.19885028\n",
      "Iteration 33, loss = 0.19707579\n",
      "Iteration 34, loss = 0.19526083\n",
      "Iteration 35, loss = 0.19372490\n",
      "Iteration 36, loss = 0.19214246\n",
      "Iteration 37, loss = 0.19059541\n",
      "Iteration 38, loss = 0.18919655\n",
      "Iteration 39, loss = 0.18791969\n",
      "Iteration 40, loss = 0.18668818\n",
      "Iteration 41, loss = 0.18544025\n",
      "Iteration 42, loss = 0.18429436\n",
      "Iteration 43, loss = 0.18312303\n",
      "Iteration 44, loss = 0.18204985\n",
      "Iteration 45, loss = 0.18101882\n",
      "Iteration 46, loss = 0.17999743\n",
      "Iteration 47, loss = 0.17897581\n",
      "Iteration 48, loss = 0.17804750\n",
      "Iteration 49, loss = 0.17717078\n",
      "Iteration 50, loss = 0.17629654\n",
      "Iteration 51, loss = 0.17537362\n",
      "Iteration 52, loss = 0.17455383\n",
      "Iteration 53, loss = 0.17367135\n",
      "Iteration 54, loss = 0.17289923\n",
      "Iteration 55, loss = 0.17216437\n",
      "Iteration 56, loss = 0.17137786\n",
      "Iteration 57, loss = 0.17052575\n",
      "Iteration 58, loss = 0.16978899\n",
      "Iteration 59, loss = 0.16904975\n",
      "Iteration 60, loss = 0.16829467\n",
      "Iteration 61, loss = 0.16758694\n",
      "Iteration 62, loss = 0.16691498\n",
      "Iteration 63, loss = 0.16625330\n",
      "Iteration 64, loss = 0.16555791\n",
      "Iteration 65, loss = 0.16490637\n",
      "Iteration 66, loss = 0.16430899\n",
      "Iteration 67, loss = 0.16366520\n",
      "Iteration 68, loss = 0.16295762\n",
      "Iteration 69, loss = 0.16239283\n",
      "Iteration 70, loss = 0.16182327\n",
      "Iteration 71, loss = 0.16120449\n",
      "Iteration 72, loss = 0.16068982\n",
      "Iteration 73, loss = 0.16012531\n",
      "Iteration 74, loss = 0.15958476\n",
      "Iteration 75, loss = 0.15909037\n",
      "Iteration 76, loss = 0.15858211\n",
      "Iteration 77, loss = 0.15813308\n",
      "Iteration 78, loss = 0.15765018\n",
      "Iteration 79, loss = 0.15718885\n",
      "Iteration 80, loss = 0.15682201\n",
      "Iteration 81, loss = 0.15627862\n",
      "Iteration 82, loss = 0.15596177\n",
      "Iteration 83, loss = 0.15540934\n",
      "Iteration 84, loss = 0.15501792\n",
      "Iteration 85, loss = 0.15457673\n",
      "Iteration 86, loss = 0.15423142\n",
      "Iteration 87, loss = 0.15381849\n",
      "Iteration 88, loss = 0.15338394\n",
      "Iteration 89, loss = 0.15300245\n",
      "Iteration 90, loss = 0.15265222\n",
      "Iteration 91, loss = 0.15228201\n",
      "Iteration 92, loss = 0.15202336\n",
      "Iteration 93, loss = 0.15160243\n",
      "Iteration 94, loss = 0.15129648\n",
      "Iteration 95, loss = 0.15095122\n",
      "Iteration 96, loss = 0.15070265\n",
      "Iteration 97, loss = 0.15034466\n",
      "Iteration 98, loss = 0.15001160\n",
      "Iteration 99, loss = 0.14975159\n",
      "Iteration 100, loss = 0.14940617\n",
      "Iteration 101, loss = 0.14914702\n",
      "Iteration 102, loss = 0.14887352\n",
      "Iteration 103, loss = 0.14856039\n",
      "Iteration 104, loss = 0.14828773\n",
      "Iteration 105, loss = 0.14801700\n",
      "Iteration 106, loss = 0.14770376\n",
      "Iteration 107, loss = 0.14742896\n",
      "Iteration 108, loss = 0.14718922\n",
      "Iteration 109, loss = 0.14688193\n",
      "Iteration 110, loss = 0.14664238\n",
      "Iteration 111, loss = 0.14639086\n",
      "Iteration 112, loss = 0.14616694\n",
      "Iteration 113, loss = 0.14591067\n",
      "Iteration 114, loss = 0.14567607\n",
      "Iteration 115, loss = 0.14547425\n",
      "Iteration 116, loss = 0.14516965\n",
      "Iteration 117, loss = 0.14492701\n",
      "Iteration 118, loss = 0.14467729\n",
      "Iteration 119, loss = 0.14445704\n",
      "Iteration 120, loss = 0.14427934\n",
      "Iteration 121, loss = 0.14397942\n",
      "Iteration 122, loss = 0.14376378\n",
      "Iteration 123, loss = 0.14353501\n",
      "Iteration 124, loss = 0.14335072\n",
      "Iteration 125, loss = 0.14312397\n",
      "Iteration 126, loss = 0.14293650\n",
      "Iteration 127, loss = 0.14268536\n",
      "Iteration 128, loss = 0.14256395\n",
      "Iteration 129, loss = 0.14230690\n",
      "Iteration 130, loss = 0.14215527\n",
      "Iteration 131, loss = 0.14191818\n",
      "Iteration 132, loss = 0.14164995\n",
      "Iteration 133, loss = 0.14151525\n",
      "Iteration 134, loss = 0.14128205\n",
      "Iteration 135, loss = 0.14111060\n",
      "Iteration 136, loss = 0.14094359\n",
      "Iteration 137, loss = 0.14074036\n",
      "Iteration 138, loss = 0.14054524\n",
      "Iteration 139, loss = 0.14038691\n",
      "Iteration 140, loss = 0.14022305\n",
      "Iteration 141, loss = 0.14005239\n",
      "Iteration 142, loss = 0.13983983\n",
      "Iteration 143, loss = 0.13969249\n",
      "Iteration 144, loss = 0.13949290\n",
      "Iteration 145, loss = 0.13931880\n",
      "Iteration 146, loss = 0.13922911\n",
      "Iteration 147, loss = 0.13898837\n",
      "Iteration 148, loss = 0.13888871\n",
      "Iteration 149, loss = 0.13869006\n",
      "Iteration 150, loss = 0.13850095\n",
      "Iteration 151, loss = 0.13838042\n",
      "Iteration 152, loss = 0.13819185\n",
      "Iteration 153, loss = 0.13805580\n",
      "Iteration 154, loss = 0.13791006\n",
      "Iteration 155, loss = 0.13774651\n",
      "Iteration 156, loss = 0.13759357\n",
      "Iteration 157, loss = 0.13741641\n",
      "Iteration 158, loss = 0.13733828\n",
      "Iteration 159, loss = 0.13708077\n",
      "Iteration 160, loss = 0.13698055\n",
      "Iteration 161, loss = 0.13677149\n",
      "Iteration 162, loss = 0.13668363\n",
      "Iteration 163, loss = 0.13648381\n",
      "Iteration 164, loss = 0.13633003\n",
      "Iteration 165, loss = 0.13623723\n",
      "Iteration 166, loss = 0.13606771\n",
      "Iteration 167, loss = 0.13589274\n",
      "Iteration 168, loss = 0.13573064\n",
      "Iteration 169, loss = 0.13565499\n",
      "Iteration 170, loss = 0.13548079\n",
      "Iteration 171, loss = 0.13533312\n",
      "Iteration 172, loss = 0.13515661\n",
      "Iteration 173, loss = 0.13510273\n",
      "Iteration 174, loss = 0.13487669\n",
      "Iteration 175, loss = 0.13474028\n",
      "Iteration 176, loss = 0.13460261\n",
      "Iteration 177, loss = 0.13448227\n",
      "Iteration 178, loss = 0.13437073\n",
      "Iteration 179, loss = 0.13419710\n",
      "Iteration 180, loss = 0.13405593\n",
      "Iteration 181, loss = 0.13397450\n",
      "Iteration 182, loss = 0.13381313\n",
      "Iteration 183, loss = 0.13363908\n",
      "Iteration 184, loss = 0.13351472\n",
      "Iteration 185, loss = 0.13340507\n",
      "Iteration 186, loss = 0.13330712\n",
      "Iteration 187, loss = 0.13311641\n",
      "Iteration 188, loss = 0.13301065\n",
      "Iteration 189, loss = 0.13292147\n",
      "Iteration 190, loss = 0.13271359\n",
      "Iteration 191, loss = 0.13259916\n",
      "Iteration 192, loss = 0.13254108\n",
      "Iteration 193, loss = 0.13234715\n",
      "Iteration 194, loss = 0.13224166\n",
      "Iteration 195, loss = 0.13214308\n",
      "Iteration 196, loss = 0.13197355\n",
      "Iteration 197, loss = 0.13186857\n",
      "Iteration 198, loss = 0.13171973\n",
      "Iteration 199, loss = 0.13162167\n",
      "Iteration 200, loss = 0.13150647\n",
      "Iteration 201, loss = 0.13134697\n",
      "Iteration 202, loss = 0.13119377\n",
      "Iteration 203, loss = 0.13112321\n",
      "Iteration 204, loss = 0.13097067\n",
      "Iteration 205, loss = 0.13081207\n",
      "Iteration 206, loss = 0.13068662\n",
      "Iteration 207, loss = 0.13062734\n",
      "Iteration 208, loss = 0.13048148\n",
      "Iteration 209, loss = 0.13033513\n",
      "Iteration 210, loss = 0.13022320\n",
      "Iteration 211, loss = 0.13010108\n",
      "Iteration 212, loss = 0.13003445\n",
      "Iteration 213, loss = 0.12984408\n",
      "Iteration 214, loss = 0.12975999\n",
      "Iteration 215, loss = 0.12963115\n",
      "Iteration 216, loss = 0.12948839\n",
      "Iteration 217, loss = 0.12935392\n",
      "Iteration 218, loss = 0.12927609\n",
      "Iteration 219, loss = 0.12920155\n",
      "Iteration 220, loss = 0.12902127\n",
      "Iteration 221, loss = 0.12891281\n",
      "Iteration 222, loss = 0.12882434\n",
      "Iteration 223, loss = 0.12866127\n",
      "Iteration 224, loss = 0.12853006\n",
      "Iteration 225, loss = 0.12844033\n",
      "Iteration 226, loss = 0.12829393\n",
      "Iteration 227, loss = 0.12818659\n",
      "Iteration 228, loss = 0.12806843\n",
      "Iteration 229, loss = 0.12798645\n",
      "Iteration 230, loss = 0.12780839\n",
      "Iteration 231, loss = 0.12765180\n",
      "Iteration 232, loss = 0.12754221\n",
      "Iteration 233, loss = 0.12741496\n",
      "Iteration 234, loss = 0.12734706\n",
      "Iteration 235, loss = 0.12714522\n",
      "Iteration 236, loss = 0.12705562\n",
      "Iteration 237, loss = 0.12692731\n",
      "Iteration 238, loss = 0.12677883\n",
      "Iteration 239, loss = 0.12670847\n",
      "Iteration 240, loss = 0.12655403\n",
      "Iteration 241, loss = 0.12643025\n",
      "Iteration 242, loss = 0.12630531\n",
      "Iteration 243, loss = 0.12620705\n",
      "Iteration 244, loss = 0.12608602\n",
      "Iteration 245, loss = 0.12594850\n",
      "Iteration 246, loss = 0.12580800\n",
      "Iteration 247, loss = 0.12572915\n",
      "Iteration 248, loss = 0.12562617\n",
      "Iteration 249, loss = 0.12554372\n",
      "Iteration 250, loss = 0.12539486\n",
      "Iteration 251, loss = 0.12530238\n",
      "Iteration 252, loss = 0.12519322\n",
      "Iteration 253, loss = 0.12507145\n",
      "Iteration 254, loss = 0.12494954\n",
      "Iteration 255, loss = 0.12481859\n",
      "Iteration 256, loss = 0.12473064\n",
      "Iteration 257, loss = 0.12461192\n",
      "Iteration 258, loss = 0.12451541\n",
      "Iteration 259, loss = 0.12441687\n",
      "Iteration 260, loss = 0.12434325\n",
      "Iteration 261, loss = 0.12416861\n",
      "Iteration 262, loss = 0.12407489\n",
      "Iteration 263, loss = 0.12398352\n",
      "Iteration 264, loss = 0.12387903\n",
      "Iteration 265, loss = 0.12382830\n",
      "Iteration 266, loss = 0.12369071\n",
      "Iteration 267, loss = 0.12366556\n",
      "Iteration 268, loss = 0.12347616\n",
      "Iteration 269, loss = 0.12340024\n",
      "Iteration 270, loss = 0.12335679\n",
      "Iteration 271, loss = 0.12320612\n",
      "Iteration 272, loss = 0.12315129\n",
      "Iteration 273, loss = 0.12304806\n",
      "Iteration 274, loss = 0.12289681\n",
      "Iteration 275, loss = 0.12279263\n",
      "Iteration 276, loss = 0.12269477\n",
      "Iteration 277, loss = 0.12262374\n",
      "Iteration 278, loss = 0.12255855\n",
      "Iteration 279, loss = 0.12240970\n",
      "Iteration 280, loss = 0.12234442\n",
      "Iteration 281, loss = 0.12226158\n",
      "Iteration 282, loss = 0.12218527\n",
      "Iteration 283, loss = 0.12208704\n",
      "Iteration 284, loss = 0.12198992\n",
      "Iteration 285, loss = 0.12191302\n",
      "Iteration 286, loss = 0.12186924\n",
      "Iteration 287, loss = 0.12177155\n",
      "Iteration 288, loss = 0.12163365\n",
      "Iteration 289, loss = 0.12155398\n",
      "Iteration 290, loss = 0.12147086\n",
      "Iteration 291, loss = 0.12135306\n",
      "Iteration 292, loss = 0.12131810\n",
      "Iteration 293, loss = 0.12129967\n",
      "Iteration 294, loss = 0.12112576\n",
      "Iteration 295, loss = 0.12109547\n",
      "Iteration 296, loss = 0.12090692\n",
      "Iteration 297, loss = 0.12082146\n",
      "Iteration 298, loss = 0.12079939\n",
      "Iteration 299, loss = 0.12067692\n",
      "Iteration 300, loss = 0.12060358\n",
      "Iteration 301, loss = 0.12050403\n",
      "Iteration 302, loss = 0.12046565\n",
      "Iteration 303, loss = 0.12035216\n",
      "Iteration 304, loss = 0.12030652\n",
      "Iteration 305, loss = 0.12020246\n",
      "Iteration 306, loss = 0.12014008\n",
      "Iteration 307, loss = 0.12007579\n",
      "Iteration 308, loss = 0.11997328\n",
      "Iteration 309, loss = 0.11984853\n",
      "Iteration 310, loss = 0.11980350\n",
      "Iteration 311, loss = 0.11970777\n",
      "Iteration 312, loss = 0.11962693\n",
      "Iteration 313, loss = 0.11962861\n",
      "Iteration 314, loss = 0.11945069\n",
      "Iteration 315, loss = 0.11938805\n",
      "Iteration 316, loss = 0.11931701\n",
      "Iteration 317, loss = 0.11924916\n",
      "Iteration 318, loss = 0.11916520\n",
      "Iteration 319, loss = 0.11907684\n",
      "Iteration 320, loss = 0.11902510\n",
      "Iteration 321, loss = 0.11898235\n",
      "Iteration 322, loss = 0.11887168\n",
      "Iteration 323, loss = 0.11881762\n",
      "Iteration 324, loss = 0.11880445\n",
      "Iteration 325, loss = 0.11863964\n",
      "Iteration 326, loss = 0.11858281\n",
      "Iteration 327, loss = 0.11848644\n",
      "Iteration 328, loss = 0.11845726\n",
      "Iteration 329, loss = 0.11836133\n",
      "Iteration 330, loss = 0.11829947\n",
      "Iteration 331, loss = 0.11820387\n",
      "Iteration 332, loss = 0.11811927\n",
      "Iteration 333, loss = 0.11809572\n",
      "Iteration 334, loss = 0.11803297\n",
      "Iteration 335, loss = 0.11793268\n",
      "Iteration 336, loss = 0.11787167\n",
      "Iteration 337, loss = 0.11777739\n",
      "Iteration 338, loss = 0.11775930\n",
      "Iteration 339, loss = 0.11764531\n",
      "Iteration 340, loss = 0.11753716\n",
      "Iteration 341, loss = 0.11752185\n",
      "Iteration 342, loss = 0.11743073\n",
      "Iteration 343, loss = 0.11738782\n",
      "Iteration 344, loss = 0.11729289\n",
      "Iteration 345, loss = 0.11720135\n",
      "Iteration 346, loss = 0.11715944\n",
      "Iteration 347, loss = 0.11709267\n",
      "Iteration 348, loss = 0.11700680\n",
      "Iteration 349, loss = 0.11695795\n",
      "Iteration 350, loss = 0.11686863\n",
      "Iteration 351, loss = 0.11686482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.html\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(14,)], # [(14,),(100,),(14,4),(7,7,)] \n",
    "    'mlp__activation' : ['relu'], #['logistic','tanh','relu'] \n",
    "    'mlp__solver' : ['sgd'],\n",
    "    'mlp__alpha' : [0.1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1 ,*0.1\n",
    "    'mlp__batch_size' : [99], # ~297 samples for train at inner cv -> 3 equal size batches , #[16,32,64,128,'auto']\n",
    "    'mlp__learning_rate' : ['constant','adaptive'], # [*'constant','invscaling','adaptive']\n",
    "    'mlp__learning_rate_init' : [0.01],\n",
    "    'mlp__power_t' : [0.5],\n",
    "    'mlp__momentum' : [0.8], # np.arange(0.1,1,0.1), *0.8\n",
    "    'mlp__max_iter' : [1000],\n",
    "    'mlp__verbose' : [1]\n",
    "}\n",
    "\n",
    "search_sgd = RandomizedSearchCV(clf_mlp, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=100)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.948     0.953     0.951       212\n",
      "   Malignant      0.953     0.948     0.950       212\n",
      "\n",
      "    accuracy                          0.950       424\n",
      "   macro avg      0.950     0.950     0.950       424\n",
      "weighted avg      0.950     0.950     0.950       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.948     0.953     0.951       212\n",
      "   Malignant      0.953     0.948     0.950       212\n",
      "\n",
      "    accuracy                          0.950       424\n",
      "   macro avg      0.950     0.950     0.950       424\n",
      "weighted avg      0.950     0.950     0.950       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Cross validate\n",
    "clf_mlp_sgd =  Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                ('mlp',MLPClassifier(solver='sgd',alpha=0.1, batch_size=99,\n",
    "                               hidden_layer_sizes=(14,),learning_rate='constant',\n",
    "                               learning_rate_init=0.01, max_iter=1000,momentum=0.8,\n",
    "                               random_state=13, verbose=0))])\n",
    "\n",
    "score = cross_val_score(clf_mlp_sgd, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)\n",
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Solver : LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "# https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote20.html\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "# https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes' : [(14,)],  # [(14,),(100,),(14,4),(7,7,)] \n",
    "    'mlp__activation' : ['relu'],\n",
    "    'mlp__solver' : ['lbfgs'],\n",
    "    'mlp__alpha' : [1], #10.0 ** -np.arange(1, 7) , np.linspace(0.1,3,20) , 1\n",
    "    'mlp__max_iter' : np.arange(300,500,10) , # np.arange(100,300,10) , [100,200,500,1000]\n",
    "}\n",
    "\n",
    "search_lbfgs = RandomizedSearchCV(clf_mlp, param_distributions=param_grid, n_jobs=-1,cv=search_cv,verbose=1,scoring='f1_macro',n_iter=100)\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "  \n",
    "# Nested Cross validation\n",
    "score = cross_val_score(search_lbfgs, x_new, y, scoring=make_scorer(classification_report_with_accuracy_score),cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign      0.944     0.953     0.948       212\n",
      "   Malignant      0.952     0.943     0.948       212\n",
      "\n",
      "    accuracy                          0.948       424\n",
      "   macro avg      0.948     0.948     0.948       424\n",
      "weighted avg      0.948     0.948     0.948       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(originalclass, predictedclass, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16'></a>\n",
    "## 16) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below are the tables of the specific feature selection method.\n",
    "* The performance of the algorithms is in descending order.\n",
    "* All the results are the average values of a 10-fold cross validation.\n",
    "* The columns contain the accuracy and the average values of precision, recall and f1 score.\n",
    "* It is observed that the number of samples of Βenign and Μalignant cancer are equal (212 respectively), so the weighted average and the macro average are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"6\"> Selected Features : Default algorithms</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>precision </th>\n",
    "        <th>recall</th>\n",
    "        <th>f1 score</th>\n",
    "        <th>accuracy</th>\n",
    "        <th>Execution Time<br>\n",
    "        (seconds)</th>  \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MLP</th>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>0.976</td>\n",
    "        <td>3.570</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>AdaBoost</th>\n",
    "        <td>0.974</td>\n",
    "        <td>0.974</td>\n",
    "        <td>0.974</td>\n",
    "        <td>0.974</td>\n",
    "        <td>0.750</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SVC</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.075</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SGD</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.064</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ridge</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.073</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LDA</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.071</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Random Forest</th>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>1.138</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LGBM</th>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.471</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>XGBoost</th>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.950</td>\n",
    "        <td>0.565</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>QDA</th>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.063</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>KNN</th>\n",
    "        <td>0.944</td>\n",
    "        <td>0.943</td>\n",
    "        <td>0.943</td>\n",
    "        <td>0.943</td>\n",
    "        <td>0.081</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>GNB</th>\n",
    "        <td>0.935</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.052</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Decision Tree</th>\n",
    "        <td>0.915</td>\n",
    "        <td>0.915</td>\n",
    "        <td>0.915</td>\n",
    "        <td>0.915</td>\n",
    "        <td>0.058</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"6\"> Selected Features : Tuned algorithms</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>precision </th>\n",
    "        <th>recall</th>\n",
    "        <th>f1 score</th>\n",
    "        <th>accuracy</th>\n",
    "        <th>Execution Time<br>\n",
    "        (seconds)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>MLP</th>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>0.979</td>\n",
    "        <td>trial and error</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SGD</th>\n",
    "        <td>0.970</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>0.969</td>\n",
    "        <td>3.455</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>SVC</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>6.842</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LGBM</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>86.751</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>XGBoost</th>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>0.965</td>\n",
    "        <td>225.429</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>LDA</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>6.350</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Ridge</th>\n",
    "        <td>0.962</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>27.177</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Random Forest</th>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>0.960</td>\n",
    "        <td>153.843</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>AdaBoost</th>\n",
    "        <td>0.958</td>\n",
    "        <td>0.958</td>\n",
    "        <td>0.958</td>\n",
    "        <td>0.958</td>\n",
    "        <td>162.774</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>KNN</th>\n",
    "        <td>0.956</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>0.955</td>\n",
    "        <td>80.449</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>QDA</th>\n",
    "        <td>0.947</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>0.946</td>\n",
    "        <td>24.638</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Decision Tree</th>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>0.934</td>\n",
    "        <td>42.595</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>GNB</th>\n",
    "        <td>0.930</td>\n",
    "        <td>0.927</td>\n",
    "        <td>0.927</td>\n",
    "        <td>0.927</td>\n",
    "        <td>6.874</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As it is seen, some models perform better with default parameters. This can happen for various reasons such as:\n",
    "    - Unlucky selection of hyperparameters from random search\n",
    "    - Hyperparameters selected cause overfitting\n",
    "    - Smaller training sample in the inner loop due to nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Sources for nested cross validation :\n",
    "    1. [Cross-Validation and Hyperparameter Search in scikit-learn - A Complete Guide](<https://dev.to/balapriya/cross-validation-and-hyperparameter-search-in-scikit-learn-a-complete-guide-5ed8>)\n",
    "    2. [Nested Cross Validation for Algorithm Selection](<https://vitalflux.com/python-nested-cross-validation-algorithm-selection/>)\n",
    "    3. [Nested Cross-Validation for Machine Learning with Python](<https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/>)\n",
    "    4. [Nested cross validation for model selection](<https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection/65158#65158>)\n",
    "    5. [scikit-learn GridSearchCV with multiple repetitions](<https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764>)\n",
    "    6. [Model selection done right: A gentle introduction to nested cross-validation](<https://ploomber.io/blog/nested-cv/>)\n",
    "    7. [How to obtain optimal hyperparameters after nested cross validation?](<https://stats.stackexchange.com/questions/254612/how-to-obtain-optimal-hyperparameters-after-nested-cross-validation>)\n",
    "    8. [Cross-validation for parameter tuning, model selection, and feature selection](<https://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb>)\n",
    "- Sources for Hyper Parameter-Optimization :\n",
    "    1. [Random Search for Hyper-Parameter Optimization](<https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>)\n",
    "    2. [Hyperparameter tuning for machine learning models](<https://www.jeremyjordan.me/hyperparameter-tuning/>)\n",
    "- Sources for code :\n",
    "    - All sources are in comments at each code part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
